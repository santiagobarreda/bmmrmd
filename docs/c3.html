<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Fitting Bayesian regression models with brms | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</title>
  <meta name="description" content="Bayesian Models for Repeated Measures" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Fitting Bayesian regression models with brms | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Bayesian Models for Repeated Measures" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Fitting Bayesian regression models with brms | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  
  <meta name="twitter:description" content="Bayesian Models for Repeated Measures" />
  

<meta name="author" content="Santiago Bareda and Noah Silbert" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="c2.html"/>
<link rel="next" href="c4.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Models for Linguists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bayesian-multilevel-models-and-repeated-measures-data"><i class="fa fa-check"></i>Bayesian Multilevel models and repeated measures data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-missing-from-this-book"><i class="fa fa-check"></i>What’s missing from this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#statistics-as-procedural-knowledge"><i class="fa fa-check"></i>Statistics as Procedural knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practice-vs-brain-power"><i class="fa fa-check"></i>Practice vs brain power</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#supplemental-resources"><i class="fa fa-check"></i>Supplemental Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#our-target-audience"><i class="fa fa-check"></i>Our target audience</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-self-starter"><i class="fa fa-check"></i>The self-starter</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-convert"><i class="fa fa-check"></i>The convert</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-instructor"><i class="fa fa-check"></i>The instructor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-you-need-installed-to-use-this-book"><i class="fa fa-check"></i>What you need installed to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-go-bayesian"><i class="fa fa-check"></i>Why go Bayesian?</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-brms"><i class="fa fa-check"></i>Why brms?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#it-takes-a-village-of-books"><i class="fa fa-check"></i>It takes a village (of books)</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="c1.html"><a href="c1.html"><i class="fa fa-check"></i><b>1</b> Introduction: Experiments and Variables</a>
<ul>
<li class="chapter" data-level="1.1" data-path="c1.html"><a href="c1.html#chapter-pre-cap"><i class="fa fa-check"></i><b>1.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="1.2" data-path="c1.html"><a href="c1.html#c1-exp-and-effects"><i class="fa fa-check"></i><b>1.2</b> Experiments and effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="c1.html"><a href="c1.html#c1-exp-and-inference"><i class="fa fa-check"></i><b>1.2.1</b> Experiments and inference</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="c1.html"><a href="c1.html#c1-exp"><i class="fa fa-check"></i><b>1.3</b> Our experiment</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="c1.html"><a href="c1.html#c1-exp-intro"><i class="fa fa-check"></i><b>1.3.1</b> Our experiment: Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="c1.html"><a href="c1.html#c1-methods"><i class="fa fa-check"></i><b>1.3.2</b> Our experimental methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="c1.html"><a href="c1.html#c1-research-questions"><i class="fa fa-check"></i><b>1.3.3</b> Our research questions</a></li>
<li class="chapter" data-level="1.3.4" data-path="c1.html"><a href="c1.html#c1-exp-data"><i class="fa fa-check"></i><b>1.3.4</b> Our experimental data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="c1.html"><a href="c1.html#c1-variables"><i class="fa fa-check"></i><b>1.4</b> Variables</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="c1.html"><a href="c1.html#c1-pops-and-samps"><i class="fa fa-check"></i><b>1.4.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.4.2" data-path="c1.html"><a href="c1.html#c1-dep-and-indep"><i class="fa fa-check"></i><b>1.4.2</b> Dependent and Independent Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="c1.html"><a href="c1.html#c1-categorical"><i class="fa fa-check"></i><b>1.4.3</b> Categorical variables and ‘factors’</a></li>
<li class="chapter" data-level="1.4.4" data-path="c1.html"><a href="c1.html#c1-quantitative"><i class="fa fa-check"></i><b>1.4.4</b> Quantitative variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="c1.html"><a href="c1.html#c1-logical"><i class="fa fa-check"></i><b>1.4.5</b> Logical variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="c1.html"><a href="c1.html#c1-inspecting"><i class="fa fa-check"></i><b>1.5</b> Inspecting our data</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="c1.html"><a href="c1.html#c1-inspecting-categorical"><i class="fa fa-check"></i><b>1.5.1</b> Inspecting categorical variables</a></li>
<li class="chapter" data-level="1.5.2" data-path="c1.html"><a href="c1.html#c1-inspecting-quantitative"><i class="fa fa-check"></i><b>1.5.2</b> Inspecting quantitative variables</a></li>
<li class="chapter" data-level="1.5.3" data-path="c1.html"><a href="c1.html#c1-inspecting-together"><i class="fa fa-check"></i><b>1.5.3</b> Exploring continuous and categorical variables together</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="c1.html"><a href="c1.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
<li class="chapter" data-level="1.7" data-path="c1.html"><a href="c1.html#references"><i class="fa fa-check"></i><b>1.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c2.html"><a href="c2.html"><i class="fa fa-check"></i><b>2</b> Probabilities, likelihood, and inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="c2.html"><a href="c2.html#chapter-pre-cap-1"><i class="fa fa-check"></i><b>2.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="2.2" data-path="c2.html"><a href="c2.html#c2-data"><i class="fa fa-check"></i><b>2.2</b> Data and research questions</a></li>
<li class="chapter" data-level="2.3" data-path="c2.html"><a href="c2.html#c2-empirical-prob"><i class="fa fa-check"></i><b>2.3</b> Empirical Probabilities</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="c2.html"><a href="c2.html#c2-conditional"><i class="fa fa-check"></i><b>2.3.1</b> Conditional and marginal probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="c2.html"><a href="c2.html#c2-joint"><i class="fa fa-check"></i><b>2.3.2</b> Joint probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="c2.html"><a href="c2.html#c2-theoretical"><i class="fa fa-check"></i><b>2.4</b> Probability distributions</a></li>
<li class="chapter" data-level="2.5" data-path="c2.html"><a href="c2.html#c2-normal"><i class="fa fa-check"></i><b>2.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="c2.html"><a href="c2.html#c2-sample-mean"><i class="fa fa-check"></i><b>2.5.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.5.2" data-path="c2.html"><a href="c2.html#c2-sample-variance"><i class="fa fa-check"></i><b>2.5.2</b> The sample variance (or standard deviation)</a></li>
<li class="chapter" data-level="2.5.3" data-path="c2.html"><a href="c2.html#c2-normal-density"><i class="fa fa-check"></i><b>2.5.3</b> The normal density</a></li>
<li class="chapter" data-level="2.5.4" data-path="c2.html"><a href="c2.html#c2-standard-normal"><i class="fa fa-check"></i><b>2.5.4</b> The standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="c2.html"><a href="c2.html#c2-models-and-inference"><i class="fa fa-check"></i><b>2.6</b> Models and inference</a></li>
<li class="chapter" data-level="2.7" data-path="c2.html"><a href="c2.html#c2-likelihoods"><i class="fa fa-check"></i><b>2.7</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="c2.html"><a href="c2.html#c2-chars-of-likelihoods"><i class="fa fa-check"></i><b>2.7.1</b> Characteristics of likelihoods</a></li>
<li class="chapter" data-level="2.7.2" data-path="c2.html"><a href="c2.html#c2-logarithms"><i class="fa fa-check"></i><b>2.7.2</b> A brief aside on logarithms</a></li>
<li class="chapter" data-level="2.7.3" data-path="c2.html"><a href="c2.html#c2-chars-of-likelihoods-2"><i class="fa fa-check"></i><b>2.7.3</b> Characteristics of likelihoods, continued</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="c2.html"><a href="c2.html#c2-inference-and-likelihood"><i class="fa fa-check"></i><b>2.8</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.9" data-path="c2.html"><a href="c2.html#exercises-1"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
<li class="chapter" data-level="2.10" data-path="c2.html"><a href="c2.html#references-1"><i class="fa fa-check"></i><b>2.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c3.html"><a href="c3.html"><i class="fa fa-check"></i><b>3</b> Fitting Bayesian regression models with <em>brms</em></a>
<ul>
<li class="chapter" data-level="3.1" data-path="c3.html"><a href="c3.html#chapter-pre-cap-2"><i class="fa fa-check"></i><b>3.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="3.2" data-path="c3.html"><a href="c3.html#c3-what-is-reg"><i class="fa fa-check"></i><b>3.2</b> What are regression models?</a></li>
<li class="chapter" data-level="3.3" data-path="c3.html"><a href="c3.html#c3-whats-bayes"><i class="fa fa-check"></i><b>3.3</b> What’s ‘Bayesian’ about these models?</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="c3.html"><a href="c3.html#c3-priors"><i class="fa fa-check"></i><b>3.3.1</b> Prior probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="c3.html"><a href="c3.html#c3-posterior"><i class="fa fa-check"></i><b>3.3.2</b> Posterior distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="c3.html"><a href="c3.html#c3-characteristics-posteriors"><i class="fa fa-check"></i><b>3.3.3</b> Posterior distributions and shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="c3.html"><a href="c3.html#c3-sampling"><i class="fa fa-check"></i><b>3.4</b> Sampling from the posterior using <em>Stan</em> and <em>brms</em></a></li>
<li class="chapter" data-level="3.5" data-path="c3.html"><a href="c3.html#c3-estimating"><i class="fa fa-check"></i><b>3.5</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="c3.html"><a href="c3.html#c3-data-qs-1"><i class="fa fa-check"></i><b>3.5.1</b> Data and Research Questions</a></li>
<li class="chapter" data-level="3.5.2" data-path="c3.html"><a href="c3.html#c3-description-1"><i class="fa fa-check"></i><b>3.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.5.3" data-path="c3.html"><a href="c3.html#c3-errors-and-residuals"><i class="fa fa-check"></i><b>3.5.3</b> Errors and residuals</a></li>
<li class="chapter" data-level="3.5.4" data-path="c3.html"><a href="c3.html#c3-model-formula"><i class="fa fa-check"></i><b>3.5.4</b> The model formula</a></li>
<li class="chapter" data-level="3.5.5" data-path="c3.html"><a href="c3.html#c3-calling-brm"><i class="fa fa-check"></i><b>3.5.5</b> Fitting the model: Calling the <em>brm</em> function</a></li>
<li class="chapter" data-level="3.5.6" data-path="c3.html"><a href="c3.html#c3-interpreting-print"><i class="fa fa-check"></i><b>3.5.6</b> Interpreting the model: The print statement</a></li>
<li class="chapter" data-level="3.5.7" data-path="c3.html"><a href="c3.html#c3-seeing-samples"><i class="fa fa-check"></i><b>3.5.7</b> Seeing the samples</a></li>
<li class="chapter" data-level="3.5.8" data-path="c3.html"><a href="c3.html#c3-getting-residuals"><i class="fa fa-check"></i><b>3.5.8</b> Getting the residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="c3.html"><a href="c3.html#c3-checking-convergence"><i class="fa fa-check"></i><b>3.6</b> Checking model convergence</a></li>
<li class="chapter" data-level="3.7" data-path="c3.html"><a href="c3.html#c3-specifying-priors"><i class="fa fa-check"></i><b>3.7</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="3.8" data-path="c3.html"><a href="c3.html#c3-log-posterior"><i class="fa fa-check"></i><b>3.8</b> The log prior and log posterior densities</a></li>
<li class="chapter" data-level="3.9" data-path="c3.html"><a href="c3.html#c3-answering-qs"><i class="fa fa-check"></i><b>3.9</b> Answering our research questions</a></li>
<li class="chapter" data-level="3.10" data-path="c3.html"><a href="c3.html#c3-frequentist"><i class="fa fa-check"></i><b>3.10</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="c3.html"><a href="c3.html#c3-vs-ttest"><i class="fa fa-check"></i><b>3.10.1</b> One-sample t-test vs. intercept-only Bayesian models</a></li>
<li class="chapter" data-level="3.10.2" data-path="c3.html"><a href="c3.html#c3-vs-ols"><i class="fa fa-check"></i><b>3.10.2</b> Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="c3.html"><a href="c3.html#exercises-2"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c4.html"><a href="c4.html"><i class="fa fa-check"></i><b>4</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="c4.html"><a href="c4.html#chapter-pre-cap-3"><i class="fa fa-check"></i><b>4.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="4.2" data-path="c4.html"><a href="c4.html#c4-multilevel"><i class="fa fa-check"></i><b>4.2</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="c4.html"><a href="c4.html#c4-levels"><i class="fa fa-check"></i><b>4.2.1</b> Multilevel models and ‘levels’ of variation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="c4.html"><a href="c4.html#c4-many-levels"><i class="fa fa-check"></i><b>4.3</b> Representing predictors with many levels</a></li>
<li class="chapter" data-level="4.4" data-path="c4.html"><a href="c4.html#c4-strategies"><i class="fa fa-check"></i><b>4.4</b> Strategies for estimating factors with many levels</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="c4.html"><a href="c4.html#c4-complete-pooling"><i class="fa fa-check"></i><b>4.4.1</b> Complete pooling</a></li>
<li class="chapter" data-level="4.4.2" data-path="c4.html"><a href="c4.html#c4-no-pooling"><i class="fa fa-check"></i><b>4.4.2</b> No pooling</a></li>
<li class="chapter" data-level="4.4.3" data-path="c4.html"><a href="c4.html#c4-partial-pooling"><i class="fa fa-check"></i><b>4.4.3</b> (Adaptive) Partial pooling</a></li>
<li class="chapter" data-level="4.4.4" data-path="c4.html"><a href="c4.html#hyperpriors"><i class="fa fa-check"></i><b>4.4.4</b> Hyperpriors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="c4.html"><a href="c4.html#c4-estimating1"><i class="fa fa-check"></i><b>4.5</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="c4.html"><a href="c4.html#c4-data-and-qs-1"><i class="fa fa-check"></i><b>4.5.1</b> Data and Research questions</a></li>
<li class="chapter" data-level="4.5.2" data-path="c4.html"><a href="c4.html#description-of-the-model"><i class="fa fa-check"></i><b>4.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="4.5.3" data-path="c4.html"><a href="c4.html#c4-fitting-1"><i class="fa fa-check"></i><b>4.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="4.5.4" data-path="c4.html"><a href="c4.html#interpreting-the-model"><i class="fa fa-check"></i><b>4.5.4</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="c4.html"><a href="c4.html#c4-random-effects"><i class="fa fa-check"></i><b>4.6</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="c4.html"><a href="c4.html#c4-inspecting-random-effects"><i class="fa fa-check"></i><b>4.6.1</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="c4.html"><a href="c4.html#c4-simulating"><i class="fa fa-check"></i><b>4.7</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="4.8" data-path="c4.html"><a href="c4.html#c4-second-random-effect"><i class="fa fa-check"></i><b>4.8</b> Adding a second random effect</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="c4.html"><a href="c4.html#c4-updating-model"><i class="fa fa-check"></i><b>4.8.1</b> Updating the model description</a></li>
<li class="chapter" data-level="4.8.2" data-path="c4.html"><a href="c4.html#fitting-and-interpreting-the-model"><i class="fa fa-check"></i><b>4.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="c4.html"><a href="c4.html#c4-investigating-shrinkage"><i class="fa fa-check"></i><b>4.9</b> Investigating ‘shrinkage’</a></li>
<li class="chapter" data-level="4.10" data-path="c4.html"><a href="c4.html#c4-answering-question"><i class="fa fa-check"></i><b>4.10</b> Answering our research questions</a></li>
<li class="chapter" data-level="4.11" data-path="c4.html"><a href="c4.html#c4-frequentist"><i class="fa fa-check"></i><b>4.11</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="c4.html"><a href="c4.html#c4-vs-lmer"><i class="fa fa-check"></i><b>4.11.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="c4.html"><a href="c4.html#exercises-3"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
<li class="chapter" data-level="4.13" data-path="c4.html"><a href="c4.html#references-2"><i class="fa fa-check"></i><b>4.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="c5.html"><a href="c5.html"><i class="fa fa-check"></i><b>5</b> Comparing two groups of observations: Factors and contrasts</a>
<ul>
<li class="chapter" data-level="5.1" data-path="c5.html"><a href="c5.html#chapter-pre-cap-4"><i class="fa fa-check"></i><b>5.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="5.2" data-path="c5.html"><a href="c5.html#comparing-two-groups"><i class="fa fa-check"></i><b>5.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="5.3" data-path="c5.html"><a href="c5.html#distribution-of-repeated-measures-across-factor-levels"><i class="fa fa-check"></i><b>5.3</b> Distribution of repeated measures across factor levels</a></li>
<li class="chapter" data-level="5.4" data-path="c5.html"><a href="c5.html#c5-data-and-qs"><i class="fa fa-check"></i><b>5.4</b> Data and research questions</a></li>
<li class="chapter" data-level="5.5" data-path="c5.html"><a href="c5.html#c5-two-means"><i class="fa fa-check"></i><b>5.5</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="c5.html"><a href="c5.html#fitting-the-model"><i class="fa fa-check"></i><b>5.5.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="c5.html"><a href="c5.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>5.5.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="c5.html"><a href="c5.html#c5-contrasts"><i class="fa fa-check"></i><b>5.6</b> Contrasts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="c5.html"><a href="c5.html#c5-treatment-coding"><i class="fa fa-check"></i><b>5.6.1</b> Treatment coding</a></li>
<li class="chapter" data-level="5.6.2" data-path="c5.html"><a href="c5.html#c5-sum-coding"><i class="fa fa-check"></i><b>5.6.2</b> Sum coding</a></li>
<li class="chapter" data-level="5.6.3" data-path="c5.html"><a href="c5.html#c5-comparison-sum-treatment"><i class="fa fa-check"></i><b>5.6.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="c5.html"><a href="c5.html#c5-refittin-sum"><i class="fa fa-check"></i><b>5.7</b> Sum coding and the decomposition of variation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="c5.html"><a href="c5.html#c5-description-1"><i class="fa fa-check"></i><b>5.7.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.7.2" data-path="c5.html"><a href="c5.html#fitting-the-model-1"><i class="fa fa-check"></i><b>5.7.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.7.3" data-path="c5.html"><a href="c5.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>5.7.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="c5.html"><a href="c5.html#c5-working-with-posteriors"><i class="fa fa-check"></i><b>5.8</b> Inspecting and manipulating the posterior samples</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="c5.html"><a href="c5.html#c5-using-hypothesis"><i class="fa fa-check"></i><b>5.8.1</b> Using the <em>hypothesis</em> function</a></li>
<li class="chapter" data-level="5.8.2" data-path="c5.html"><a href="c5.html#c5-manipulating-random-effects"><i class="fa fa-check"></i><b>5.8.2</b> Working with the random effects</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="c5.html"><a href="c5.html#c5-robustness"><i class="fa fa-check"></i><b>5.9</b> Making our models more robust: The (non-standardized) t distribution</a></li>
<li class="chapter" data-level="5.10" data-path="c5.html"><a href="c5.html#re-fitting-with-t-distributed-errors."><i class="fa fa-check"></i><b>5.10</b> Re-fitting with t-distributed errors</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="c5.html"><a href="c5.html#description-of-the-model-1"><i class="fa fa-check"></i><b>5.10.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.10.2" data-path="c5.html"><a href="c5.html#fitting-and-interpreting-the-model-1"><i class="fa fa-check"></i><b>5.10.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="c5.html"><a href="c5.html#c5-simulating"><i class="fa fa-check"></i><b>5.11</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="5.12" data-path="c5.html"><a href="c5.html#c5-answering-qs"><i class="fa fa-check"></i><b>5.12</b> Answering our research questions</a></li>
<li class="chapter" data-level="5.13" data-path="c5.html"><a href="c5.html#c5-frequentist"><i class="fa fa-check"></i><b>5.13</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="c5.html"><a href="c5.html#bayesian-multilevel-models-vs.-lmer"><i class="fa fa-check"></i><b>5.13.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="c5.html"><a href="c5.html#exercises-4"><i class="fa fa-check"></i><b>5.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="c6.html"><a href="c6.html"><i class="fa fa-check"></i><b>6</b> Variation in parameters (‘random effects’) and model comparison</a>
<ul>
<li class="chapter" data-level="6.1" data-path="c6.html"><a href="c6.html#chapter-pre-cap-5"><i class="fa fa-check"></i><b>6.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="6.2" data-path="c6.html"><a href="c6.html#c6-data-and-qs"><i class="fa fa-check"></i><b>6.2</b> Data and research questions</a></li>
<li class="chapter" data-level="6.3" data-path="c6.html"><a href="c6.html#c6-variation-sources"><i class="fa fa-check"></i><b>6.3</b> Variation in parameters across sources of data</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="c6.html"><a href="c6.html#description-of-our-model"><i class="fa fa-check"></i><b>6.3.1</b> Description of our model</a></li>
<li class="chapter" data-level="6.3.2" data-path="c6.html"><a href="c6.html#c6-correlations"><i class="fa fa-check"></i><b>6.3.2</b> Correlations between random parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="c6.html"><a href="c6.html#c6-random-and-mvn"><i class="fa fa-check"></i><b>6.3.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.4" data-path="c6.html"><a href="c6.html#c6-mvn-priors"><i class="fa fa-check"></i><b>6.3.4</b> Specifying priors for a multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.5" data-path="c6.html"><a href="c6.html#updating-our-model-description"><i class="fa fa-check"></i><b>6.3.5</b> Updating our model description</a></li>
<li class="chapter" data-level="6.3.6" data-path="c6.html"><a href="c6.html#c6-fitting"><i class="fa fa-check"></i><b>6.3.6</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="c6.html"><a href="c6.html#c6-model-comparison"><i class="fa fa-check"></i><b>6.4</b> Model Comparison</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="c6.html"><a href="c6.html#c6-in-and-out-prediction"><i class="fa fa-check"></i><b>6.4.1</b> In-sample and out-of-sample prediction</a></li>
<li class="chapter" data-level="6.4.2" data-path="c6.html"><a href="c6.html#c6-out-sample-adjust"><i class="fa fa-check"></i><b>6.4.2</b> Out-of-sample prediction: Adjusting predictive accuracy</a></li>
<li class="chapter" data-level="6.4.3" data-path="c6.html"><a href="c6.html#c6-out-sample-crossval"><i class="fa fa-check"></i><b>6.4.3</b> Out-of-sample prediction: Cross validation</a></li>
<li class="chapter" data-level="6.4.4" data-path="c6.html"><a href="c6.html#selecting-a-model"><i class="fa fa-check"></i><b>6.4.4</b> Selecting a model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="c6.html"><a href="c6.html#c6-answering"><i class="fa fa-check"></i><b>6.5</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.6" data-path="c6.html"><a href="c6.html#c6-frequentist"><i class="fa fa-check"></i><b>6.6</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="c6.html"><a href="c6.html#c6-vs-lmer"><i class="fa fa-check"></i><b>6.6.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="c6.html"><a href="c6.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="c6.html"><a href="c6.html#references-3"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><i class="fa fa-check"></i><b>7</b> Comparing many groups, interactions, and posterior predictive checks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#chapter-pre-cap-6"><i class="fa fa-check"></i><b>7.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="7.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions"><i class="fa fa-check"></i><b>7.2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.2.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-description-1"><i class="fa fa-check"></i><b>7.2.2</b> Description of our model</a></li>
<li class="chapter" data-level="7.2.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-2"><i class="fa fa-check"></i><b>7.2.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-multiple-factors-simultaneously"><i class="fa fa-check"></i><b>7.3</b> Investigating multiple factors simultaneously</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>7.3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.3.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-the-model-2"><i class="fa fa-check"></i><b>7.3.2</b> Description of the model</a></li>
<li class="chapter" data-level="7.3.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-3"><i class="fa fa-check"></i><b>7.3.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-posterior-prediction"><i class="fa fa-check"></i><b>7.4</b> Posterior prediction: Using our models to predict new data</a></li>
<li class="chapter" data-level="7.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-interactions-and-plots"><i class="fa fa-check"></i><b>7.5</b> Interactions and interaction plots</a></li>
<li class="chapter" data-level="7.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>7.6</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>7.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.6.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#model-formulas"><i class="fa fa-check"></i><b>7.6.2</b> Model formulas</a></li>
<li class="chapter" data-level="7.6.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-our-model-1"><i class="fa fa-check"></i><b>7.6.3</b> Description of our model</a></li>
<li class="chapter" data-level="7.6.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-4"><i class="fa fa-check"></i><b>7.6.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="7.6.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-calc-means"><i class="fa fa-check"></i><b>7.6.5</b> Caulculating group means in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#calculating-simple-effects-in-the-presence-of-interactions"><i class="fa fa-check"></i><b>7.6.6</b> Calculating simple effects in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#assessing-model-fit-bayesian-r2"><i class="fa fa-check"></i><b>7.6.7</b> Assessing model fit: Bayesian <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-answering"><i class="fa fa-check"></i><b>7.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="7.8" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.8</b> Factors with more than two levels</a></li>
<li class="chapter" data-level="7.9" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-frequentist"><i class="fa fa-check"></i><b>7.9</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#bayesian-multilevel-models-vs.-lmer-1"><i class="fa fa-check"></i><b>7.9.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#exercises-6"><i class="fa fa-check"></i><b>7.10</b> Exercises</a></li>
<li class="chapter" data-level="7.11" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#references-4"><i class="fa fa-check"></i><b>7.11</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html"><i class="fa fa-check"></i><b>8</b> Varying variances, more about priors, and prior predictive checks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#chapter-pre-cap-7"><i class="fa fa-check"></i><b>8.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>8.2</b> Data and Research questions</a></li>
<li class="chapter" data-level="8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-about-priors"><i class="fa fa-check"></i><b>8.3</b> More about priors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-prior-prediction"><i class="fa fa-check"></i><b>8.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.3.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-specific-priors"><i class="fa fa-check"></i><b>8.3.2</b> More specific priors</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#heteroskedasticity-and-distributional-or-mixture-models"><i class="fa fa-check"></i><b>8.4</b> Heteroskedasticity and distributional (or mixture) models</a></li>
<li class="chapter" data-level="8.5" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-simple-model-error-varies-according-to-a-single-fixed-effect"><i class="fa fa-check"></i><b>8.5</b> A ‘simple’ model: Error varies according to a single fixed effect</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#description-of-our-model-2"><i class="fa fa-check"></i><b>8.5.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.5.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#prior-predictive-checks"><i class="fa fa-check"></i><b>8.5.2</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.5.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-5"><i class="fa fa-check"></i><b>8.5.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-complex-model-error-varies-according-to-fixed-and-random-effects"><i class="fa fa-check"></i><b>8.6</b> A ‘complex’ model: Error varies according to fixed and random effects</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2"><i class="fa fa-check"></i><b>8.6.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.6.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-6"><i class="fa fa-check"></i><b>8.6.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#answering-our-research-questions"><i class="fa fa-check"></i><b>8.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="8.8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability"><i class="fa fa-check"></i><b>8.8</b> Building identifiable and supportable models</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#collinearity"><i class="fa fa-check"></i><b>8.8.1</b> Collinearity</a></li>
<li class="chapter" data-level="8.8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#predictable-values-of-categorical-predictors"><i class="fa fa-check"></i><b>8.8.2</b> Predictable values of categorical predictors</a></li>
<li class="chapter" data-level="8.8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#saturated-and-nearly-saturated-models"><i class="fa fa-check"></i><b>8.8.3</b> Saturated, and ‘nearly-saturated’, models</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#exercises-7"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
<li class="chapter" data-level="8.10" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#references-5"><i class="fa fa-check"></i><b>8.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html"><i class="fa fa-check"></i><b>9</b> Quantitative predictors and their interactions with factors</a>
<ul>
<li class="chapter" data-level="9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#chapter-pre-cap-8"><i class="fa fa-check"></i><b>9.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>9.2</b> Data and research questions</a></li>
<li class="chapter" data-level="9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#modeling-variation-along-lines"><i class="fa fa-check"></i><b>9.3</b> Modeling variation along lines</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-3"><i class="fa fa-check"></i><b>9.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.3.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-centering"><i class="fa fa-check"></i><b>9.3.2</b> Centering quantitative predictors</a></li>
<li class="chapter" data-level="9.3.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-an-interpreting-the-model"><i class="fa fa-check"></i><b>9.3.3</b> Fitting an interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>9.4</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-4"><i class="fa fa-check"></i><b>9.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.4.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-7"><i class="fa fa-check"></i><b>9.4.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.4.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-shared-non-zero-slopes"><i class="fa fa-check"></i><b>9.4.3</b> Interpreting group effects in the presence of shared (non-zero) slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>9.5</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-5"><i class="fa fa-check"></i><b>9.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.5.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-3"><i class="fa fa-check"></i><b>9.5.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.5.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-varying-slopes"><i class="fa fa-check"></i><b>9.5.3</b> Interpreting group effects in the presence of varying slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-interim-discussion"><i class="fa fa-check"></i><b>9.6</b> Answering our research questions: Interim discussion</a></li>
<li class="chapter" data-level="9.7" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-updated"><i class="fa fa-check"></i><b>9.7</b> Data and research questions: Updated</a></li>
<li class="chapter" data-level="9.8" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-intercepts-and-slopes-for-each-level-of-a-grouping-factor-i.e.-random-slopes"><i class="fa fa-check"></i><b>9.8</b> Models with intercepts and slopes for each level of a grouping factor (i.e. ‘random slopes’)</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-6"><i class="fa fa-check"></i><b>9.8.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.8.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-8"><i class="fa fa-check"></i><b>9.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-multiple-predictors-for-each-level-of-a-grouping-factor"><i class="fa fa-check"></i><b>9.9</b> Models with multiple predictors for each level of a grouping factor</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-7"><i class="fa fa-check"></i><b>9.9.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-5"><i class="fa fa-check"></i><b>9.9.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#model-selection"><i class="fa fa-check"></i><b>9.9.3</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-updated"><i class="fa fa-check"></i><b>9.10</b> Answering our research questions: Updated</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#a-word-on-causality"><i class="fa fa-check"></i><b>9.10.1</b> A word on causality</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#exercises-8"><i class="fa fa-check"></i><b>9.11</b> Exercises</a></li>
<li class="chapter" data-level="9.12" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#references-6"><i class="fa fa-check"></i><b>9.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html"><i class="fa fa-check"></i><b>10</b> Logistic regression and signal detection theory models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#chapter-pre-cap-9"><i class="fa fa-check"></i><b>10.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous"><i class="fa fa-check"></i><b>10.2</b> Dichotomous variables and data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#generalizing-our-linear-models"><i class="fa fa-check"></i><b>10.3</b> Generalizing our linear models</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logits"><i class="fa fa-check"></i><b>10.4.1</b> Logits</a></li>
<li class="chapter" data-level="10.4.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit"><i class="fa fa-check"></i><b>10.4.2</b> The inverse logit link function</a></li>
<li class="chapter" data-level="10.4.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#building-intuitions-about-logits-and-the-inverse-logit-function"><i class="fa fa-check"></i><b>10.4.3</b> Building intuitions about logits and the inverse logit function</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-with-one-quantitative-predictor"><i class="fa fa-check"></i><b>10.5</b> Logistic regression with one quantitative predictor</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>10.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.5.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-8"><i class="fa fa-check"></i><b>10.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.5.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-0"><i class="fa fa-check"></i><b>10.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="10.5.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1"><i class="fa fa-check"></i><b>10.5.4</b> Interpreting the model</a></li>
<li class="chapter" data-level="10.5.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification"><i class="fa fa-check"></i><b>10.5.5</b> Using logistic models to understand classification</a></li>
<li class="chapter" data-level="10.5.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-question"><i class="fa fa-check"></i><b>10.5.6</b> Answering our research question</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#measuring-sensitivity-and-bias"><i class="fa fa-check"></i><b>10.6</b> Measuring sensitivity and bias</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-6"><i class="fa fa-check"></i><b>10.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.6.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-9"><i class="fa fa-check"></i><b>10.6.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.6.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#fitting-and-interpreting-the-model-9"><i class="fa fa-check"></i><b>10.6.3</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="10.6.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>10.6.4</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#exercises-9"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
<li class="chapter" data-level="10.8" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#references-7"><i class="fa fa-check"></i><b>10.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><i class="fa fa-check"></i><b>11</b> Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#chapter-pre-cap-10"><i class="fa fa-check"></i><b>11.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#models-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.2</b> Models with multiple quantitative predictors</a></li>
<li class="chapter" data-level="11.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#interactions-between-quantitative-predictors"><i class="fa fa-check"></i><b>11.3</b> Interactions between quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#centering-quantitative-predictors-when-including-interactions"><i class="fa fa-check"></i><b>11.3.1</b> Centering quantitative predictors when including interactions</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-7"><i class="fa fa-check"></i><b>11.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="11.3.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-description-1"><i class="fa fa-check"></i><b>11.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="11.3.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-the-model-2"><i class="fa fa-check"></i><b>11.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="11.3.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#advantages-of-bayesian-multilevel-models-for-large-models"><i class="fa fa-check"></i><b>11.3.5</b> Advantages of Bayesian multilevel models for large models</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-BANOVA"><i class="fa fa-check"></i><b>11.4</b> Bayesian Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#getting-the-standard-deviations-from-our-models-manually"><i class="fa fa-check"></i><b>11.4.1</b> Getting the standard deviations from our models ‘manually’</a></li>
<li class="chapter" data-level="11.4.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#using-the-banova-function"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>banova</code> function</a></li>
<li class="chapter" data-level="11.4.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-comparing-the-reduced-model"><i class="fa fa-check"></i><b>11.4.3</b> Fitting and comparing the reduced model</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#a-logistic-regression-model-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.5</b> A logistic regression model with multiple quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-8"><i class="fa fa-check"></i><b>11.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="11.5.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#description-of-the-model-10"><i class="fa fa-check"></i><b>11.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="11.5.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-the-model-and-applying-a-bayesian-anova"><i class="fa fa-check"></i><b>11.5.3</b> Fitting and the model and applying a Bayesian ANOVA</a></li>
<li class="chapter" data-level="11.5.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c12-2d-categorization"><i class="fa fa-check"></i><b>11.5.4</b> Categorization in two dimensions</a></li>
<li class="chapter" data-level="11.5.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#model-selection-and-misspecification"><i class="fa fa-check"></i><b>11.5.5</b> Model selection and misspecification</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#exercises-10"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#references-8"><i class="fa fa-check"></i><b>11.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial and Ordinal regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#chapter-pre-cap-11"><i class="fa fa-check"></i><b>12.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="12.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logits-and-the-softmax-function"><i class="fa fa-check"></i><b>12.2.1</b> Multinomial logits and the softmax function</a></li>
<li class="chapter" data-level="12.2.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#comparison-to-logistic-regression"><i class="fa fa-check"></i><b>12.2.2</b> Comparison to logistic regression</a></li>
<li class="chapter" data-level="12.2.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-9"><i class="fa fa-check"></i><b>12.2.3</b> Data and research questions</a></li>
<li class="chapter" data-level="12.2.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-our-model-3"><i class="fa fa-check"></i><b>12.2.4</b> Description of our model</a></li>
<li class="chapter" data-level="12.2.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-the-model-3"><i class="fa fa-check"></i><b>12.2.5</b> Fitting the model</a></li>
<li class="chapter" data-level="12.2.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>12.2.6</b> Interpreting the model</a></li>
<li class="chapter" data-level="12.2.7" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-multinomial-territorial-maps"><i class="fa fa-check"></i><b>12.2.7</b> Multinomial models and territorial maps</a></li>
<li class="chapter" data-level="12.2.8" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#refitting-the-model-without-speaker-random-effects"><i class="fa fa-check"></i><b>12.2.8</b> Refitting the model without speaker random effects</a></li>
<li class="chapter" data-level="12.2.9" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-2"><i class="fa fa-check"></i><b>12.2.9</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Ordinal (logistic) regression</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-cumulative-density"><i class="fa fa-check"></i><b>12.3.1</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="12.3.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-10"><i class="fa fa-check"></i><b>12.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="12.3.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-the-model-11"><i class="fa fa-check"></i><b>12.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="12.3.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-and-interpreting-the-model-10"><i class="fa fa-check"></i><b>12.3.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="12.3.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#listener-specific-discrimination-terms"><i class="fa fa-check"></i><b>12.3.5</b> Listener-specific discrimination terms</a></li>
<li class="chapter" data-level="12.3.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-3"><i class="fa fa-check"></i><b>12.3.6</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#exercises-11"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
<li class="chapter" data-level="12.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#references-9"><i class="fa fa-check"></i><b>12.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><i class="fa fa-check"></i><b>13</b> Writing up experiments: An investigation of the perception of apparent speaker characteristics from speech acoustics</a>
<ul>
<li class="chapter" data-level="13.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#fundamental-frequency-and-voice-pitch"><i class="fa fa-check"></i><b>13.1.1</b> Fundamental frequency and voice pitch</a></li>
<li class="chapter" data-level="13.1.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-fundamental-frequency-between-speakers"><i class="fa fa-check"></i><b>13.1.2</b> Variation in fundamental frequency between speakers</a></li>
<li class="chapter" data-level="13.1.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#voice-resonance-and-vocal-tract-length"><i class="fa fa-check"></i><b>13.1.3</b> Voice resonance and vocal-tract length</a></li>
<li class="chapter" data-level="13.1.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-estimating-vtl"><i class="fa fa-check"></i><b>13.1.4</b> Estimating vocal-tracts length from speech</a></li>
<li class="chapter" data-level="13.1.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-vocal-tract-length-between-speakers"><i class="fa fa-check"></i><b>13.1.5</b> Variation in vocal-tract length between speakers</a></li>
<li class="chapter" data-level="13.1.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-perception-of-chars"><i class="fa fa-check"></i><b>13.1.6</b> Perception of age, gender and size</a></li>
<li class="chapter" data-level="13.1.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#category-dependent-behavior"><i class="fa fa-check"></i><b>13.1.7</b> Category-dependent behavior</a></li>
<li class="chapter" data-level="13.1.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-current-experiment"><i class="fa fa-check"></i><b>13.1.8</b> The current experiment</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#methods"><i class="fa fa-check"></i><b>13.2</b> Methods</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#participants"><i class="fa fa-check"></i><b>13.2.1</b> Participants</a></li>
<li class="chapter" data-level="13.2.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-stimuli"><i class="fa fa-check"></i><b>13.2.2</b> Stimuli</a></li>
<li class="chapter" data-level="13.2.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#procedure"><i class="fa fa-check"></i><b>13.2.3</b> Procedure</a></li>
<li class="chapter" data-level="13.2.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#data-screening"><i class="fa fa-check"></i><b>13.2.4</b> Data screening</a></li>
<li class="chapter" data-level="13.2.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#loading-the-data-and-packages"><i class="fa fa-check"></i><b>13.2.5</b> Loading the data and packages</a></li>
<li class="chapter" data-level="13.2.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-height"><i class="fa fa-check"></i><b>13.2.6</b> Statistical Analysis: Apparent height</a></li>
<li class="chapter" data-level="13.2.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-gender"><i class="fa fa-check"></i><b>13.2.7</b> Statistical Analysis: Apparent gender</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-height-judgments"><i class="fa fa-check"></i><b>13.3</b> Results: Apparent height judgments</a></li>
<li class="chapter" data-level="13.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-height"><i class="fa fa-check"></i><b>13.4</b> Discussion: Apparent height</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#age-dependent-use-of-vtl-cues-on-apparent-height"><i class="fa fa-check"></i><b>13.4.1</b> Age-dependent use of VTL cues on apparent height</a></li>
<li class="chapter" data-level="13.4.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-effect-for-apparent-gender-on-apparent-height"><i class="fa fa-check"></i><b>13.4.2</b> The effect for apparent gender on apparent height</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-height-judgments"><i class="fa fa-check"></i><b>13.5</b> Conclusion: Apparent height judgments</a></li>
<li class="chapter" data-level="13.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.6</b> Results: Apparent gender judgments</a></li>
<li class="chapter" data-level="13.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.7</b> Discussion: Apparent gender judgments</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#effect-of-apparent-age-on-the-perception-of-femaleness"><i class="fa fa-check"></i><b>13.7.1</b> Effect of apparent age on the perception of femaleness</a></li>
<li class="chapter" data-level="13.7.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#between-listener-variation-in-gender-perception"><i class="fa fa-check"></i><b>13.7.2</b> Between-listener variation in gender perception</a></li>
<li class="chapter" data-level="13.7.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#beyond-gross-acoustic-cues-in-gender-perception"><i class="fa fa-check"></i><b>13.7.3</b> Beyond gross acoustic cues in gender perception</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-gender"><i class="fa fa-check"></i><b>13.8</b> Conclusion: Apparent gender</a></li>
<li class="chapter" data-level="13.9" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#next-steps"><i class="fa fa-check"></i><b>13.9</b> Next steps</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#research-design-variable-selection-etc."><i class="fa fa-check"></i><b>13.9.1</b> Research design, variable selection, etc.</a></li>
<li class="chapter" data-level="13.9.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#non-linear-models"><i class="fa fa-check"></i><b>13.9.2</b> Non-linear models</a></li>
<li class="chapter" data-level="13.9.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#other-data-distributions"><i class="fa fa-check"></i><b>13.9.3</b> Other data distributions</a></li>
<li class="chapter" data-level="13.9.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#multivariate-analyses"><i class="fa fa-check"></i><b>13.9.4</b> Multivariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#references-10"><i class="fa fa-check"></i><b>13.10</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="http://www.santiagobarreda.com" target="blank">Written by Santiago Barreda</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="c3" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Fitting Bayesian regression models with <em>brms</em><a href="c3.html#c3" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter we’re going to start answering basic research questions with Bayesian regression models using the <code>brms</code> package in R. The model we’ll use initially is not ‘correct’ for our data, but it’s simple enough to work as an introduction to Bayesian regression models. In the next chapter we’ll use <code>brms</code> to build models that are closer to ‘correct’ given the structure of our data. Before using a Bayesian regression model to investigate our data, we’ll explain what we mean by <em>regression model</em> and what specifically makes the models in this book <em>Bayesian</em>. We’ll leave a discussion of the ‘multilevel’ aspect of our models for the following chapter.</p>
<p>Throughout this book, we will present formal descriptions of our models. The relationship between statistical concepts and the formal notation used to represent them is very similar to the ability to play music and read musical notation. Someone who can play a song undoubtedly <em>knows</em> that song. However, in the absence of formal musical training that same person might not recognize sheet music representing the song. This person may also lack the vocabulary to discuss components of the song, and may find it difficult to learn to play new pieces without instruction. In the same way, most people have an excellent intuitive understanding of many statistical concepts (to be discussed in upcoming chapters) such as slopes, interactions, error, and so on. However, they may lack the knowledge of formal statistical notation that enables a deeper understanding of these concepts, and the ability to generalize these concepts to new situations. As a result, learning to read/write this notation will help you describe your models efficiently, and understand the models used by other people more effectively.</p>
<div id="chapter-pre-cap-2" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Chapter pre-cap<a href="c3.html#chapter-pre-cap-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter we introduce the concept of a regression model and explain what makes a regression model ‘Bayesian’. We discuss posterior probabilities, and also <code>brms</code>, an R package that can be used to fit Bayesian regression models using the Stan programming language. After this, we fit an ‘intercept only’ Bayesian regression model to our experimental data. We present topics related to model fitting such as thinning, chains, and iterations, and also discuss the specification of prior probabilities. After that, we outline the interpretation of the <code>brm</code> model print statement, and introduce the concepts of errors and residuals. Finally, we provide information regarding how to work directly with the model posterior samples, and discuss the log prior and posterior densities of our models.</p>
</div>
<div id="c3-what-is-reg" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> What are regression models?<a href="c3.html#c3-what-is-reg" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It’s difficult to offer a precise definition for <strong>regression</strong> because the term is so broad, but regression models are often models that help you understand systematic variation in the mean parameter (<span class="math inline">\(\mu\)</span>) of a normal distribution. Actually, you can model variation in other parameters and use a variety of other probability distributions, and we will do so later in this book. However, for now we’ll focus on models based on the normal distribution. Here is a general summary of the concepts underlying a regression that assumes normally-distributed errors:</p>
<ul>
<li><p>You have a variable you are interested in, <span class="math inline">\(y\)</span>, which is a vector containing <span class="math inline">\(n\)</span> observations. We can refer to any one of these observations like this: <span class="math inline">\(y_{[i]}\)</span> for the <span class="math inline">\(i^{th}\)</span> observation. Although it’s a bit atypical and, strictly speaking, not necessary, we’re going to put the index variables associated with trial number (<span class="math inline">\(i\)</span>) in brackets like this <span class="math inline">\(y_{[i]}\)</span>. This is just to make it easier to identify, and to highlight the similarity to vectors in R (e.g., <code>mens_height[i]</code>).</p></li>
<li><p>You assume that the random variation in your data is well described by a normal probability distribution centered at <span class="math inline">\(\mu\)</span>. This is a mathematical function (<span class="math inline">\(\mathrm{N}(\mu,\sigma)\)</span>) that describes what is and is not probable based on two parameters. You also assume that the random variation in each observation is independent from the random variation in each other observation. This means, for example, that you can’t really say why any two observations are above or below the mean.</p></li>
<li><p>The mean of this distribution is either fixed, or varies in a systematic manner. The standard deviation of the error distribution is usually fixed, but can vary (more on this in chapter 8).</p></li>
<li><p>The variation in the mean of this distribution can be understood using some predictor variables, and regression is a tool for modeling these relations.</p></li>
</ul>
<p>We can write our model more formally as in equation <a href="c3.html#eq:3-1">(3.1)</a>. This says that we think the tokens of our dependent variable (<span class="math inline">\(y\)</span>) are distributed according to (<span class="math inline">\(\sim\)</span>) a normal distribution with parameters equal to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Notice that <span class="math inline">\(y\)</span> gets a subscript while <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> do not. That means that we are modeling all of the observations with fixed parameters, while the value of <span class="math inline">\(y\)</span> changes for each observation based on the <span class="math inline">\(i\)</span> subscript.</p>
<p><span class="math display" id="eq:3-1">\[
\begin{equation}
y_{[i]} \sim \mathrm{N}(\mu,\sigma)
\tag{3.1}
\end{equation}
\]</span></p>
<p>Equation <a href="c3.html#eq:3-1">(3.1)</a> formalizes the fact that we think the <em>shape</em> of our data distribution will be like that of a normal distribution with a mean equal to <span class="math inline">\(\mu\)</span> and a standard deviation equal to <span class="math inline">\(\sigma\)</span>. When you see this, <span class="math inline">\(\mathrm{N}(\mu,\sigma)\)</span>, picture in your mind the shape of a normal distribution just like if you see this <span class="math inline">\(y=x^2\)</span> you may imagine a parabola. <span class="math inline">\(\mathrm{N}(\mu,\sigma)\)</span> really just represents the shape of the normal distribution and associated expectations about more and less probable outcomes. The above relationship can also be presented as in equation <a href="c3.html#eq:3-2">(3.2)</a>.</p>
<p><span class="math display" id="eq:3-2">\[
\begin{equation}
y_{[i]} = \mu + \mathrm{N}(0,\sigma)
\tag{3.2}
\end{equation}
\]</span></p>
<p>Notice that we got rid of the <span class="math inline">\(\sim\)</span> symbol, moved <span class="math inline">\(\mu\)</span> out of the distribution function, and that the mean of the distribution function is now 0. This representation breaks up our variable into two components:</p>
<ol style="list-style-type: decimal">
<li><p>A systematic component, systematic variation in the <em>expected value</em> <span class="math inline">\(\mu\)</span> of the variable <span class="math inline">\(y\)</span>.</p></li>
<li><p>A random component, the <em>error</em> <span class="math inline">\(\mathrm{N}(0,\sigma)\)</span>, that causes unpredictable variation around the expected value, often <span class="math inline">\(\mu\)</span>.</p></li>
</ol>
<p>Regression models separate observed variables into their <strong>systematic</strong> and <strong>random</strong> components. In this case, the systematic component is predictable for all observations in the data. The random component represents the <em>noise</em>, or <em>error</em> in our data, random variation around our expected values that isn’t explained. This doesn’t mean that it’s inexplicable in general, it only means that we’ve structured our model in a way that doesn’t let us explain it. In other words, a model like this thinks all variations from the mean are noise because it is structured in such a way that treats such deviations as noise.</p>
<p>In regression models, we can try to understand variation in <span class="math inline">\(\mu\)</span> using predictor variables <span class="math inline">\(x\)</span>. These predictor variables co-vary (vary with) our <span class="math inline">\(y\)</span> variable, and, we think (or hope), help explain the variation in <span class="math inline">\(y\)</span>. For example, in <a href="c3.html#eq:3-3">(3.3)</a> we’re saying we think <span class="math inline">\(\mu\)</span> is equal to some combination of the three predictor variables <span class="math inline">\(x_{1}\)</span>, <span class="math inline">\(x_{2}\)</span>, and <span class="math inline">\(x_{3}\)</span>. For example, we might expect that the apparent height of a speaker is affected by their fundamental frequency (<span class="math inline">\(x_{1}\)</span>), vocal-tract length (<span class="math inline">\(x_{2}\)</span>), and resonance (<span class="math inline">\(x_{1}\)</span>). So, when we combine these predictors we think we can come up with a pretty good estimate of how tall someone will sound.</p>
<p><span class="math display" id="eq:3-3">\[
\begin{equation}
\mu = x_{1} + x_{2} + x_{3}
\tag{3.3}
\end{equation}
\]</span></p>
<p>The values of the predictor variables will vary across observations, and are not fixed. In fact, often the whole point of running an experiment is to predict differences in observations based on differing predictor values. If we expect our predictors to vary from trial to trial, that means that the equation above should include <span class="math inline">\(i\)</span> subscripts indicating that the equation refers to the value of the predictors <em>for that trial</em> rather than overall. If we expect the predictors to vary across observations, naturally it’s possible that <span class="math inline">\(\mu\)</span> may take on different values from trial to trial, and it therefore also needs an <span class="math inline">\(i\)</span> subscript. This update is reflected in equation <a href="c3.html#eq:3-4">(3.4)</a>.</p>
<p><span class="math display" id="eq:3-4">\[
\begin{equation}
\mu_{[i]} = x_{1[i]} + x_{2[i]} + x_{3[i]}
\tag{3.4}
\end{equation}
\]</span></p>
<p>The predicted value (<span class="math inline">\(\mu_{[i]}\)</span>) for a given trial is very unlikely to be an equal combination of the predictors (as in equation <a href="c3.html#eq:3-4">(3.4)</a>), so that a <em>weighting</em> of the predictors will be necessary. We can use the symbol <span class="math inline">\(\alpha\)</span> for these weights as in equation <a href="c3.html#eq:3-5">(3.5)</a>. For example, maybe <span class="math inline">\(x_{1}\)</span> is twice as important as the other two predictors and so <span class="math inline">\(\alpha_1=2\)</span>, while <span class="math inline">\(\alpha_2=1\)</span> and <span class="math inline">\(\alpha_3=1\)</span>. Actually, maybe one predictor has a <em>negative</em> effect so that <span class="math inline">\(\alpha_3=-1\)</span>. The ‘weights’ associated with each predictor are the <strong>coefficients</strong> (or parameters) of our model. Note that the weight terms (<span class="math inline">\(\alpha\)</span>) do not get an <span class="math inline">\(i\)</span> subscript. This is because they do not change from trial to trial. The values of the <em>predictors</em> change from trial to trial, but the way that they are combined to produce an expected value for any given observation is a stable property of the model.</p>
<p><span class="math display" id="eq:3-5">\[
\begin{equation}
\mu_{[i]} = \alpha_1 \cdot x_{1[i]} + \alpha_2 \cdot x_{2[i]} + \alpha_3 \cdot x_{3[i]}  
\tag{3.5}
\end{equation}
\]</span></p>
<p>We can insert equation <a href="c3.html#eq:3-5">(3.5)</a> into equation <a href="c3.html#eq:3-2">(3.2)</a>, resulting in equation <a href="c3.html#eq:3-6">(3.6)</a>. At this point our model consists of an average value that has been broken up into three component parts and the random component represented by normally-distributed noise.</p>
<p><span class="math display" id="eq:3-6">\[
\begin{equation}
y_{[i]} =  (\alpha_1 \cdot x_{1[i]} + \alpha_2 \cdot x_{2[i]} + \alpha_3 \cdot x_{3[i]} ) + \mathrm{N}(0,\sigma)  
\tag{3.6}
\end{equation}
\]</span></p>
<p>Often, <span class="math inline">\(\varepsilon\)</span> is used to represent the random component, the error term, as in equation <a href="c3.html#eq:3-7">(3.7)</a>. Notice that the error term <em>does</em> get an <span class="math inline">\(i\)</span> subscript, as in <span class="math inline">\(\varepsilon_{[i]}\)</span>. This is because the exact value of the error <em>does</em> change from trial to trial, even though the statistical characteristics of the error (i.e., <span class="math inline">\(\mathrm{N}(0,\sigma)\)</span>) do not. This is a typical way to express a <strong>regression equation</strong> or a <em>regression model</em>. <strong>Fitting</strong> a regression model to data basically consists of finding the ‘best’ values of its coefficients, <span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, and <span class="math inline">\(\alpha_3\)</span> and the parameter governing the error, <span class="math inline">\(\sigma\)</span>, given our data and model structure.</p>
<p><span class="math display" id="eq:3-7">\[
\begin{equation}
y_{[i]} = \alpha_1 \cdot x_{1[i]} + \alpha_2 \cdot x_{2[i]} + \alpha_3 \cdot x_{3[i]}+ \varepsilon_{[i]}
\tag{3.7}
\end{equation}
\]</span></p>
<p>Notice that according to equation <a href="c3.html#eq:3-6">(3.6)</a>, regression models do not require that our <em>data</em> (<span class="math inline">\(y\)</span>) be normally distributed, but only that the <em>random error</em> in our data (<span class="math inline">\(\varepsilon\)</span>) be normally distributed. In <a href="c3.html#eq:3-8">(3.8)</a>, we see the sort of representation of our model that we will use in this book.</p>
<p><span class="math display" id="eq:3-8">\[
\begin{equation}
\begin{split}
y_{[i]} \sim \mathrm{N}(\mu,\sigma)\\
\mu_{[i]} = \alpha_1 \cdot x_{1[i]} + \alpha_2 \cdot x_{2[i]} + \alpha_3 \cdot x_{3[i]}
\end{split}
\tag{3.8}
\end{equation}
\]</span>
This representation presents the random and systematic components of our regression model separately, and it clearly and succinctly describes the structure of our model. In plain English this is:</p>
<blockquote>
<p>“Our observations are expected to be randomly distributed around the mean value according to a normal distribution with a standard deviation equal to sigma (<span class="math inline">\(\sigma\)</span>), and a mean of mu (<span class="math inline">\(\mu\)</span>). We expect the mean of our variable to vary from trial to trial based on three predictors. The combination of these predictors is based on model-specific coefficients (<span class="math inline">\(\alpha_1,\alpha_2,\alpha_3\)</span>) that are static across trials.”.</p>
</blockquote>
<p>By the way, the combination of parameters we use is a <strong>linear combination</strong>, one in which terms (<span class="math inline">\(x\)</span>) are added together after being multiplied by a real number (<span class="math inline">\(a\)</span>). The fact that our regression models make predictions using linear combinations is what makes them <strong>linear regression models</strong>. We will be focusing entirely on linear regression models in this book.</p>
</div>
<div id="c3-whats-bayes" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> What’s ‘Bayesian’ about these models?<a href="c3.html#c3-whats-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In one common ‘traditional’ approach to statistics, model parameters are estimated by trying to find the values that maximize the value of likelihood functions based on a theoretical probability distribution of interest, and given a particular model structure (i.e., maximum-likelihood estimation, as discussed in Chapter 2). We’re not going to discuss this approach to statistical inference in any detail as there are hundreds, if not thousands, of books available on the subject (for recommended readings, see the preface). Rather than dwell on these ‘traditional’ approaches to statistical inference, we’re going to talk about what makes Bayesian inference <em>Bayesian</em>, focusing on practical rather than ‘philosophical’ differences.</p>
<p>Instead of estimating parameters using only information from likelihood functions (and data), Bayesian models estimate the <strong>posterior probabilities</strong> of parameters. To explain what posterior probabilities are, we need to talk about joint probabilities again. We’ll begin by stating something obvious: The probability of events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurring is the same as the probability of <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> occurring.</p>
<p><span class="math display" id="eq:3-9">\[
\begin{equation}
P(A \,\&amp;\, B) = P(B \,\&amp;\, A)
\tag{3.9}
\end{equation}
\]</span></p>
<p>Equation <a href="c3.html#eq:3-9">(3.9)</a> can be reformulated as in <a href="c3.html#eq:3-10">(3.10)</a> (see section <a href="c2.html#c2-joint">2.3.2</a>).</p>
<p><span class="math display" id="eq:3-10">\[
\begin{equation}
P(A|B) \cdot P(B) = P(B|A) \cdot P(A)
\tag{3.10}
\end{equation}
\]</span></p>
<p>Recall that <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span> are simply placeholders for the probability of some event. We can replace these with <span class="math inline">\(P(y)\)</span> and <span class="math inline">\(P(\mu)\)</span>, which represent “the probability of observing your data” and “the probability of observing a certain parameter value” respectively. This is seen in equation <a href="c3.html#eq:3-11">(3.11)</a>, which states that “the probability of the parameter and the data is the same as the probability of the data and parameter”.</p>
<p><span class="math display" id="eq:3-11">\[
\begin{equation}
P(\mu|y) \cdot P(y) = P(y|\mu) \cdot P(\mu)
\tag{3.11}
\end{equation}
\]</span></p>
<p>We can isolate <span class="math inline">\(P(\mu|y)\)</span> on the left hand side by dividing both sides by <span class="math inline">\(P(y)\)</span>, resulting in equation <a href="c3.html#eq:3-12">(3.12)</a>. When structured in this way, this is a common formulation of <strong>Bayes theorem</strong>, and it underlies Bayesian statistical inference. Note however, that all we did was state a basic principle of probability theory (equation <a href="c3.html#eq:3-9">(3.9)</a>) and then expand and re-arrange some terms.</p>
<p><span class="math display" id="eq:3-12">\[
\begin{equation}
P(\mu|y) = \frac{P(y|\mu) \cdot P(\mu)}{P(y)}
\tag{3.12}
\end{equation}
\]</span></p>
<p>Each of the components in <a href="c3.html#eq:3-12">(3.12)</a> has a name:</p>
<ul>
<li><p><span class="math inline">\(P(\mu)\)</span> is the <strong>prior probability</strong> of the parameter <span class="math inline">\(\mu\)</span>. This is the <em>a priori</em> (before observation) probability of parameter values independent of the data <span class="math inline">\(y\)</span>. This a priori expectation can come from world knowledge, previous experiments, common sense, or some combination thereof. For example, before you measure the height of adults in San Francisco, you know the average is not 4 feet and it is not 7 feet.</p></li>
<li><p><span class="math inline">\(P(y|\mu)\)</span> is the likelihood, discussed at length in chapter 2. This is the product of the probability density values corresponding to the data points for a given value of <span class="math inline">\(\mu\)</span>. The likelihood reflects the probability that the data, <span class="math inline">\(y\)</span>, would be observed or generated for particular values of <span class="math inline">\(\mu\)</span>. As a result, the likelihood tells us about the distribution of possible/credible parameter values given the data and probability model.</p></li>
<li><p><span class="math inline">\(P(\mu|y)\)</span> is the <strong>posterior probability</strong> of the parameter given the data, the probability model, and the prior. This is the <strong>a posteriori</strong> (after observation) probability that the parameter is <span class="math inline">\(\mu\)</span> given your data <span class="math inline">\(y\)</span>, and the structure of your model. You get the posterior probability by combining the prior distribution and the likelihood, and in doing so incorporating your current observations into your prior beliefs.</p></li>
<li><p><span class="math inline">\(P(y)\)</span> is the <strong>marginal probability</strong> of the data. This is necessary to scale the numerator so that the posterior density has a total area under the curve equal to one. However, note that the marginal probability does not vary as a function of <span class="math inline">\(\mu\)</span>. As a result, this does not affect the relative posterior probability of different values of <span class="math inline">\(\mu\)</span>. For this reason, and for computational reasons described later, you don’t typically need to worry about the marginal probability. In fact, you can sample from the posterior even if you don’t calculate the denominator of <a href="c3.html#eq:3-12">(3.12)</a>.</p></li>
</ul>
<p>As noted above, ‘traditional’ models focus primarily (or exclusively) on how <em>likely</em> different conclusions are given your data. In contrast, Bayesian models focus on the posterior probability of different parameter values, that is on the combination of the likelihood and prior probabilities of parameters.</p>
<div id="c3-priors" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Prior probabilities<a href="c3.html#c3-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a Bayesian model, every parameter whose value is being estimated needs a prior probability distribution to be specified. For example, imagine you are interested in estimating the mean of a set of values, <span class="math inline">\(\mu\)</span>. You decide to use a Bayesian model and decide that <span class="math inline">\(\mu\)</span> will have a normal prior with a mean and standard deviation equal to <span class="math inline">\(\mu_{prior}\)</span> and <span class="math inline">\(\sigma_{prior}\)</span>. In other words, the prior distribution of <span class="math inline">\(P(\mu)\)</span> in <a href="c3.html#eq:3-12">(3.12)</a> is <span class="math inline">\(\mathrm{N}(\mu_{prior},\sigma_{prior})\)</span> such that <span class="math inline">\(\mu \sim \mathrm{N}(\mu_{prior},\sigma_{prior})\)</span>. To estimate this model you would need to provide fixed values for <span class="math inline">\(\mu_{prior}\)</span> and <span class="math inline">\(\sigma_{prior}\)</span>, for example we could use <span class="math inline">\(\mu_{prior}=3\)</span> and <span class="math inline">\(\sigma_{prior}=5\)</span> so that <span class="math inline">\(\mu \sim \mathrm{N}(3, 5)\)</span>. Note that the parameters of the prior do not get prior distributions themselves. This is because we are not <em>estimating</em> values of <span class="math inline">\(\mu_{prior}\)</span> and <span class="math inline">\(\sigma_{prior}\)</span>, but are instead setting them to fixed values such as 3 and 5. Only estimated parameters need priors.</p>
<p>The use of prior probabilities is sometimes said to make Bayesian models inherently ‘subjective’ but this concern is a bit overblown in most situations. First, when there is plenty of data (as is often the case for repeated measures data), prior probabilities have little to no effect on outcomes. This is because when you have many observations, the posterior probability of parameters is dominated by the likelihood (as will be discussed in <a href="c3.html#c3-posterior">3.3.2</a>). Second, a researcher will always use ‘common sense’ (i.e. their prior expectations) to interpret their data.</p>
<p>For example, if a listener was asked to judge apparent height and reported that all adult males were 90 cm tall, a researcher would have to wonder if this subject understands height in centimeters or if they were carrying out the experiment in good faith. So, even when they do not explicitly assign prior probabilities to parameter values, researchers still often use their expectations to ‘screen’ results in a manner broadly consistent with the use of prior probabilities in Bayesian reasoning. A Bayesian model requires you to build your expectations into your model. It <em>formalizes</em> them, makes them <em>definable</em> and <em>replicable</em>.</p>
<p>Finally, every model involves arbitrary decisions which can substantially affect our results so that the design of a model can never be said to be strictly ‘objective’. As a result, there is no particular reason to worry about the objectivity involved in establishing a prior in Bayesian modeling without also worrying about the objectivity involved in model building more generally.</p>
</div>
<div id="c3-posterior" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Posterior distributions<a href="c3.html#c3-posterior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The calculation of posterior distributions involves the combination of the likelihood function with the prior probability distribution and the marginal probability. The marginal probability does not affect the ‘shape’ of the posterior distribution, and exists to scale the posterior so that the area under the curve is equal to one (to satisfy the requirements of probability theory). As a result, we will focus on the combination of the likelihood and prior probabilities.</p>
<p>The combination of the likelihood and prior probability density functions is straightforward conceptually: You multiply the values of the two functions (i.e. curves) at each x-axis location. This works because we are interested in the <em>joint</em> probability of the likelihood and the prior, reformulated as the product of a conditional probability (the likelihood) and a marginal probability (the prior). The resulting curve represents the <strong>joint density</strong> of the two distributions. In figure <a href="c3.html#fig:F3-1">3.1</a>, several likelihoods and priors are combined, showing the effects of variations in priors, and the number of observations, on posterior distributions.</p>
<div class="figure"><span style="display:block;" id="fig:F3-1"></span>
<img src="_main_files/figure-html/F3-1-1.jpeg" alt="Demonstration of the effect of different prior probabilities and number of observations on resulting posterior distributions. In each case, the posterior is the product of the likelihood and the prior. All curves have been scaled to have the same height in the figure. This makes the figures visually interpretable but does not affect any of the points made in the discussion." width="4800" />
<p class="caption">
Figure 3.1: Demonstration of the effect of different prior probabilities and number of observations on resulting posterior distributions. In each case, the posterior is the product of the likelihood and the prior. All curves have been scaled to have the same height in the figure. This makes the figures visually interpretable but does not affect any of the points made in the discussion.
</p>
</div>
<p>The different standard deviations used for the prior probabilities of <span class="math inline">\(\mu\)</span> encode different levels of prior belief regarding expected apparent heights for adult male speakers. Researchers often distinguish between different types of priors based on how much they are expected to affect conclusions. For example, priors are often referred to as <strong>vague</strong>, <strong>weakly informative</strong>, or <strong>informative</strong>. The boundaries between these categories are fuzzy, and the ‘informativeness’ of a prior is more continuous than discrete, therefore, we will discuss these categories in terms of how they might differently affect your models. It’s important to note however, that there is no generally accepted definition for these terms and what follows is just an informal guide to thinking about these categories of priors.</p>
<p>Recall that the ‘width’ of the likelihood depends on the amount of underlying variability and the sample size. The underlying variability in a measurement or parameter determines the underlying ‘width’ of the likelihood. However, as the sample size increases the likelihood becomes narrower and narrower with respect to the underlying variation in the measurement/parameter (see <a href="c2.html#c2-chars-of-likelihoods-2">2.7.3</a>). As a result, a prior that is wider than the underlying variability in the measurement has no chance of meaningfully affecting outcomes given even only a small number of observations. This is shown in the top row of figure <a href="c3.html#fig:F3-1">3.1</a>, where even three observations can overwhelm a very broad prior. As a result, we will be thinking of the ‘informativeness’ of priors with respect to the variation we expect in them a priori, with priors going from <em>vague</em> to <em>informative</em> as they become smaller than the expected random variation.</p>
<p>Each column of figure <a href="c3.html#fig:F3-1">3.1</a> differs in terms of the number of observations used to calculate the likelihood (n = 3, 10, 675), and rows differ in terms of the standard deviation of the prior probabilities of the mean with <span class="math inline">\(\mu\)</span> equal to 180 cm and <span class="math inline">\(\sigma\)</span> = 100, 15, 1. In each case, the calculation of the likelihood assumes that the data has the same standard deviation as our apparent height data (7.8 cm). Note that the yellow curve (the likelihood) is the same in each column. What changes is that priors become narrower across rows top to bottom, and therefore have an increasing effect on posterior densities.</p>
<p>In the middle row of Figure <a href="c3.html#fig:F3-1">3.1</a> we see the influence of a <em>weakly informative</em> prior. This prior is a normal distribution with a mean of 180 and a standard deviation of 15 (i.e. <span class="math inline">\(P(\mu)=\mathrm{N}(180,15)\)</span>). The standard deviation was set to twice the value of the standard deviation of adult male heights in the United States. As a result, it places reasonable constraints on our expectations but it will not strongly influence results within that reasonable range. We see that in this case, the likelihood still dominates the posterior even when only three observations are available.</p>
<p>In the top row of Figure <a href="c3.html#fig:F3-1">3.1</a> we see the influence of a <em>vague</em> or <em>diffuse</em> prior on inference. Since this distribution has a standard deviation of 100 and a mean of 180, this means that basically all average heights from zero to 400 cm are plausible a priori. A prior probability this wide is only going to place very minimal constraints on the posterior probabilities that fall inside this range. This is reflected in the top row of figure <a href="c3.html#fig:F3-1">3.1</a> where even in the case of only three observations the posterior probability almost exactly matches the likelihood. So, in the presence of very weak prior beliefs, the most credible values for your parameters <em>a posteriori</em> (after incorporating your data) will be dominated by the most likely values. This makes sense.</p>
<p>In the bottom row of Figure <a href="c3.html#fig:F3-1">3.1</a> we see the influence of a <em>informative</em> prior. This distribution has a standard deviation of 1 and a mean of 180. A prior distribution this narrow basically says that we are <em>sure</em> that the average height is around 180 cm. This is because the prior probability of any value outside of 178-182 cm is nearly zero, meaning that we will be very hesitant to believe values outside this bound before collecting our data. We can see that under these conditions, the prior distribution can actually have a strong effect on the posterior, especially in cases with few observations. However, with a large enough sample size the likelihood can still come to dominate even the narrowest prior distributions.</p>
</div>
<div id="c3-characteristics-posteriors" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Posterior distributions and shrinkage<a href="c3.html#c3-characteristics-posteriors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we focus on the general characteristics of posterior distributions with respect to the characteristics of priors and likelihoods, a pattern emerges. Specifically, the posterior is a mix of the prior and likelihood which takes their relative ‘variances’ into account. We know that the width of the likelihood function is dependent on the underlying error in the data and on the sample size. Greater error increases the width of the likelihood, since a wider spread of data will be compatible with a wider range of parameter values. On the other hand, more observations will tend to decrease the width of the likelihood, since it consists of the product of density values across all the observations in a data set. So, we see that the characteristics of the posterior distribution will depend simultaneously on the ‘width’ of the prior distribution, the amount of noise in your dependent variable, and the size of the sample involved in the calculation of your likelihood.</p>
<p>A consequence of the ‘merging’ of prior information with the likelihood is that posterior distributions can be <strong>shrunk</strong> towards the prior. Recall from chapter 2 that the maximum likelihood estimate of a parameter can be thought of as the ‘best’ parameter in that it is the parameter that makes your data as probable as it can be given the model structure. In figure <a href="c3.html#fig:F3-1">3.1</a> we see that in some cases the posterior distribution is not exactly like the likelihood and has been <em>pulled</em> towards the prior. This means that the maximum <em>a posteriori</em> parameter value is not necessarily equal to the <em>maximum likelihood</em> parameter value.</p>
<p>The <em>pull</em> exerted by priors is referred to as <strong>shrinkage</strong>, because it tends to <em>shrink</em> the magnitude of effects by pulling them towards the prior mean (which is often zero). Broadly speaking, deviations from prior expectations are maintained when there is good enough evidence for them, and shrunk when there is not. What constitutes ‘enough’ evidence is based on the structure of the model and the nature of the data. As seen in figure <a href="c3.html#fig:F3-1">3.1</a>, a wide enough prior will not meaningfully affect estimates even for extremely small sample sizes. You may wonder, is shrinkage a good thing? It turns out that shrinkage can help models arrive at more reliable parameter estimates by reducing weakly supported values that deviate substantially from prior expectations. This will be discussed further in chapter 4.</p>
</div>
</div>
<div id="c3-sampling" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Sampling from the posterior using <em>Stan</em> and <em>brms</em><a href="c3.html#c3-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We want to understand the posterior distribution of parameters. How do we get this information? For very simple models, posterior distributions can be derived <strong>analytically</strong>, that is by finding exact solutions to a series of equations. However, for more complicated models, such as the ones we’ll discuss in this book, it can be difficult (if not impossible) to understand the characteristics of the posterior distributions of parameters in this way. As a result, these questions are answered <strong>numerically</strong> using software that samples from posterior distributions using algorithms designed to do so as accurately and efficiently as possible. Given enough samples from the posterior distribution, we can estimate the properties of the distribution (just as we could by sampling the distribution of other variable).</p>
<p>Many different software approaches to sampling from posterior distributions have been developed through the years including <em>winBUGS</em>, <em>JAGS</em>, <em>PyMC</em>, and <em>Stan</em>. The software used in this book is <strong>Stan</strong>. We use this because it is (relatively) fast, reliable, extremely flexible, and widely adopted. However, the modeling and statistical principles explained in this book apply to all Bayesian models regardless of the software used to fit them, and the central concepts extend to linear modeling more generally.</p>
<p>One downside with working with <em>Stan</em> directly is that you need to write your own models. This is not too difficult, but it is not particularly <em>easy</em> either, and it can be a bit time consuming, especially for complicated models. In this book we will rely on the <code>brms</code> package to use <em>Stan</em>. <code>brms</code> simplifies the use of <em>Stan</em> by making the specification of highly-efficient models very simple, and providing us with a great deal of flexibility in doing so. It also includes many helper functions that make working with Bayesian models very convenient. So, even though we will use <code>brms</code> for simplicity, the topics we discuss in this book could be used to directly write your own models for <em>Stan</em> (or any other statistical software).</p>
<p>In order to sample from the posterior distribution using software like <em>Stan</em>, the user provides data and a description of a model which specifies:</p>
<ul>
<li><p>The relations between the variables in the data. For example, what is the dependent variable? What are the independent variables? How do these relate?</p></li>
<li><p>The nature of random variation in the model. To this point we have only discussed single sources of normally-distributed noise in our models.</p></li>
<li><p>Prior distributions for all <em>estimated</em> parameters.</p></li>
</ul>
<p>Given this information, <em>Stan</em> samples from the posterior distributions of your parameters and returns vectors containing these samples (one vector for each estimated parameter), rather than looking for the single ‘best’ estimate. The result of this process is a <strong>chain</strong> of samples from the posterior distribution for each of the parameters you are estimating in your model. Together, the samples in these chains tell us about the characteristics of the parameter they represent. Incredibly, under a very reasonable set of conditions, randomly sampling from the posterior distribution will result in a distribution of sampled values of parameters (e.g. <span class="math inline">\(\mu\)</span>) that will converge on the posterior distribution of the parameter given your data and model structure (including prior probabilities).</p>
</div>
<div id="c3-estimating" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Estimating a single mean with the <code>brms</code> package<a href="c3.html#c3-estimating" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="c3-data-qs-1" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Data and Research Questions<a href="c3.html#c3-data-qs-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’re going to use the same experimental data we looked at last chapter: The height judgments collected for the adult male speakers in our experiment. For more information on the experiment, see section <a href="c1.html#c1-methods">1.3.2</a>. Below we load the book package and extract only those trials including adult male speakers. In addition, we will focus only on the natural productions (contained in <code>exp_data</code>), excluding those trials involving the manipulated ‘big’ resonance level.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="c3.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load book package and brms</span></span>
<span id="cb43-2"><a href="c3.html#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb43-3"><a href="c3.html#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb43-4"><a href="c3.html#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="c3.html#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># load and subset experimental data</span></span>
<span id="cb43-6"><a href="c3.html#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb43-7"><a href="c3.html#cb43-7" aria-hidden="true" tabindex="-1"></a>men <span class="ot">=</span> exp_data[exp_data<span class="sc">$</span>C_v<span class="sc">==</span><span class="st">&#39;m&#39;</span>,]</span>
<span id="cb43-8"><a href="c3.html#cb43-8" aria-hidden="true" tabindex="-1"></a>mens_height <span class="ot">=</span> men<span class="sc">$</span>height</span></code></pre></div>
<p>We’re going to revisit the research questions posed at the beginning of Chapter 2:</p>
<p>(Q1) How tall does the average adult male sound?</p>
<p>(Q2) Can we set limits on credible average apparent heights based on the data we collected?</p>
<p>However, this time we’re going to approach these questions more formally using a Bayesian regression model using <code>brms</code> (and <em>Stan</em>).</p>
</div>
<div id="c3-description-1" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Description of the model<a href="c3.html#c3-description-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’re beginning with a model that treats all of our data as random deviations drawn from a single, undifferentiated normal distribution. Our model for a single group of normally distributed values can be thought of in several different ways. In <a href="c3.html#eq:3-13">(3.13)</a>, the value of your dependent variable for any given trial (<span class="math inline">\(y_{[i]}\)</span>) is thought of as being a normally-distributed variable with a constant mean of <span class="math inline">\(\mu\)</span>, and a fixed standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display" id="eq:3-13">\[
\begin{equation}
y_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma)
\tag{3.13}
\end{equation}
\]</span></p>
<p>Our trial-specific mean will be constant across trials for now, but we introduce the formalism of the trial-dependent mean (<span class="math inline">\(\mu_{[i]}\)</span>) now in <a href="c3.html#eq:3-14">(3.14)</a> as it will be useful in our discussion below. So, we can still present our variable as coming from a trial-specific value of <span class="math inline">\(\mu\)</span> even if we expect <span class="math inline">\(\mu_{[i]}=\mu_{[j]}\)</span> for all values of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> (some other number), i.e. that all observations share the same <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display" id="eq:3-14">\[
\begin{equation}
y_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma)
\tag{3.14}
\end{equation}
\]</span></p>
<p>We can also think of this model as in <a href="c3.html#eq:3-15">(3.15)</a>, which says that your dependent variable is the sum of some <em>expected value</em> for that trial, (<span class="math inline">\(\mu_{[i]}\)</span>) and some specific random error for that trial (<span class="math inline">\(\varepsilon_{[i]}\)</span>). The random error is expected to be normally distributed with a mean of 0 and some unknown standard deviation (as in: <span class="math inline">\(\varepsilon_{[i]} \sim \mathrm{N}(0,\sigma)\)</span>).</p>
<p><span class="math display" id="eq:3-15">\[
\begin{equation}
y_{[i]} = \mu_{[i]} + \varepsilon_{[i]}
\tag{3.15}
\end{equation}
\]</span></p>
<p>In general, we use regression models to understand orderly variation in <span class="math inline">\(\mu_{[i]}\)</span> from trial to trial by breaking it up into predictors (<span class="math inline">\(x_{1}, x_{2},...\)</span>) that are combined based on weights as determined by the model coefficients (e.g., <span class="math inline">\(\alpha_1, \alpha_2,...\)</span>). However, in this case we expect the value of <span class="math inline">\(\mu_{[i]}\)</span> to actually be equal for all trials. When we are only trying to estimate a single average, we don’t have any predictors to explain variation in <span class="math inline">\(\mu_{[i]}\)</span>. In fact, our model structure suggests we expect no variation in <span class="math inline">\(\mu_{[i]}\)</span> from trial to trial. However, mathematically we can’t just say ‘we have no predictor’ since everything needs to be represented by a number. As a result, we use a single ‘predictor’ <span class="math inline">\(x\)</span> with a value of 1 so that our regression equation is as in <a href="c3.html#eq:3-16">(3.16)</a>). Now, our model is trying to guess the value of a single coefficient (<span class="math inline">\(\alpha_1\)</span>), and we expect this coefficient to be equal to <span class="math inline">\(\mu_{[i]}\)</span> since it is being multiplied by a ‘predictor’ with a constant value of 1.</p>
<p><span class="math display" id="eq:3-16">\[
\begin{equation}
\mu_{[i]} = \alpha_1 \cdot 1
\tag{3.16}
\end{equation}
\]</span></p>
<p>This kind of model is called an <strong>Intercept only</strong> model. Regression models are really about representing <em>differences</em>, differences between groups and across conditions. When you are encoding differences, you need an overall reference point. For example, saying that something is ‘5 miles north’ is only interpretable given some reference point. The ‘reference point’ used by your model is called your ‘Intercept’, and it is the center of your model’s universe. At this point our model consists <em>only</em> of a single reference point, and the <span class="math inline">\(\alpha_1\)</span> parameter reflects its value (as shown in equation <a href="c3.html#eq:3-16">(3.16)</a>). As a result, the <span class="math inline">\(\alpha_1\)</span> coefficient is called the ‘Intercept’ in our model. When a coefficient is just being multiplied by a ‘fake’ predictor that always equals one, we can omit it from the regression model (but its still secretly there). So, our model investigating the apparent heights of adult males can be formalized like this:</p>
<p><span class="math display" id="eq:3-17">\[
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\
\mu_{[i]} = \mathrm{Intercept} \\
\end{split}
\tag{3.17}
\end{equation}
\]</span></p>
<p>Put in plain English, each line in the model says the following:</p>
<ul>
<li><p>We expect that apparent height for a given observation <span class="math inline">\(i\)</span> is normally distributed according to some trial-specific expected value and some unknown (but fixed) standard deviation.</p></li>
<li><p>The expected value for any given trial (<span class="math inline">\(\mu_{[i]}\)</span>) is equal to the intercept of the model for all trials. This means it’s fixed and we have the same expected value for all tokens.</p></li>
</ul>
</div>
<div id="c3-errors-and-residuals" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Errors and residuals<a href="c3.html#c3-errors-and-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our model above implicitly says that the error, the random variation around <span class="math inline">\(\mu\)</span>, is drawn from a normal distribution with a mean of 0 and a standard deviation of <span class="math inline">\(\sigma\)</span>. This distribution represents all deviations in apparent height around the mean apparent height for the sample (<span class="math inline">\(\mu_{[i]}\)</span>). In other words, the error for this model is expected to look like:</p>
<p><span class="math display" id="eq:3-18">\[
\begin{equation}
\varepsilon_{[i]} \sim \mathrm{N}(0,\sigma)
\tag{3.18}
\end{equation}
\]</span></p>
<p>We can rearrange the terms in <a href="c3.html#eq:3-15">(3.15)</a> to isolate the random term on the left side, as in <a href="c3.html#eq:3-19">(3.19)</a>. When we do this, we see that <strong>error</strong> is what we call the difference between the value of an observation and the expected value for that observation.</p>
<p><span class="math display" id="eq:3-19">\[
\begin{equation}
\varepsilon_{[i]} = y_{[i]} - \mu_{[i]}
\tag{3.19}
\end{equation}
\]</span></p>
<p>In practice, you never know the true expected value, the <em>real</em> exact parameter for whatever distribution you are working with. Instead, you work with an estimate of the predicted value <span class="math inline">\(\hat{\mu}\)</span>. As a consequence, you do not have access to the exact errors but instead to estimated errors <span class="math inline">\(\hat{\varepsilon}\)</span>, as seen in <a href="c3.html#eq:3-20">(3.20)</a>. Estimated errors are called <strong>residuals</strong>.</p>
<p><span class="math display" id="eq:3-20">\[
\begin{equation}
\hat{\varepsilon}_{[i]} = y_{[i]} - \hat{\mu}_{[i]}
\tag{3.20}
\end{equation}
\]</span></p>
<p>As noted in section <a href="c3.html#c3-what-is-reg">3.2</a>, regression models assume that the random error, the unpredictable deviations about the expected value across trials, are independent and identically distributed. We can now be more specific and say that we expect our <em>residuals</em> to be independent and identically distributed. This assumption is obviously violated for our experimental data since we have multiple observations from each listener, each of who had their own tendencies (as discussed in section <a href="c2.html#c2-conditional">2.3.1</a>). For this reason, we can say that this model is ‘wrong’; it’s built in such a way that we know it is not a good fit for our data. We will discuss this, and the problems it causes, in the following chapter.</p>
</div>
<div id="c3-model-formula" class="section level3 hasAnchor" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> The model formula<a href="c3.html#c3-model-formula" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model structures are expressed in R using a very specific syntax. Think of writing a model formula as a sub-language within R. Generally, model formulas in R have the form:</p>
<p><code>y ~ predictors</code></p>
<p>The variable we are interested in understanding (<code>y</code>) goes on the left hand side of the tilde (<code>~</code>) and our predictors go on the right hand side. Notice that information regarding the random term (<span class="math inline">\(\varepsilon\)</span>) is not included in the model formula. The formula above can be read as ‘y is distributed according to some predictor’, which really means “we think there is systematic variation in our y variable that can be understood by considering its joint variation with our predictor variable(s).”</p>
<p>For intercept-only models, the number <code>1</code> is included in the model formula to indicate that a single constant value is being estimated (as in <a href="c3.html#eq:3-16">(3.16)</a>). As a result, our model formula will have the form seen below.</p>
<p><code>height ~ 1</code></p>
<p>This model formula could be said out loud like “we are trying to estimate the mean height” or “we are predicting mean height given only an intercept”.</p>
</div>
<div id="c3-calling-brm" class="section level3 hasAnchor" number="3.5.5">
<h3><span class="header-section-number">3.5.5</span> Fitting the model: Calling the <em>brm</em> function<a href="c3.html#c3-calling-brm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>brms</code> package contains the <code>brm</code> (Bayesian regression models) function, which we will use to fit our models. The <code>brm</code> function takes a model specification, data, and some other information, and fits a model that estimates all the model parameters. Unless otherwise specified, <code>brm</code> assumes that the error component (<span class="math inline">\(\varepsilon\)</span>) of your model is normally distributed. The first argument in the function call below is the model formula, and the second argument tells the function where to find the data (a data frame called <code>men</code>). The other arguments tell the function to estimate a single set of samples (chains = 1) using a single core on your CPU (cores = 1). These arguments will be discussed in more detail in the next chapter.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="c3.html#cb44-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> men, <span class="at">chains =</span> <span class="dv">1</span>, <span class="at">cores =</span> <span class="dv">1</span>)</span>
<span id="cb44-2"><a href="c3.html#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="c3.html#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Compiling Stan program...</span></span>
<span id="cb44-4"><a href="c3.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Start sampling</span></span>
<span id="cb44-5"><a href="c3.html#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb44-6"><a href="c3.html#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="do">## SAMPLING FOR MODEL &#39;03859e54349182b6cd9cd51aa7ca25d3&#39; NOW (CHAIN 1).</span></span>
<span id="cb44-7"><a href="c3.html#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: </span></span>
<span id="cb44-8"><a href="c3.html#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Gradient evaluation took 0 seconds</span></span>
<span id="cb44-9"><a href="c3.html#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.</span></span>
<span id="cb44-10"><a href="c3.html#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Adjust your expectations accordingly!</span></span>
<span id="cb44-11"><a href="c3.html#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: </span></span>
<span id="cb44-12"><a href="c3.html#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: </span></span>
<span id="cb44-13"><a href="c3.html#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)</span></span>
<span id="cb44-14"><a href="c3.html#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)</span></span>
<span id="cb44-15"><a href="c3.html#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)</span></span>
<span id="cb44-16"><a href="c3.html#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)</span></span>
<span id="cb44-17"><a href="c3.html#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)</span></span>
<span id="cb44-18"><a href="c3.html#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)</span></span>
<span id="cb44-19"><a href="c3.html#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)</span></span>
<span id="cb44-20"><a href="c3.html#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)</span></span>
<span id="cb44-21"><a href="c3.html#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)</span></span>
<span id="cb44-22"><a href="c3.html#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)</span></span>
<span id="cb44-23"><a href="c3.html#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)</span></span>
<span id="cb44-24"><a href="c3.html#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)</span></span>
<span id="cb44-25"><a href="c3.html#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1: </span></span>
<span id="cb44-26"><a href="c3.html#cb44-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1:  Elapsed Time: 0.103 seconds (Warm-up)</span></span>
<span id="cb44-27"><a href="c3.html#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1:                0.057 seconds (Sampling)</span></span>
<span id="cb44-28"><a href="c3.html#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="do">## Chain 1:                0.16 seconds (Total)</span></span></code></pre></div>
<p>By default, <code>brms</code> takes 2000 samples, throwing out the first 1000 and returning the last 1000. The first 1000 samples are the <strong>warmup</strong>, the time the model uses to find appropriate parameter values for the model, and to tune the behavior of the sampling algorithm. The output above shows you that the sampler is working, and tells you about the progress as it works. This is a small amount of data and a simple model so it should fit pretty quickly. You can fit the models discussed in this book on your own computer using the code provided, or you can download them directly from the book GitHub repo using the code below.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="c3.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the model above from the GitHub page.</span></span>
<span id="cb45-2"><a href="c3.html#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># File names are in the formant: &#39;chapterNumber_modelName&#39;</span></span>
<span id="cb45-3"><a href="c3.html#cb45-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;3_model.RDS&#39;</span>)</span></code></pre></div>
</div>
<div id="c3-interpreting-print" class="section level3 hasAnchor" number="3.5.6">
<h3><span class="header-section-number">3.5.6</span> Interpreting the model: The print statement<a href="c3.html#c3-interpreting-print" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Typing the model name into the console and hitting enter prints the default <code>brms</code> model print statement:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="c3.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect model</span></span>
<span id="cb46-2"><a href="c3.html#cb46-2" aria-hidden="true" tabindex="-1"></a>model</span></code></pre></div>
<p>The first part provides you with some basic information and tells you some technical details that we don’t have to worry about for now (though some are obvious).</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="c3.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Family: gaussian </span></span>
<span id="cb47-2"><a href="c3.html#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="do">##  Links: mu = identity; sigma = identity </span></span>
<span id="cb47-3"><a href="c3.html#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="do">##Formula: height ~ 1 </span></span>
<span id="cb47-4"><a href="c3.html#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   Data: men (Number of observations: 675) </span></span>
<span id="cb47-5"><a href="c3.html#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;</span></span>
<span id="cb47-6"><a href="c3.html#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="do">##         total post-warmup draws = 1000</span></span></code></pre></div>
<p>Next we see estimated effects for our predictors, in this case only an intercept. This is an estimated <strong>population-level effect</strong> because it is shared by all observations in our sample, and it is not specific to any particular subset of observations.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="c3.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects: </span></span>
<span id="cb48-2"><a href="c3.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb48-3"><a href="c3.html#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept   173.78      0.30   173.16   174.33 1.00     1055      714</span></span></code></pre></div>
<p>The information above provides the mean (<code>Estimate</code>) and standard deviation (<code>Est. Error</code>) of the posterior distribution of <span class="math inline">\(\mu\)</span> (<code>Intercept</code>), i.e. <span class="math inline">\(P(\mu | y)\)</span>. The values of <code>l-95% CI</code> and <code>u-95% CI</code> represent the lower and upper 95% <strong>credible interval</strong> of the posterior distribution. An <span class="math inline">\(x\%\)</span> credible interval of a parameter is an interval such that the parameter has an <span class="math inline">\(x\%\)</span> chance (<span class="math inline">\(0.x\)</span> probability) of falling inside the interval. The credible intervals provided by <code>brms</code> are based on quantiles so that the <code>l-95% CI</code> and <code>u-95% CI</code> represent 2.5% and 97.5% quantiles of the posterior samples of a parameter. Based on its 95% credible interval, we see that there is a 95% probability that <span class="math inline">\(\mu\)</span> is between 173.2 and 174.3 cm given our data and model structure.</p>
<p>Our model also provides us an estimate of the error standard deviation(<span class="math inline">\(\sigma\)</span>), under <code>Family Specific Parameters: sigma</code>. This estimate closely matches our sample standard deviation estimate of 7.77 cm. In addition, we also get a 95% credible interval for this parameter, spanning from 2.5% = 7.37 to 97.5% = 8.21. Although our focus is often on the estimation of mean parameters, it’s very important to keep in mind that our model in <a href="c3.html#eq:3-15">(3.15)</a> involves the estimation of <em>two</em> parameters: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="c3.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters: </span></span>
<span id="cb49-2"><a href="c3.html#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb49-3"><a href="c3.html#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma     7.77      0.22     7.37     8.21 1.00     1139      741</span></span></code></pre></div>
<p>This last section is just boilerplate and contains some basic reminders which will generally look the same.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="c3.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS</span></span>
<span id="cb50-2"><a href="c3.html#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="do">## and Tail_ESS are effective sample size measures, and Rhat is the potential</span></span>
<span id="cb50-3"><a href="c3.html#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="do">## scale reduction factor on split chains (at convergence, Rhat = 1).</span></span></code></pre></div>
</div>
<div id="c3-seeing-samples" class="section level3 hasAnchor" number="3.5.7">
<h3><span class="header-section-number">3.5.7</span> Seeing the samples<a href="c3.html#c3-seeing-samples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In section <a href="c3.html#c3-sampling">3.4</a>, we discussed that Bayesian modeling software (like <em>Stan</em>) takes <em>samples</em> of the posterior distributions of parameters given the data and model structure. It’s helpful to see that our model is really just a series of <strong>posterior samples</strong>, that is, samples of values from the distribution of <span class="math inline">\(\mu\)</span> given your data and priors, i.e. <span class="math inline">\(P(\mu | y)\)</span>.</p>
<p>Compact descriptions of our models, such as the one in the print described above, are just summaries of the information contained in the posterior samples. Below we get the posterior samples from the model we fit above, in the form of a data frame using the <code>get_samples</code> function from the <code>bmmb</code> package. As expected, we have 1000 samples of each parameter. The first column represents the model intercept (<code>b_Intercept</code>), the second column is the error (<code>sigma</code>) (i.e. <span class="math inline">\(P(\sigma | y)\)</span>). The third and fourth columns (<code>lprior</code>, <code>lp__</code>) are the log prior and log posterior densities, to be discussed in section <a href="c3.html#c3-log-posterior">3.8</a>.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="c3.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get posterior samples from model</span></span>
<span id="cb51-2"><a href="c3.html#cb51-2" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_samples</span> (model)</span>
<span id="cb51-3"><a href="c3.html#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="c3.html#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># check number of samples</span></span>
<span id="cb51-5"><a href="c3.html#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span> (samples)</span>
<span id="cb51-6"><a href="c3.html#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1000</span></span>
<span id="cb51-7"><a href="c3.html#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="c3.html#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># see first six samples</span></span>
<span id="cb51-9"><a href="c3.html#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (samples)</span>
<span id="cb51-10"><a href="c3.html#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="do">##   b_Intercept sigma lprior  lp__</span></span>
<span id="cb51-11"><a href="c3.html#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 1       173.2 7.518 -5.885 -2347</span></span>
<span id="cb51-12"><a href="c3.html#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 2       174.1 7.613 -5.880 -2345</span></span>
<span id="cb51-13"><a href="c3.html#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 3       173.3 7.940 -5.945 -2346</span></span>
<span id="cb51-14"><a href="c3.html#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 4       174.2 7.564 -5.872 -2346</span></span>
<span id="cb51-15"><a href="c3.html#cb51-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 5       173.7 7.879 -5.924 -2345</span></span>
<span id="cb51-16"><a href="c3.html#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 6       174.0 7.859 -5.916 -2345</span></span></code></pre></div>
<p>We can plot the individual samples for the intercept parameter on the left in figure <a href="c3.html#fig:F3-2">3.2</a>, and on the right we can see a histogram of the samples.</p>
<div class="figure"><span style="display:block;" id="fig:F3-2"></span>
<img src="_main_files/figure-html/F3-2-1.jpeg" alt="(left) Individual samples from the posterior distribution of the model intercept parameter. (right) A histogram of the samples on the left. The curve shows the density of a normal distribution with a mean of 173.8 and a standard deviation of 0.30." width="4800" />
<p class="caption">
Figure 3.2: (left) Individual samples from the posterior distribution of the model intercept parameter. (right) A histogram of the samples on the left. The curve shows the density of a normal distribution with a mean of 173.8 and a standard deviation of 0.30.
</p>
</div>
<p>Recall that our model output provides information about 95% credible intervals for the mean parameter: It was expected to fall between 173.2 and 174.3 cm. We know that this interval simply corresponds to the 2.5% and 97.5% quantiles of the posterior samples. We can confirm this by checking the quantiles on the vector containing our posterior samples and see that these exactly correspond to the values of <code>Estimate</code>, <code>l-95% CI</code>, and <code>u-95% CI</code> in the model print statement above.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="c3.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span> (samples[,<span class="st">&quot;b_Intercept&quot;</span>], <span class="fu">c</span>(.<span class="dv">025</span>, .<span class="dv">975</span>))</span>
<span id="cb52-2"><a href="c3.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="do">##  2.5% 97.5% </span></span>
<span id="cb52-3"><a href="c3.html#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 173.2 174.3</span></span></code></pre></div>
<p>One of the great things about Bayesian models is that you can make your own summaries of the posterior samples, summarize them in several ways as required, and ask different questions easily. For example, there is no special status for the 2.5 and 97.5% quantiles, and we can easily check the values of other ones, such as the first and third quartiles:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="c3.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span> (samples[,<span class="st">&quot;b_Intercept&quot;</span>], <span class="fu">c</span>(.<span class="dv">25</span>, .<span class="dv">75</span>))</span>
<span id="cb53-2"><a href="c3.html#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="do">##   25%   75% </span></span>
<span id="cb53-3"><a href="c3.html#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 173.6 174.0</span></span></code></pre></div>
<p>We can also use the posterior distribution to find the probability that the mean parameter is over/under any arbitrary value:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="c3.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># probability that the intercept is less than 174 cm</span></span>
<span id="cb54-2"><a href="c3.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (samples[,<span class="st">&quot;b_Intercept&quot;</span>] <span class="sc">&lt;</span> <span class="dv">174</span>)</span>
<span id="cb54-3"><a href="c3.html#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.76</span></span></code></pre></div>
<p>Let’s take a second to think about why this works. Recall that the probability is the odds that something will occur, relative to all other outcomes. Our vector <code>samples[,"b_Intercept"]</code> represents 1000 observations of a random variable, 1000 possible values of the posterior distribution of the mean apparent height of adult males (<span class="math inline">\(P(\mu | y)\)</span>). If we find the total number of these observations that were below 174 cm and then divide by the total number of observations (1000), we are effectively calculating the probability of observing a mean estimate below 174 cm. As a result, the calculation above says that there is a 0.76 probability (a 76% chance) that the mean apparent height of adult male speakers in this population is under 174 cm, given our data and model structure. We come to this conclusion by finding that 76% of the posterior samples of the parameter of interest are below 174 cm.</p>
</div>
<div id="c3-getting-residuals" class="section level3 hasAnchor" number="3.5.8">
<h3><span class="header-section-number">3.5.8</span> Getting the residuals<a href="c3.html#c3-getting-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can get the model residuals using the <code>residuals</code> function. By default it returns a data frame where one row corresponds to each observation in your data, and the different columns provide you information about the estimate. You will notice that you also get credible intervals for each of the estimated residuals. This is because there is one prediction for each set of posterior samples. Since we have 1000 posterior samples that means we have 1000 slightly different predicted values for each observation and therefore 1000 slightly different estimated errors for each observation.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="c3.html#cb55-1" aria-hidden="true" tabindex="-1"></a>model_residuals <span class="ot">=</span> <span class="fu">residuals</span> (model, )</span>
<span id="cb55-2"><a href="c3.html#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (model_residuals)</span>
<span id="cb55-3"><a href="c3.html#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error     Q2.5    Q97.5</span></span>
<span id="cb55-4"><a href="c3.html#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]  -3.8844    0.2988  -4.4347  -3.2638</span></span>
<span id="cb55-5"><a href="c3.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]  -0.2844    0.2988  -0.8347   0.3362</span></span>
<span id="cb55-6"><a href="c3.html#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]  -1.7844    0.2988  -2.3347  -1.1638</span></span>
<span id="cb55-7"><a href="c3.html#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,] -16.0844    0.2988 -16.6347 -15.4638</span></span>
<span id="cb55-8"><a href="c3.html#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,] -20.3844    0.2988 -20.9347 -19.7638</span></span>
<span id="cb55-9"><a href="c3.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   0.4156    0.2988  -0.1347   1.0362</span></span></code></pre></div>
<p>By default, information regarding the posterior distribution of residuals for each data point is presented. However, you can get all the posterior estimates for each individual residual by setting <code>summary=FALSE</code>. When you do this, you will get a matrix of size m (rows) by n (columns) for m posterior samples and n data points. We can see below that we get 1000 rows representing our posterior samples and 675 columns representing our data points.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="c3.html#cb56-1" aria-hidden="true" tabindex="-1"></a>model_residuals <span class="ot">=</span> <span class="fu">residuals</span> (model, <span class="at">summary=</span><span class="cn">FALSE</span>)</span>
<span id="cb56-2"><a href="c3.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span> (model_residuals)</span>
<span id="cb56-3"><a href="c3.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1000  675</span></span></code></pre></div>
<p>In the left plot of figure <a href="c3.html#fig:F3-3">3.3</a> we show a histogram of our residuals and compare this to a normal distribution centered at zero, with a standard deviation equal to the error in our model (<code>sigma = 7.78</code>). It’s no surprise that these match because this is precisely what the sigma parameter in our model is an estimate of, the standard deviation of the residuals. Since our model consists of only an intercept, a single expected value (<span class="math inline">\(\mu\)</span>) for all instances of the variable, all variation around the mean constitutes error. We can show this by comparing our residual estimates to the centered data (right plot of figure <a href="c3.html#fig:F3-3">3.3</a>) and seeing that these are basically the same.</p>
<div class="figure"><span style="display:block;" id="fig:F3-3"></span>
<img src="_main_files/figure-html/F3-3-1.jpeg" alt="(left) Histogram of the residuals for `model`. (right) A comparison of our residuals and centered height judgments shows that these are nearly equal." width="4800" />
<p class="caption">
Figure 3.3: (left) Histogram of the residuals for <code>model</code>. (right) A comparison of our residuals and centered height judgments shows that these are nearly equal.
</p>
</div>
</div>
</div>
<div id="c3-checking-convergence" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Checking model convergence<a href="c3.html#c3-checking-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our parameter estimates are based on a set of samples from the posterior distribution of a parameter. As with any other inference based on samples, our parameter estimates will be unreliable if we don’t have enough samples, or if our samples do not represent the population we are trying to understand. For this reason, it’s important to look at the <strong>ESS</strong> values (the <em>effective sample size</em>), and the <strong>Rhat</strong> values provided for <code>brm</code> model print statements.</p>
<p>Rhat tells you about whether your chains have converged, and it basically compares how much within-chain variation there is relative to the amount of between-chain overlap. Ideally, you want your chains to overlap almost entirely in the values that they span since they are supposed to be sampling the same thing. As noted in the ‘boilerplate’ at the end of the <code>brm</code> model print statement, values of Rhat near 1 are good, and values higher than around 1.1 are a bad sign.</p>
<p>ESS tells you approximately how many <em>independent</em> samples you have taken from the posterior. Bulk ESS tells you how many samples the sampler took in the ‘thick’ part of the density, and Tail ESS reflects how much time the sampler spent in the ‘thin’ part, in the tails of the distribution. Ideally we would like several hundred samples (at least) for mean estimates, and thousands to be confident in the 95% credible intervals. You may sometimes fit a model and get a warning message like this:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="c3.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning messages:</span></span>
<span id="cb57-2"><a href="c3.html#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 1: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and  </span></span>
<span id="cb57-3"><a href="c3.html#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="do">## medians may be unreliable. Running the chains for more iterations may help. See:</span></span>
<span id="cb57-4"><a href="c3.html#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="do">## http://mc-stan.org/misc/warnings.html#bulk-ess</span></span>
<span id="cb57-5"><a href="c3.html#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 2: Tail Effective Samples Size (ESS) is too low, indicating posterior variances </span></span>
<span id="cb57-6"><a href="c3.html#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="do">## and tail quantiles may be unreliable.</span></span>
<span id="cb57-7"><a href="c3.html#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Running the chains for more iterations may help. See</span></span>
<span id="cb57-8"><a href="c3.html#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="do">## http://mc-stan.org/misc/warnings.html#tail-ess</span></span></code></pre></div>
<p>That is <code>brms</code> telling you that you need to collect more samples in order to be confident in your parameter estimates. To get more samples, we can run the model for more iterations, or we can use more <em>chains</em>. Each chain is a separate set of samples for your parameter values. A model can be fit in parallel across several cores on your computer, resulting in several independent chains in roughly the same amount of computing time. Since these chains are all supposed to be sampling from the same posterior distribution, their samples can be merged across chains after sampling. There is a fixed number of samples a single core of your computer can make in a given amount of time. When you do this across <span class="math inline">\(n\)</span> cores, you can get (approximately) <span class="math inline">\(n\)</span> times as many samples in the same amount of time. Since many personal computers these days have 4-8 (or more) cores, we can take advantage of parallel processing to fit models faster. Before fitting a model across multiple cores, you should confirm how many you have. You can use the following command (you may need to install the <code>parallel</code> package):</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="c3.html#cb58-1" aria-hidden="true" tabindex="-1"></a>parallel<span class="sc">::</span><span class="fu">detectCores</span>()</span></code></pre></div>
<p>The example code throughout this book will use four cores to fit models. If you only have four total cores on your computer, you should change the models to use 2-3 chains and cores so that your computer can take care of other necessary functions while you fit your model(s). One thing to keep in mind is that these models can be computationally intensive to fit. As the data sets become larger and the models become more complicated, more powerful computers are needed in order to fit a model in a reasonable amount of time. Below, we re-fit our initial model but run it on four chains, and on four cores at once.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="c3.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb59-2"><a href="c3.html#cb59-2" aria-hidden="true" tabindex="-1"></a>model_multicore <span class="ot">=</span>  </span>
<span id="cb59-3"><a href="c3.html#cb59-3" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> men, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>)</span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="c3.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb60-2"><a href="c3.html#cb60-2" aria-hidden="true" tabindex="-1"></a>model_multicore <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;3_model_multicore.RDS&#39;</span>)</span></code></pre></div>
<p>We print the model below, and can see that using four chains has substantially increased our ESS, without taking up much more computing time. Towards the top of our print statement we see that <code>4 chains</code> have collected <code>total post-warmup samples = 4000</code>. This means our model has 4000 samples for every parameter in the model. However, for some parameters we have only about 3000 ‘effective samples’. This means some of our samples are basically dead weight, taking up space and slowing down future computations for no good reason. The discrepancy between the number of samples and the ‘effective’ number of samples is due to something called <strong>autocorrelation</strong>, the self-similarity of nearby observations in a series of observations (discussed further in section <a href="c5.html#c5-manipulating-random-effects">5.8.2</a>).</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="c3.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect model</span></span>
<span id="cb61-2"><a href="c3.html#cb61-2" aria-hidden="true" tabindex="-1"></a>model_multicore</span>
<span id="cb61-3"><a href="c3.html#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  Family: gaussian </span></span>
<span id="cb61-4"><a href="c3.html#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   Links: mu = identity; sigma = identity </span></span>
<span id="cb61-5"><a href="c3.html#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula: height ~ 1 </span></span>
<span id="cb61-6"><a href="c3.html#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="do">##    Data: men (Number of observations: 675) </span></span>
<span id="cb61-7"><a href="c3.html#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;</span></span>
<span id="cb61-8"><a href="c3.html#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="do">##          total post-warmup draws = 4000</span></span>
<span id="cb61-9"><a href="c3.html#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb61-10"><a href="c3.html#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects: </span></span>
<span id="cb61-11"><a href="c3.html#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb61-12"><a href="c3.html#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept   173.79      0.30   173.21   174.38 1.00     3139     2534</span></span>
<span id="cb61-13"><a href="c3.html#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb61-14"><a href="c3.html#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters: </span></span>
<span id="cb61-15"><a href="c3.html#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb61-16"><a href="c3.html#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma     7.78      0.21     7.39     8.21 1.00     3671     2476</span></span>
<span id="cb61-17"><a href="c3.html#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb61-18"><a href="c3.html#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS</span></span>
<span id="cb61-19"><a href="c3.html#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="do">## and Tail_ESS are effective sample size measures, and Rhat is the potential</span></span>
<span id="cb61-20"><a href="c3.html#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="do">## scale reduction factor on split chains (at convergence, Rhat = 1).</span></span></code></pre></div>
<p>Sometimes consecutive samples can be too similar and so don’t give you that much <em>independent</em> information. When this happens you end up with less information about a parameter than you might think based on the number of samples you have. Think of it like measuring the temperature of a place to get an idea of its average annual weather. Measurements need to be well separated in order to be really independent and to give an accurate picture of the true average. If you were to measure the temperature every 5 minutes these measurements would have a high <em>autocorrelation</em>, and would not give you a good impression of the range of temperatures that place tends to experience in a calendar year.</p>
<p>One way to increase the ESS without increasing the final number of posterior samples is to run longer chains and keep only every <span class="math inline">\(n^{th}\)</span> one. This strategy is called <strong>thinning</strong>, and it lets your models be smaller while containing approximately the same information. To do this you have to change the <code>iter</code>, <code>warmup</code> and <code>thin</code> parameters when you fit your model. Default behavior is that the models you fit keep every sample after the <code>warmup</code> is done, up to the <code>iter</code> maximum. So if <code>iter=3000</code> and <code>warmup=1000</code> you will end up with 2000 samples. If you set <code>thin</code> to some value other than 1, you keep only one every <code>thin</code> samples. As a result, you will end up with (<code>iter</code>-<code>warmup</code>) / <code>thin</code> samples per chain. If you are doing this across <code>cores</code> cores, then you will end up with (<code>iter</code>-<code>warmup</code>) / <code>thin</code>) <span class="math inline">\(\times\)</span> <code>cores</code> samples in total. Below, we ask for 3000 samples per chain. Since the warmup is 1000 this means we will keep 2000 post warmup per chain, for 8000 total samples across all four chains. However, since <code>thin=2</code>, we will keep only half of these of these. As a result, we will end up with 4000 samples in total (i.e. ((3000-1000) / 2) <span class="math inline">\(\times\)</span> 4).</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="c3.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb62-2"><a href="c3.html#cb62-2" aria-hidden="true" tabindex="-1"></a>model_thinned <span class="ot">=</span>  </span>
<span id="cb62-3"><a href="c3.html#cb62-3" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> men, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb62-4"><a href="c3.html#cb62-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">3000</span>, <span class="at">thin =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="c3.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb63-2"><a href="c3.html#cb63-2" aria-hidden="true" tabindex="-1"></a>model_thinned <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;3_model_thinned.RDS&#39;</span>)</span></code></pre></div>
<p>We inspect the model print statement and see that despite having the same number of samples as the <code>model_multicore</code>, the ESS for this model is higher than for the previous model, in particular for the <code>sigma</code> parameter. Before moving on from ESS we just want to note that it is possible for your ESS to be <em>higher</em> than your actual (real) number of samples. This is simply due to the way that ESS is calculated and is nothing to worry about, in fact, it may mean that your model is sampling the posterior very efficiently.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="c3.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect model</span></span>
<span id="cb64-2"><a href="c3.html#cb64-2" aria-hidden="true" tabindex="-1"></a>model_thinned</span>
<span id="cb64-3"><a href="c3.html#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  Family: gaussian </span></span>
<span id="cb64-4"><a href="c3.html#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   Links: mu = identity; sigma = identity </span></span>
<span id="cb64-5"><a href="c3.html#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula: height ~ 1 </span></span>
<span id="cb64-6"><a href="c3.html#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="do">##    Data: men (Number of observations: 675) </span></span>
<span id="cb64-7"><a href="c3.html#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 2;</span></span>
<span id="cb64-8"><a href="c3.html#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="do">##          total post-warmup draws = 4000</span></span>
<span id="cb64-9"><a href="c3.html#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb64-10"><a href="c3.html#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects: </span></span>
<span id="cb64-11"><a href="c3.html#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb64-12"><a href="c3.html#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept   173.78      0.30   173.21   174.39 1.00     3772     3556</span></span>
<span id="cb64-13"><a href="c3.html#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb64-14"><a href="c3.html#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters: </span></span>
<span id="cb64-15"><a href="c3.html#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb64-16"><a href="c3.html#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma     7.77      0.21     7.38     8.20 1.00     3933     3568</span></span>
<span id="cb64-17"><a href="c3.html#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb64-18"><a href="c3.html#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS</span></span>
<span id="cb64-19"><a href="c3.html#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="do">## and Tail_ESS are effective sample size measures, and Rhat is the potential</span></span>
<span id="cb64-20"><a href="c3.html#cb64-20" aria-hidden="true" tabindex="-1"></a><span class="do">## scale reduction factor on split chains (at convergence, Rhat = 1).</span></span></code></pre></div>
<p>Before moving on we want to talk about another problem you may run into: <strong>Divergent transitions</strong>. When you see an error like this:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="c3.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="do">## There were n divergent transitions after warmup. Increasing adapt_delta </span></span>
<span id="cb65-2"><a href="c3.html#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="do">## above 0.8 may help. See </span></span>
<span id="cb65-3"><a href="c3.html#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="do">## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup`</span></span></code></pre></div>
<p>It means your model encountered <span class="math inline">\(n\)</span> (some integer) divergent transitions during sampling. Your model has very specific expectations regarding parameter likelihoods and samples from the posterior distribution on this basis. A <em>divergent transition</em> indicates that as it sampled, it came across things that were not as expected. The fact that there is a difference between what your model though would happen and what actually happened then calls the reliability of the entire simulation into question. One way to fix this problem is to increase <code>adapt_delta</code> to 0.95 or 0.99. The <code>adapt_delta</code> parameter is set to 0.8 by default, and has a maximum value of one. This can be done as seen below.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="c3.html#cb66-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> men, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, </span>
<span id="cb66-2"><a href="c3.html#cb66-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">iter =</span> <span class="dv">3000</span>, <span class="at">thin =</span> <span class="dv">2</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.9</span>))</span></code></pre></div>
<p>Increasing the <code>adapt_delta</code> parameter results in fewer divergences and a more robust, but slower, sampler. Think of it like walking on ice taking little tiny steps; you are less likely to fall but will take longer to get there. If you raise <code>adapt_delta</code> to 0.999 or something and are still getting divergent transitions, there may be an error in your data or in the way that your model is specified. We can’t really be more specific than that at this point (see section <a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability">8.8</a>), however, we can say that divergent transitions are often not a <em>Stan</em> problem but rather a data or model structure problem.</p>
</div>
<div id="c3-specifying-priors" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Specifying prior probabilities<a href="c3.html#c3-specifying-priors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In section <a href="c3.html#c3-whats-bayes">3.3</a> we mentioned that in Bayesian models all estimated parameters <em>must</em> have prior probability distributions specified for them. And yet, to this point we’ve been fitting models without explicitly specifying prior probability distributions for their parameters. If you don’t specify prior probabilities for your parameters, <code>brm</code> will use its own default priors using the characteristics of your data. We can use the function <code>get_prior</code> in <code>brms</code> to see what the default priors are for our model, and to see which parameters in our model require priors. Of course, we should know this based on the structure of our model but this method is useful to help verify our expectations.</p>
<p>Below we can see that our model requires priors for our two estimated parameters, the <code>Intercept</code> (<span class="math inline">\(\mu\)</span>) and <code>sigma</code> (<span class="math inline">\(\sigma\)</span>) parameters, and that these have been given <code>default</code> values. The default values use a t distribution (<code>student_t()</code>), which we will discuss in section <a href="c5.html#c5-robustness">5.9</a>. We’ve omitted a few (empty) columns below so the printout will fit on the page, but we encourage you to erase the <code>[,-c(7:9)]</code> part in the code below and run it yourself so you can see the full output.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="c3.html#cb67-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">get_prior</span> (height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> men)[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">7</span><span class="sc">:</span><span class="dv">9</span>)]</span>
<span id="cb67-2"><a href="c3.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                     prior     class coef group resp dpar  source</span></span>
<span id="cb67-3"><a href="c3.html#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  student_t(3, 174.5, 7.1) Intercept                      default</span></span>
<span id="cb67-4"><a href="c3.html#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="do">##      student_t(3, 0, 7.1)     sigma                      default</span></span></code></pre></div>
<p><code>brms</code> makes it easy to specify prior probabilities for specific parameters or whole ‘classes’ of parameters. Setting priors for entire classes of parameters is faster for you and makes the model run faster, so it is a good idea to do it where possible. Right now, our model only includes the following classes of parameters:</p>
<ul>
<li><code>Intercept</code>: This is a unique class, only for intercepts.</li>
<li><code>sigma</code>: This is for the standard deviation of our error parameters. Our model only has one for now, <code>sigma</code> (<span class="math inline">\(\sigma\)</span>), but it can have more.</li>
</ul>
<p>We’re going to set <em>weakly informative</em> prior probabilities for our parameters. This means we’re going to set our priors to be about the same size as the variation we expect in the data itself. To set these you have to use what you know about your variables and the world in general. Since we know that the average male over 20 in the US is 176 cm tall, this seems like a reasonable prior expectation for how tall the adult males in our sample will sound. We also know that the standard deviation of adult male heights in the US is 7.5 cm, and will double this for our priors. This is to account for the fact that there may be more variation in how tall people ‘sound’ compared to how tall they <em>are</em>. The code to set the priors for our model looks like this:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="c3.html#cb68-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;normal(176, 15)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb68-2"><a href="c3.html#cb68-2" aria-hidden="true" tabindex="-1"></a>          brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;normal(0, 15)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sigma&quot;</span>))</span></code></pre></div>
<p>The code above tells our model to use a normal distribution with a mean of 176 and a standard deviation of 15 (<code>normal(176, 15)</code>) for the prior distribution of the intercept. Around 90-95% of the mass of normal distributions is within two standard deviations of the mean. This means that we are saying that we expect, a priori, that the intercept should be between around 146 (176 - 15 <span class="math inline">\(\times\)</span> 2) and 206 cm (176 + 15 <span class="math inline">\(\times\)</span> 2). This is probably too broad, but it places the expected outcomes within reasonable human ranges.</p>
<p>The random error, <code>sigma</code>, was given a prior with a normal distribution with a mean of 0 and a standard deviation of 15 (<code>normal(0, 15)</code>). Again, this is likely an overestimation of the magnitude of the random error in this data. However, it is likely to be in the ballpark. Our prior specifies a normal distribution centered at 0 for the standard deviation. Since standard deviations, like variances, can only be positive, the sampler (<em>Stan</em>) used by <code>brm</code> ignores the negative half and uses only the positive half of the prior distribution. This prior basically says that we expect the average variation around the mean to be less than 30 cm, which it is very likely to be.</p>
<p>The left plot in figure <a href="c3.html#fig:F3-4">3.4</a> compares the normal distribution we used (blue line) to a histogram of our height judgments. As we can see, the prior distribution we used for the intercept is much broader (more vague) than the data distribution so that it will probably have little to no practical effect on our posterior distribution (but will help our model fit properly). The right plot compares the prior for the standard deviation parameters to the absolute value of the centered apparent heights. This presentation shows how far each observation is from the mean apparent height (at 174 cm), and again we see that most of these deviations are in the thicker part of the prior density. As a result, neither of these priors is going to have much of an effect on our parameter estimates given the size of our sample (see figure <a href="c3.html#fig:F3-1">3.1</a>).</p>
<div class="figure"><span style="display:block;" id="fig:F3-4"></span>
<img src="_main_files/figure-html/F3-4-1.jpeg" alt="(left) The densities of the prior probability for our model intercept, compared to a histogram of height judgments for male speakers. (right) The distribution of absolute deviations from the mean height judgment, compared to the prior distribution for the error parameter ($\sigma$) in our model." width="4800" />
<p class="caption">
Figure 3.4: (left) The densities of the prior probability for our model intercept, compared to a histogram of height judgments for male speakers. (right) The distribution of absolute deviations from the mean height judgment, compared to the prior distribution for the error parameter (<span class="math inline">\(\sigma\)</span>) in our model.
</p>
</div>
<p>We can update the description of our model to include the specification of prior distributions for each estimated parameter, as in <a href="c3.html#eq:3-21">(3.21)</a>. In the future, our model descriptions will always include these. Our model specification now makes it clear: We expect height judgments to be normally distributed, we expect the mean to always equal the intercept, and we have specific prior distributions in mind for all estimated model parameters (<span class="math inline">\(\mathrm{Intercept}\)</span> and <span class="math inline">\(\sigma\)</span>).</p>
<p><span class="math display" id="eq:3-21">\[
\begin{equation}
\begin{split}
\\
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\
\mu_{[i]} = \mathrm{Intercept} \\
\\
\textrm{Priors:} \\
\mathrm{Intercept} \sim \mathrm{N}(176, 15) \\
\sigma \sim \mathrm{N}(0, 15) \\
\end{split}
\tag{3.21}
\end{equation}
\]</span></p>
<p>We can fit this new model below, passing the lines that specify the prior distributions for our parameters to the <code>prior</code> parameter of the <code>brm</code> function.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="c3.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself, or</span></span>
<span id="cb69-2"><a href="c3.html#cb69-2" aria-hidden="true" tabindex="-1"></a>model_priors <span class="ot">=</span>  </span>
<span id="cb69-3"><a href="c3.html#cb69-3" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> men, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb69-4"><a href="c3.html#cb69-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">3500</span>, <span class="at">thin =</span> <span class="dv">2</span>,</span>
<span id="cb69-5"><a href="c3.html#cb69-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">prior =</span> <span class="fu">c</span>(brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;normal(176, 15)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb69-6"><a href="c3.html#cb69-6" aria-hidden="true" tabindex="-1"></a>                 brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;normal(0, 15)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sigma&quot;</span>)))</span></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="c3.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the model above from the GitHub page.</span></span>
<span id="cb70-2"><a href="c3.html#cb70-2" aria-hidden="true" tabindex="-1"></a>model_priors <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;3_model_priors.RDS&#39;</span>)</span></code></pre></div>
<p>We can use the <code>short_summary</code> function in the <code>bmmb</code> package to get an abridged version of the model print statements. These shorter versions are not a replacement for the complete statement as they omit important information about our models. However, these abbreviated print statements will help us compare models while minimizing redundant information on the page, and so are useful for this book. If we compare the output of <code>model_thinned</code>:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="c3.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect model</span></span>
<span id="cb71-2"><a href="c3.html#cb71-2" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">short_summary</span> (model_thinned)</span>
<span id="cb71-3"><a href="c3.html#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula:  height ~ 1</span></span>
<span id="cb71-4"><a href="c3.html#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects:</span></span>
<span id="cb71-5"><a href="c3.html#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb71-6"><a href="c3.html#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept    173.8       0.3    173.2    174.4</span></span>
<span id="cb71-7"><a href="c3.html#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb71-8"><a href="c3.html#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters:</span></span>
<span id="cb71-9"><a href="c3.html#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb71-10"><a href="c3.html#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma     7.77      0.21     7.38      8.2</span></span></code></pre></div>
<p>To that of the model where we specified our own priors (<code>model_priors</code>), we see that there is no noticeable effect on our results.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="c3.html#cb72-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">short_summary</span> (model_priors)</span>
<span id="cb72-2"><a href="c3.html#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula:  height ~ 1</span></span>
<span id="cb72-3"><a href="c3.html#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects:</span></span>
<span id="cb72-4"><a href="c3.html#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb72-5"><a href="c3.html#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept    173.8      0.31    173.2    174.4</span></span>
<span id="cb72-6"><a href="c3.html#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb72-7"><a href="c3.html#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters:</span></span>
<span id="cb72-8"><a href="c3.html#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb72-9"><a href="c3.html#cb72-9" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma     7.77      0.21     7.37     8.19</span></span></code></pre></div>
<p>This is because the prior matters less and less when you have a lot of data, and because we have set priors that are appropriate (but vague) given our data. Although the priors may not matter much for models as simple as these, they can be very important when working with more complex data, and are a necessary component of Bayesian modeling.</p>
</div>
<div id="c3-log-posterior" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> The log prior and log posterior densities<a href="c3.html#c3-log-posterior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The information presented in this section is not strictly necessary to understand the models outlined in this book. However, it is useful information to know and so we put it here so that you will know when to find it when you want it.</p>
<p>When we got our posterior samples with the <code>get_samples</code> function, in addition to our mean and standard deviation estimates we got estimates of something called <code>lprior</code> and <code>lp__</code>. We can see these below:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="c3.html#cb73-1" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_samples</span> (model_priors)</span>
<span id="cb73-2"><a href="c3.html#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (samples)</span>
<span id="cb73-3"><a href="c3.html#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   b_Intercept sigma lprior  lp__</span></span>
<span id="cb73-4"><a href="c3.html#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 1       173.4 8.041 -6.719 -2347</span></span>
<span id="cb73-5"><a href="c3.html#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 2       173.5 7.997 -6.716 -2346</span></span>
<span id="cb73-6"><a href="c3.html#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 3       173.8 8.043 -6.715 -2346</span></span>
<span id="cb73-7"><a href="c3.html#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 4       174.8 7.936 -6.704 -2351</span></span>
<span id="cb73-8"><a href="c3.html#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 5       173.4 7.630 -6.705 -2346</span></span>
<span id="cb73-9"><a href="c3.html#cb73-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 6       173.8 7.965 -6.712 -2346</span></span></code></pre></div>
<p>These are the <em>log prior</em> and the (unnormalized) <em>log posterior</em> densities, respectively. To explain what these are, let’s generalize equation <a href="c3.html#eq:3-12">(3.12)</a>, which presented the posterior probability of a <span class="math inline">\(\mu\)</span> parameter given your data and priors, to encompass all of our model parameters represented by <span class="math inline">\(\theta\)</span> in <a href="c3.html#eq:3-22">(3.22)</a>. Now, <a href="c3.html#eq:3-22">(3.22)</a> represents the posterior probability of <em>all</em> our model parameters given our data.</p>
<p><span class="math display" id="eq:3-22">\[
\begin{equation}
P(\theta|y) = \frac{P(y|\theta) \cdot P(\theta)}{P(y)}
\tag{3.22}
\end{equation}
\]</span></p>
<p>First, we can discuss <code>lprior</code>, the log prior density. This is the logarithm of the joint prior density of all of your parameters (i.e. the sum of their log densities), in other words <span class="math inline">\(\log(P(\theta))\)</span>. Specifically for our model, the is equal to <span class="math inline">\(\log(P(\mu \: \&amp; \: \sigma))\)</span>. We can see the prior probabilities of our model parameters below:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="c3.html#cb74-1" aria-hidden="true" tabindex="-1"></a>model_priors<span class="sc">$</span>prior</span>
<span id="cb74-2"><a href="c3.html#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="do">##            prior     class coef group resp dpar nlpar lb ub source</span></span>
<span id="cb74-3"><a href="c3.html#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  normal(176, 15) Intercept                                    user</span></span>
<span id="cb74-4"><a href="c3.html#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="do">##    normal(0, 15)     sigma                             0      user</span></span></code></pre></div>
<p>And we can use this information to calculate <code>lprior</code> for the first and fourth values of <code>lprior</code> seen in the samples above (-6.719 and -6.704). In the lines below we find the log density of the mean, and the log density of the error term (<code>sigma</code>) for the first and fourth posterior estimates of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. For example, <code>dnorm (173.4,176,15,log=TRUE)</code> returns the log (since <code>log=TRUE</code>) density over a point at 173.4 (the first posterior sample of the mean) given a normal distribution with the characteristics of the prior of our mean (i.e., a mean of 176 cm and a standard deviation of 15 cm). Since variances are bounded by zero, <em>Stan</em> ignores the negative half of the density. As a result, add log(2) (i.e. multiply by two) to multiply the density of the <code>sigma</code> term by two, and make the area under the curve still be equal to one. By adding these terms together, we can recreate the values of <code>lprior</code> above.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="c3.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span> (<span class="fl">173.4</span>,<span class="dv">176</span>,<span class="dv">15</span>,<span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">dnorm</span> (<span class="fl">8.041</span>,<span class="dv">0</span>,<span class="dv">15</span>,<span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">log</span>(<span class="dv">2</span>)</span>
<span id="cb75-2"><a href="c3.html#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -6.72</span></span>
<span id="cb75-3"><a href="c3.html#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span> (<span class="fl">174.8</span>,<span class="dv">176</span>,<span class="dv">15</span>,<span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">dnorm</span> (<span class="fl">7.936</span>,<span class="dv">0</span>,<span class="dv">15</span>,<span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">log</span>(<span class="dv">2</span>)</span>
<span id="cb75-4"><a href="c3.html#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -6.704</span></span></code></pre></div>
<p>The <strong>log posterior density</strong> (<code>lp__</code>) is basically the overall posterior probability of your model parameters given your data. The <em>unnormalized</em> posterior density ignores the marginal distribution (<span class="math inline">\(P(y)\)</span>) and so returns a density that is proportional to the posterior density up to some constant, as in <a href="c3.html#eq:3-23">(3.23)</a>.</p>
<p><span class="math display" id="eq:3-23">\[
\begin{equation}
\begin{split}
P(\theta|y) \propto P(y|\theta) \cdot P(\theta)
\end{split}
\tag{3.23}
\end{equation}
\]</span></p>
<p>By saying that it’s defined up to a <strong>proportional constant</strong>, this is what we mean. We cannot, or do not bother, calculating the exact posterior density. However, we can calculate a value that when multiplied by some constant <span class="math inline">\(C\)</span> (which we happen to know is equal to (<span class="math inline">\(1/P(y)\)</span>)), equals the posterior density. This value is <span class="math inline">\([P(y|\theta) \cdot P(\theta)]\)</span>, presented in square brackets in <a href="c3.html#eq:3-23a">(3.24)</a>. When we model the unnormalized posterior density we take this approach, we try to estimate <span class="math inline">\(P(\theta|y)\)</span> while ignoring <span class="math inline">\(C\)</span> (i.e., <span class="math inline">\(1/P(y)\)</span>).</p>
<p><span class="math display" id="eq:3-23a">\[
\begin{equation}
\begin{split}
P(\theta|y) = [P(y|\theta) \cdot P(\theta)] \cdot C
\end{split}
\tag{3.24}
\end{equation}
\]</span></p>
<p>If we log-transform both sides of <a href="c3.html#eq:3-23a">(3.24)</a>, we get the log-posterior density, shown in <a href="c3.html#eq:3-24">(3.25)</a>.</p>
<p><span class="math display" id="eq:3-24">\[
\begin{equation}
\log (P(\theta|y)) = \log (P(y|\theta)) + \log(P(\theta)) + \log(C)
\tag{3.25}
\end{equation}
\]</span></p>
<p>Let’s pause to think about what <a href="c3.html#eq:3-24">(3.25)</a> means and why it’s a useful measure. First, let’s talk about <span class="math inline">\(P(y|\theta)\)</span> which is the model likelihood: The joint density of the data given the values of the parameters. We can think about the probability of observing each of the observations in our height data given the posterior mean estimates of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> provided by <code>model_priors</code>. Below, we see the values of the probability density above the first six observations, and then the logarithm of these values below.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="c3.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># density over the first 6 observations</span></span>
<span id="cb76-2"><a href="c3.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (<span class="fu">dnorm</span> (mens_height, <span class="fl">173.8</span>, <span class="fl">7.77</span>))</span>
<span id="cb76-3"><a href="c3.html#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.045267 0.051306 0.049985 0.006000 0.001636 0.051276</span></span>
<span id="cb76-4"><a href="c3.html#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="c3.html#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="co"># log density over first 6 observations</span></span>
<span id="cb76-6"><a href="c3.html#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (<span class="fu">dnorm</span> (mens_height, <span class="fl">173.8</span>, <span class="fl">7.77</span>, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb76-7"><a href="c3.html#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -3.095 -2.970 -2.996 -5.116 -6.416 -2.971</span></span></code></pre></div>
<p>To find the joint density of the data given the model parameters, we can sum the log-densities as seen below. This provides us an estimate of the log likelihood of the model given all parameter values (<span class="math inline">\(\log (P(y|\theta))\)</span>).</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="c3.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (<span class="fu">dnorm</span> (mens_height, <span class="fl">173.8</span>, <span class="fl">7.77</span>, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb77-2"><a href="c3.html#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -2341</span></span></code></pre></div>
<p>We can then add the logarithm of the priors for our parameters to this. By combining our priors and likelihoods in this way, we are calculating the <em>unnormalized</em> posterior density.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="c3.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (<span class="fu">dnorm</span> (mens_height, <span class="fl">173.8</span>, <span class="fl">7.78</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)) <span class="sc">+</span> <span class="co"># the likelihood</span></span>
<span id="cb78-2"><a href="c3.html#cb78-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dnorm</span> (<span class="fl">173.8</span>,<span class="dv">176</span>,<span class="dv">15</span>,<span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">+</span>   <span class="co"># prior probability of mu </span></span>
<span id="cb78-3"><a href="c3.html#cb78-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dnorm</span> (<span class="fl">7.77</span>,<span class="dv">0</span>,<span class="dv">15</span>,<span class="at">log=</span><span class="cn">TRUE</span>)<span class="sc">+</span><span class="fu">log</span>(<span class="dv">2</span>) <span class="co"># prior probability of sigma </span></span>
<span id="cb78-4"><a href="c3.html#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -2347</span></span></code></pre></div>
<p>We can see that our calculation is very close to the average value of the <code>lp__</code> estimates provided by our model.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="c3.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (samples<span class="sc">$</span>lp__)</span>
<span id="cb79-2"><a href="c3.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -2346</span></span></code></pre></div>
<p>The log posterior density, or values related to it, will come up again in chapter 6 when we discuss model comparison. This is because this value captures the posterior probability of the parameters given the data. Broadly speaking, models with a higher posterior probability are more believable because they are more likely to generate the observed data, and/or they have parameter values that are more probable a priori.</p>
</div>
<div id="c3-answering-qs" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Answering our research questions<a href="c3.html#c3-answering-qs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finally, let’s return again to the research questions we posed initially in chapter 2, and again at the beginning of this chapter:</p>
<p>(Q1) How tall does the average adult male sound?</p>
<p>(Q2) Can we set limits on credible average apparent heights based on the data we collected?</p>
<p>Here’s what we had to say about this at the end of chapter 2:</p>
<p>“the average male speaker is <em>most likely</em> to sound about 174 cm tall. We can also conclude informally based on Figure <a href="c2.html#fig:F2-8">2.8</a> that the most likely mean values fall between (approximately) 173 and 174.5 cm”</p>
<p>We can reconsider the answers to these questions provided by our final model, <code>model_priors</code>. Usually, parameters should be reported with <em>at least</em> the mean/median and standard deviations of the posterior distribution, in addition to some useful credible interval (e.g. 50%, 95%) around that parameter. Based on the result of our final model, an answer to each question might be something like this:</p>
<p>(A1) Based on our model the average apparent height for adult males is likely to be 174 cm. In a paper we might report this like: “The mean height is 174 cm (s.d. = 0.3, 95% CI = [173.2, 174.4])”.</p>
<p>(A2) Yes we can. There is a 95% probability that the population mean is between 173.2 and 174.4 given our data and model structure. In other words, 95% of the posterior density is concentrated between the values of 173.2 and 174.4.</p>
<p>Notice that our answers correspond closely to what we concluded at the end of last chapter. The reason for this correspondence is because we made our inferences at the end of chapter 2 using only the likelihood and, due to the shape of the prior and the number of observations in our data, the posterior distribution of our model is being dominated by the likelihood. As a result, the two approaches converge on approximately the same solution in this situation.</p>
</div>
<div id="c3-frequentist" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> ‘Traditionalists’ corner<a href="c3.html#c3-frequentist" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In traditionalists corner, we’re going to compare the output of <code>brms</code> to some more ‘traditional’ approaches. We’re not going to talk about the traditional models in any detail, the focus of this section is simply to highlight the similarities between different approaches, and to point out where to find equivalent information in the different models. If you are already familiar with these approaches, these sections may be helpful. If not, some of the information provided here may not make much sense, although it may still be helpful. If you want to know more about the statistical methods being discussed here, please see the preface for a list of suggested background reading in statistics.</p>
<div id="c3-vs-ttest" class="section level3 hasAnchor" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> One-sample t-test vs. intercept-only Bayesian models<a href="c3.html#c3-vs-ttest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A one sample t-test helps investigate whether the mean of a set of observations is consistent with a true underlying value of zero. The t-test answers questions using the likelihood of parameters given the data (priors and posteriors are not involved). In addition, t-tests are calculated using analytic (exact) methods, and do not involve sampling from distributions. The t-test assumes that all of your observations are independent, so it is not appropriate to use for our experimental data. However, we just want to compare it to our initial Bayesian model (which made the same assumptions). We can apply a one-sample t-test to our vector of apparent-height judgments:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="c3.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span> (mens_height)</span>
<span id="cb80-2"><a href="c3.html#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb80-3"><a href="c3.html#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  One Sample t-test</span></span>
<span id="cb80-4"><a href="c3.html#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb80-5"><a href="c3.html#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="do">## data:  mens_height</span></span>
<span id="cb80-6"><a href="c3.html#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="do">## t = 582, df = 674, p-value &lt;2e-16</span></span>
<span id="cb80-7"><a href="c3.html#cb80-7" aria-hidden="true" tabindex="-1"></a><span class="do">## alternative hypothesis: true mean is not equal to 0</span></span>
<span id="cb80-8"><a href="c3.html#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 95 percent confidence interval:</span></span>
<span id="cb80-9"><a href="c3.html#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  173.2 174.4</span></span>
<span id="cb80-10"><a href="c3.html#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="do">## sample estimates:</span></span>
<span id="cb80-11"><a href="c3.html#cb80-11" aria-hidden="true" tabindex="-1"></a><span class="do">## mean of x </span></span>
<span id="cb80-12"><a href="c3.html#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="do">##     173.8</span></span></code></pre></div>
<p>Notice that the interval provided around the mean (<code>95 percent confidence interval: 173.2 174.4</code>) corresponds very well to the 95% credible interval around the intercept provided in <code>model</code> (<code>173.16, 174.33</code>). The reason they align so well is because both models have the same general structure and make similar mathematical assumptions. In addition, our Bayesian estimate is being dominated by the likelihood, and the more ‘traditional’ t-test <em>only</em> considers information from the likelihood.</p>
</div>
<div id="c3-vs-ols" class="section level3 hasAnchor" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models<a href="c3.html#c3-vs-ols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ordinary-least-squares (OLS) regression is an approach to fitting regression models using likelihoods (without priors or posteriors). OLS regression assumes that your residuals are independent and that your error variation is normally distributed. We can fit an OLS model using the <code>lm</code> (linear model) function in R, using the same model formula we used for our Bayesian models.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="c3.html#cb81-1" aria-hidden="true" tabindex="-1"></a>ols_model <span class="ot">=</span> <span class="fu">lm</span> (mens_height <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb81-2"><a href="c3.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span> (ols_model)</span>
<span id="cb81-3"><a href="c3.html#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb81-4"><a href="c3.html#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb81-5"><a href="c3.html#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = mens_height ~ 1)</span></span>
<span id="cb81-6"><a href="c3.html#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb81-7"><a href="c3.html#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb81-8"><a href="c3.html#cb81-8" aria-hidden="true" tabindex="-1"></a><span class="do">##    Min     1Q Median     3Q    Max </span></span>
<span id="cb81-9"><a href="c3.html#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -34.09  -4.59   0.71   5.31  18.51 </span></span>
<span id="cb81-10"><a href="c3.html#cb81-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb81-11"><a href="c3.html#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb81-12"><a href="c3.html#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb81-13"><a href="c3.html#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  173.788      0.299     582   &lt;2e-16 ***</span></span>
<span id="cb81-14"><a href="c3.html#cb81-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb81-15"><a href="c3.html#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb81-16"><a href="c3.html#cb81-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb81-17"><a href="c3.html#cb81-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 7.76 on 674 degrees of freedom</span></span></code></pre></div>
<p>Again, we see a close similarity to our initial <code>model</code>. The <code>Std.error</code> for the Intercept above (0.299) corresponds closely to the <code>Est.error</code> of the intercept below (0.30), and the <code>Residual standard error</code> above (7.76) corresponds closely to the <code>sigma</code> estimate below (7.77).</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="c3.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects: </span></span>
<span id="cb82-2"><a href="c3.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb82-3"><a href="c3.html#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept   173.78      0.30   173.16   174.33 1.00     1055      714</span></span>
<span id="cb82-4"><a href="c3.html#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb82-5"><a href="c3.html#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters: </span></span>
<span id="cb82-6"><a href="c3.html#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb82-7"><a href="c3.html#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma     7.77      0.22     7.37     8.21 1.00     1139      741</span></span></code></pre></div>
<p>Just as with the t-test, these similarities are due to the models having the same general structure, making the same assumptions, and being dominated by the likelihood of the parameters.</p>
</div>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> Exercises<a href="c3.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The analyses in the main body of the text all involve only the unmodified ‘actual’ resonance level (in <code>exp_data</code>). Responses for the stimuli with the simulated ‘big’ resonance are reserved for exercises throughout. You can get the ‘big’ resonance in the <code>exp_ex</code> data frame, or all data in the <code>exp_data_all</code> data frame.</p>
<p>Fit and interpret one or more of the suggested models:</p>
<ol style="list-style-type: decimal">
<li><p>Easy: Analyze the (pre-fit) model that’s exactly like <code>model_priors</code>, except using the data in <code>exp_ex</code> (<code>bmmb::get_model("3_model_priors_ex.RDS")</code>).</p></li>
<li><p>Medium: Fit a model just like <code>model_priors</code>, but for the data from some other group, for either the original or big resonance levels.</p></li>
<li><p>Hard: Fit two models like <code>model_priors</code> for two arbitrary groups, and compare results across models.</p></li>
</ol>
<p>In any case, describe the model, present and explain the results, and include some figures.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="c2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="c4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
