<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Logistic regression and signal detection theory models | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</title>
  <meta name="description" content="Bayesian Models for Repeated Measures" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Logistic regression and signal detection theory models | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Bayesian Models for Repeated Measures" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Logistic regression and signal detection theory models | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  
  <meta name="twitter:description" content="Bayesian Models for Repeated Measures" />
  

<meta name="author" content="Santiago Bareda and Noah Silbert" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="quantitative-predictors-and-their-interactions-with-factors.html"/>
<link rel="next" href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Repeated Measures data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bayesian-multilevel-models-and-repeated-measures-data"><i class="fa fa-check"></i>Bayesian Multilevel models and repeated measures data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-missing-from-this-book"><i class="fa fa-check"></i>What’s missing from this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#statistics-as-procedural-knowledge"><i class="fa fa-check"></i>Statistics as Procedural knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practice-vs-brain-power"><i class="fa fa-check"></i>Practice vs brain power</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#supplemental-resources"><i class="fa fa-check"></i>Supplemental Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#our-target-audience"><i class="fa fa-check"></i>Our target audience</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-self-starter"><i class="fa fa-check"></i>The self-starter</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-convert"><i class="fa fa-check"></i>The convert</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-instructor"><i class="fa fa-check"></i>The instructor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-you-need-installed-to-use-this-book"><i class="fa fa-check"></i>What you need installed to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-go-bayesian"><i class="fa fa-check"></i>Why go Bayesian?</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-brms"><i class="fa fa-check"></i>Why brms?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#it-takes-a-village-of-books"><i class="fa fa-check"></i>It takes a village (of books)</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html"><i class="fa fa-check"></i><b>1</b> Introduction: Experiments and Variables</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#chapter-pre-cap"><i class="fa fa-check"></i><b>1.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-and-effects"><i class="fa fa-check"></i><b>1.2</b> Experiments and effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-and-inference"><i class="fa fa-check"></i><b>1.2.1</b> Experiments and inference</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp"><i class="fa fa-check"></i><b>1.3</b> Our experiment</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-intro"><i class="fa fa-check"></i><b>1.3.1</b> Our experiment: Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-methods"><i class="fa fa-check"></i><b>1.3.2</b> Our experimental methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-research-questions"><i class="fa fa-check"></i><b>1.3.3</b> Our research questions</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-data"><i class="fa fa-check"></i><b>1.3.4</b> Our experimental data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-variables"><i class="fa fa-check"></i><b>1.4</b> Variables</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-pops-and-samps"><i class="fa fa-check"></i><b>1.4.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-dep-and-indep"><i class="fa fa-check"></i><b>1.4.2</b> Dependent and Independent Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-categorical"><i class="fa fa-check"></i><b>1.4.3</b> Categorical variables and ‘factors’</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-quantitative"><i class="fa fa-check"></i><b>1.4.4</b> Quantitative variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-logical"><i class="fa fa-check"></i><b>1.4.5</b> Logical variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting"><i class="fa fa-check"></i><b>1.5</b> Inspecting our data</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-categorical"><i class="fa fa-check"></i><b>1.5.1</b> Inspecting categorical variables</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-quantitative"><i class="fa fa-check"></i><b>1.5.2</b> Inspecting quantitative variables</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-together"><i class="fa fa-check"></i><b>1.5.3</b> Exploring continuous and categorical variables together</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
<li class="chapter" data-level="1.7" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#references"><i class="fa fa-check"></i><b>1.7</b> References</a></li>
<li class="chapter" data-level="1.8" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#plot-code"><i class="fa fa-check"></i><b>1.8</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html"><i class="fa fa-check"></i><b>2</b> Probabilities, likelihood, and inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#chapter-pre-cap-1"><i class="fa fa-check"></i><b>2.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="2.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-data"><i class="fa fa-check"></i><b>2.2</b> Data and research questions</a></li>
<li class="chapter" data-level="2.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-empirical-prob"><i class="fa fa-check"></i><b>2.3</b> Empirical Probabilities</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-conditional"><i class="fa fa-check"></i><b>2.3.1</b> Conditional and marginal probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-joint"><i class="fa fa-check"></i><b>2.3.2</b> Joint probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-theoretical"><i class="fa fa-check"></i><b>2.4</b> Probability distributions</a></li>
<li class="chapter" data-level="2.5" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-normal"><i class="fa fa-check"></i><b>2.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-sample-mean"><i class="fa fa-check"></i><b>2.5.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.5.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-sample-variance"><i class="fa fa-check"></i><b>2.5.2</b> The sample variance (or standard deviation)</a></li>
<li class="chapter" data-level="2.5.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-normal-density"><i class="fa fa-check"></i><b>2.5.3</b> The normal density</a></li>
<li class="chapter" data-level="2.5.4" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-standard-normal"><i class="fa fa-check"></i><b>2.5.4</b> The standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-models-and-inference"><i class="fa fa-check"></i><b>2.6</b> Models and inference</a></li>
<li class="chapter" data-level="2.7" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-likelihoods"><i class="fa fa-check"></i><b>2.7</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods"><i class="fa fa-check"></i><b>2.7.1</b> Characteristics of likelihoods</a></li>
<li class="chapter" data-level="2.7.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-logarithms"><i class="fa fa-check"></i><b>2.7.2</b> A brief aside on logarithms</a></li>
<li class="chapter" data-level="2.7.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods-2"><i class="fa fa-check"></i><b>2.7.3</b> Characteristics of likelihoods, continued</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-inference-and-likelihood"><i class="fa fa-check"></i><b>2.8</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.9" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#exercises-1"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
<li class="chapter" data-level="2.10" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#references-1"><i class="fa fa-check"></i><b>2.10</b> References</a></li>
<li class="chapter" data-level="2.11" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#plot-code-1"><i class="fa fa-check"></i><b>2.11</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html"><i class="fa fa-check"></i><b>3</b> Fitting Bayesian regression models with <em>brms</em></a>
<ul>
<li class="chapter" data-level="3.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#chapter-pre-cap-2"><i class="fa fa-check"></i><b>3.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="3.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-what-is-reg"><i class="fa fa-check"></i><b>3.2</b> What are regression models?</a></li>
<li class="chapter" data-level="3.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-whats-bayes"><i class="fa fa-check"></i><b>3.3</b> What’s ‘Bayesian’ about these models?</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-priors"><i class="fa fa-check"></i><b>3.3.1</b> Prior probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-posterior"><i class="fa fa-check"></i><b>3.3.2</b> Posterior distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-characteristics-posteriors"><i class="fa fa-check"></i><b>3.3.3</b> Posterior distributions and shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-sampling"><i class="fa fa-check"></i><b>3.4</b> Sampling from the posterior using <em>Stan</em> and <em>brms</em></a></li>
<li class="chapter" data-level="3.5" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-estimating"><i class="fa fa-check"></i><b>3.5</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-data-qs-1"><i class="fa fa-check"></i><b>3.5.1</b> Data and Research Questions</a></li>
<li class="chapter" data-level="3.5.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-description-1"><i class="fa fa-check"></i><b>3.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.5.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-errors-and-residuals"><i class="fa fa-check"></i><b>3.5.3</b> Errors and residuals</a></li>
<li class="chapter" data-level="3.5.4" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-model-formula"><i class="fa fa-check"></i><b>3.5.4</b> The model formula</a></li>
<li class="chapter" data-level="3.5.5" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-calling-brm"><i class="fa fa-check"></i><b>3.5.5</b> Fitting the model: Calling the <em>brm</em> function</a></li>
<li class="chapter" data-level="3.5.6" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-interpreting-print"><i class="fa fa-check"></i><b>3.5.6</b> Interpreting the model: The print statement</a></li>
<li class="chapter" data-level="3.5.7" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-seeing-samples"><i class="fa fa-check"></i><b>3.5.7</b> Seeing the samples</a></li>
<li class="chapter" data-level="3.5.8" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-getting-residuals"><i class="fa fa-check"></i><b>3.5.8</b> Getting the residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-checking-convergence"><i class="fa fa-check"></i><b>3.6</b> Checking model convergence</a></li>
<li class="chapter" data-level="3.7" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-specifying-priors"><i class="fa fa-check"></i><b>3.7</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="3.8" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-log-posterior"><i class="fa fa-check"></i><b>3.8</b> The log prior and log posterior densities</a></li>
<li class="chapter" data-level="3.9" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-answering-qs"><i class="fa fa-check"></i><b>3.9</b> Answering our research questions</a></li>
<li class="chapter" data-level="3.10" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-frequentist"><i class="fa fa-check"></i><b>3.10</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-vs-ttest"><i class="fa fa-check"></i><b>3.10.1</b> One-sample t-test vs. intercept-only Bayesian models</a></li>
<li class="chapter" data-level="3.10.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-vs-ols"><i class="fa fa-check"></i><b>3.10.2</b> Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#exercises-2"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
<li class="chapter" data-level="3.12" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#plot-code-2"><i class="fa fa-check"></i><b>3.12</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><i class="fa fa-check"></i><b>4</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#chapter-pre-cap-3"><i class="fa fa-check"></i><b>4.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-multilevel"><i class="fa fa-check"></i><b>4.2</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-levels"><i class="fa fa-check"></i><b>4.2.1</b> Multilevel models and ‘levels’ of variation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-many-levels"><i class="fa fa-check"></i><b>4.3</b> Representing predictors with many levels</a></li>
<li class="chapter" data-level="4.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-strategies"><i class="fa fa-check"></i><b>4.4</b> Strategies for estimating factors with many levels</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-complete-pooling"><i class="fa fa-check"></i><b>4.4.1</b> Complete pooling</a></li>
<li class="chapter" data-level="4.4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-no-pooling"><i class="fa fa-check"></i><b>4.4.2</b> No pooling</a></li>
<li class="chapter" data-level="4.4.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-partial-pooling"><i class="fa fa-check"></i><b>4.4.3</b> (Adaptive) Partial pooling</a></li>
<li class="chapter" data-level="4.4.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#hyperpriors"><i class="fa fa-check"></i><b>4.4.4</b> Hyperpriors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-estimating1"><i class="fa fa-check"></i><b>4.5</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-data-and-qs-1"><i class="fa fa-check"></i><b>4.5.1</b> Data and Research questions</a></li>
<li class="chapter" data-level="4.5.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model"><i class="fa fa-check"></i><b>4.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="4.5.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-fitting-1"><i class="fa fa-check"></i><b>4.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="4.5.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#interpreting-the-model"><i class="fa fa-check"></i><b>4.5.4</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-random-effects"><i class="fa fa-check"></i><b>4.6</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-inspecting-random-effects"><i class="fa fa-check"></i><b>4.6.1</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-simulating"><i class="fa fa-check"></i><b>4.7</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="4.8" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-second-random-effect"><i class="fa fa-check"></i><b>4.8</b> Adding a second random effect</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-updating-model"><i class="fa fa-check"></i><b>4.8.1</b> Updating the model description</a></li>
<li class="chapter" data-level="4.8.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-and-interpreting-the-model"><i class="fa fa-check"></i><b>4.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-investigating-shrinkage"><i class="fa fa-check"></i><b>4.9</b> Investigating ‘shrinkage’</a></li>
<li class="chapter" data-level="4.10" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-answering-question"><i class="fa fa-check"></i><b>4.10</b> Answering our research questions</a></li>
<li class="chapter" data-level="4.11" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-frequentist"><i class="fa fa-check"></i><b>4.11</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-vs-lmer"><i class="fa fa-check"></i><b>4.11.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#exercises-3"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
<li class="chapter" data-level="4.13" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#references-2"><i class="fa fa-check"></i><b>4.13</b> References</a></li>
<li class="chapter" data-level="4.14" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#plot-code-3"><i class="fa fa-check"></i><b>4.14</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html"><i class="fa fa-check"></i><b>5</b> Comparing two groups of observations: Factors and contrasts</a>
<ul>
<li class="chapter" data-level="5.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#chapter-pre-cap-4"><i class="fa fa-check"></i><b>5.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="5.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#comparing-two-groups"><i class="fa fa-check"></i><b>5.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="5.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#distribution-of-repeated-measures-across-factor-levels"><i class="fa fa-check"></i><b>5.3</b> Distribution of repeated measures across factor levels</a></li>
<li class="chapter" data-level="5.4" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-data-and-qs"><i class="fa fa-check"></i><b>5.4</b> Data and research questions</a></li>
<li class="chapter" data-level="5.5" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-two-means"><i class="fa fa-check"></i><b>5.5</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-the-model"><i class="fa fa-check"></i><b>5.5.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>5.5.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-contrasts"><i class="fa fa-check"></i><b>5.6</b> Contrasts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-treatment-coding"><i class="fa fa-check"></i><b>5.6.1</b> Treatment coding</a></li>
<li class="chapter" data-level="5.6.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-sum-coding"><i class="fa fa-check"></i><b>5.6.2</b> Sum coding</a></li>
<li class="chapter" data-level="5.6.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-comparison-sum-treatment"><i class="fa fa-check"></i><b>5.6.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-refittin-sum"><i class="fa fa-check"></i><b>5.7</b> Sum coding and the decomposition of variation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-description-1"><i class="fa fa-check"></i><b>5.7.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.7.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-the-model-1"><i class="fa fa-check"></i><b>5.7.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.7.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>5.7.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-working-with-posteriors"><i class="fa fa-check"></i><b>5.8</b> Inspecting and manipulating the posterior samples</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-using-hypothesis"><i class="fa fa-check"></i><b>5.8.1</b> Using the <em>hypothesis</em> function</a></li>
<li class="chapter" data-level="5.8.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-manipulating-random-effects"><i class="fa fa-check"></i><b>5.8.2</b> Working with the random effects</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-robustness"><i class="fa fa-check"></i><b>5.9</b> Making our models more robust: The (non-standardized) t distribution</a></li>
<li class="chapter" data-level="5.10" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#re-fitting-with-t-distributed-errors."><i class="fa fa-check"></i><b>5.10</b> Re-fitting with t-distributed errors</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#description-of-the-model-1"><i class="fa fa-check"></i><b>5.10.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.10.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-and-interpreting-the-model-1"><i class="fa fa-check"></i><b>5.10.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-simulating"><i class="fa fa-check"></i><b>5.11</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="5.12" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-answering-qs"><i class="fa fa-check"></i><b>5.12</b> Answering our research questions</a></li>
<li class="chapter" data-level="5.13" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-frequentist"><i class="fa fa-check"></i><b>5.13</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#bayesian-multilevel-models-vs.-lmer"><i class="fa fa-check"></i><b>5.13.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#exercises-4"><i class="fa fa-check"></i><b>5.14</b> Exercises</a></li>
<li class="chapter" data-level="5.15" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#plot-code-4"><i class="fa fa-check"></i><b>5.15</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html"><i class="fa fa-check"></i><b>6</b> Variation in parameters (‘random effects’) and model comparison</a>
<ul>
<li class="chapter" data-level="6.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#chapter-pre-cap-5"><i class="fa fa-check"></i><b>6.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="6.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-data-and-qs"><i class="fa fa-check"></i><b>6.2</b> Data and research questions</a></li>
<li class="chapter" data-level="6.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-variation-sources"><i class="fa fa-check"></i><b>6.3</b> Variation in parameters across sources of data</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#description-of-our-model"><i class="fa fa-check"></i><b>6.3.1</b> Description of our model</a></li>
<li class="chapter" data-level="6.3.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-correlations"><i class="fa fa-check"></i><b>6.3.2</b> Correlations between random parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-random-and-mvn"><i class="fa fa-check"></i><b>6.3.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-mvn-priors"><i class="fa fa-check"></i><b>6.3.4</b> Specifying priors for a multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.5" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#updating-our-model-description"><i class="fa fa-check"></i><b>6.3.5</b> Updating our model description</a></li>
<li class="chapter" data-level="6.3.6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-fitting"><i class="fa fa-check"></i><b>6.3.6</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-model-comparison"><i class="fa fa-check"></i><b>6.4</b> Model Comparison</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-in-and-out-prediction"><i class="fa fa-check"></i><b>6.4.1</b> In-sample and out-of-sample prediction</a></li>
<li class="chapter" data-level="6.4.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-adjust"><i class="fa fa-check"></i><b>6.4.2</b> Out-of-sample prediction: Adjusting predictive accuracy</a></li>
<li class="chapter" data-level="6.4.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-crossval"><i class="fa fa-check"></i><b>6.4.3</b> Out-of-sample prediction: Cross validation</a></li>
<li class="chapter" data-level="6.4.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#selecting-a-model"><i class="fa fa-check"></i><b>6.4.4</b> Selecting a model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-answering"><i class="fa fa-check"></i><b>6.5</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-frequentist"><i class="fa fa-check"></i><b>6.6</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-vs-lmer"><i class="fa fa-check"></i><b>6.6.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#references-3"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
<li class="chapter" data-level="6.9" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#plot-code-5"><i class="fa fa-check"></i><b>6.9</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><i class="fa fa-check"></i><b>7</b> Comparing many groups, interactions, and posterior predictive checks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#chapter-pre-cap-6"><i class="fa fa-check"></i><b>7.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="7.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions"><i class="fa fa-check"></i><b>7.2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.2.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-description-1"><i class="fa fa-check"></i><b>7.2.2</b> Description of our model</a></li>
<li class="chapter" data-level="7.2.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-2"><i class="fa fa-check"></i><b>7.2.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-multiple-factors-simultaneously"><i class="fa fa-check"></i><b>7.3</b> Investigating multiple factors simultaneously</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>7.3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.3.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-the-model-2"><i class="fa fa-check"></i><b>7.3.2</b> Description of the model</a></li>
<li class="chapter" data-level="7.3.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-3"><i class="fa fa-check"></i><b>7.3.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-posterior-prediction"><i class="fa fa-check"></i><b>7.4</b> Posterior prediction: Using our models to predict new data</a></li>
<li class="chapter" data-level="7.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-interactions-and-plots"><i class="fa fa-check"></i><b>7.5</b> Interactions and interaction plots</a></li>
<li class="chapter" data-level="7.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>7.6</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>7.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.6.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#model-formulas"><i class="fa fa-check"></i><b>7.6.2</b> Model formulas</a></li>
<li class="chapter" data-level="7.6.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-our-model-1"><i class="fa fa-check"></i><b>7.6.3</b> Description of our model</a></li>
<li class="chapter" data-level="7.6.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-4"><i class="fa fa-check"></i><b>7.6.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="7.6.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-calc-means"><i class="fa fa-check"></i><b>7.6.5</b> Caulculating group means in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#calculating-simple-effects-in-the-presence-of-interactions"><i class="fa fa-check"></i><b>7.6.6</b> Calculating simple effects in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#assessing-model-fit-bayesian-r2"><i class="fa fa-check"></i><b>7.6.7</b> Assessing model fit: Bayesian <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-answering"><i class="fa fa-check"></i><b>7.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="7.8" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.8</b> Factors with more than two levels</a></li>
<li class="chapter" data-level="7.9" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-frequentist"><i class="fa fa-check"></i><b>7.9</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#bayesian-multilevel-models-vs.-lmer-1"><i class="fa fa-check"></i><b>7.9.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#exercises-6"><i class="fa fa-check"></i><b>7.10</b> Exercises</a></li>
<li class="chapter" data-level="7.11" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#references-4"><i class="fa fa-check"></i><b>7.11</b> References</a></li>
<li class="chapter" data-level="7.12" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#plot-code-6"><i class="fa fa-check"></i><b>7.12</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html"><i class="fa fa-check"></i><b>8</b> Varying variances, more about priors, and prior predictive checks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#chapter-pre-cap-7"><i class="fa fa-check"></i><b>8.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>8.2</b> Data and Research questions</a></li>
<li class="chapter" data-level="8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-about-priors"><i class="fa fa-check"></i><b>8.3</b> More about priors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-prior-prediction"><i class="fa fa-check"></i><b>8.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.3.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-specific-priors"><i class="fa fa-check"></i><b>8.3.2</b> More specific priors</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#heteroskedasticity-and-distributional-or-mixture-models"><i class="fa fa-check"></i><b>8.4</b> Heteroskedasticity and distributional (or mixture) models</a></li>
<li class="chapter" data-level="8.5" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-simple-model-error-varies-according-to-a-single-fixed-effect"><i class="fa fa-check"></i><b>8.5</b> A ‘simple’ model: Error varies according to a single fixed effect</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#description-of-our-model-2"><i class="fa fa-check"></i><b>8.5.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.5.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#prior-predictive-checks"><i class="fa fa-check"></i><b>8.5.2</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.5.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-5"><i class="fa fa-check"></i><b>8.5.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-complex-model-error-varies-according-to-fixed-and-random-effects"><i class="fa fa-check"></i><b>8.6</b> A ‘complex’ model: Error varies according to fixed and random effects</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2"><i class="fa fa-check"></i><b>8.6.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.6.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-6"><i class="fa fa-check"></i><b>8.6.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#answering-our-research-questions"><i class="fa fa-check"></i><b>8.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="8.8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability"><i class="fa fa-check"></i><b>8.8</b> Building identifiable and supportable models</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#collinearity"><i class="fa fa-check"></i><b>8.8.1</b> Collinearity</a></li>
<li class="chapter" data-level="8.8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#predictable-values-of-categorical-predictors"><i class="fa fa-check"></i><b>8.8.2</b> Predictable values of categorical predictors</a></li>
<li class="chapter" data-level="8.8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#saturated-and-nearly-saturated-models"><i class="fa fa-check"></i><b>8.8.3</b> Saturated, and ‘nearly-saturated’, models</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#exercises-7"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
<li class="chapter" data-level="8.10" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#references-5"><i class="fa fa-check"></i><b>8.10</b> References</a></li>
<li class="chapter" data-level="8.11" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#plot-code-7"><i class="fa fa-check"></i><b>8.11</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html"><i class="fa fa-check"></i><b>9</b> Quantitative predictors and their interactions with factors</a>
<ul>
<li class="chapter" data-level="9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#chapter-pre-cap-8"><i class="fa fa-check"></i><b>9.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>9.2</b> Data and research questions</a></li>
<li class="chapter" data-level="9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#modeling-variation-along-lines"><i class="fa fa-check"></i><b>9.3</b> Modeling variation along lines</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-3"><i class="fa fa-check"></i><b>9.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.3.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-centering"><i class="fa fa-check"></i><b>9.3.2</b> Centering quantitative predictors</a></li>
<li class="chapter" data-level="9.3.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-an-interpreting-the-model"><i class="fa fa-check"></i><b>9.3.3</b> Fitting an interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>9.4</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-4"><i class="fa fa-check"></i><b>9.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.4.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-7"><i class="fa fa-check"></i><b>9.4.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.4.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-shared-non-zero-slopes"><i class="fa fa-check"></i><b>9.4.3</b> Interpreting group effects in the presence of shared (non-zero) slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>9.5</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-5"><i class="fa fa-check"></i><b>9.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.5.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-3"><i class="fa fa-check"></i><b>9.5.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.5.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-varying-slopes"><i class="fa fa-check"></i><b>9.5.3</b> Interpreting group effects in the presence of varying slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-interim-discussion"><i class="fa fa-check"></i><b>9.6</b> Answering our research questions: Interim discussion</a></li>
<li class="chapter" data-level="9.7" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-updated"><i class="fa fa-check"></i><b>9.7</b> Data and research questions: Updated</a></li>
<li class="chapter" data-level="9.8" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-intercepts-and-slopes-for-each-level-of-a-grouping-factor-i.e.-random-slopes"><i class="fa fa-check"></i><b>9.8</b> Models with intercepts and slopes for each level of a grouping factor (i.e. ‘random slopes’)</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-6"><i class="fa fa-check"></i><b>9.8.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.8.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-8"><i class="fa fa-check"></i><b>9.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-multiple-predictors-for-each-level-of-a-grouping-factor"><i class="fa fa-check"></i><b>9.9</b> Models with multiple predictors for each level of a grouping factor</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-7"><i class="fa fa-check"></i><b>9.9.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-5"><i class="fa fa-check"></i><b>9.9.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#model-selection"><i class="fa fa-check"></i><b>9.9.3</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-updated"><i class="fa fa-check"></i><b>9.10</b> Answering our research questions: Updated</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#a-word-on-causality"><i class="fa fa-check"></i><b>9.10.1</b> A word on causality</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#exercises-8"><i class="fa fa-check"></i><b>9.11</b> Exercises</a></li>
<li class="chapter" data-level="9.12" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#references-6"><i class="fa fa-check"></i><b>9.12</b> References</a></li>
<li class="chapter" data-level="9.13" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#plot-code-8"><i class="fa fa-check"></i><b>9.13</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html"><i class="fa fa-check"></i><b>10</b> Logistic regression and signal detection theory models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#chapter-pre-cap-9"><i class="fa fa-check"></i><b>10.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous"><i class="fa fa-check"></i><b>10.2</b> Dichotomous variables and data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#generalizing-our-linear-models"><i class="fa fa-check"></i><b>10.3</b> Generalizing our linear models</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logits"><i class="fa fa-check"></i><b>10.4.1</b> Logits</a></li>
<li class="chapter" data-level="10.4.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit"><i class="fa fa-check"></i><b>10.4.2</b> The inverse logit link function</a></li>
<li class="chapter" data-level="10.4.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#building-intuitions-about-logits-and-the-inverse-logit-function"><i class="fa fa-check"></i><b>10.4.3</b> Building intuitions about logits and the inverse logit function</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-with-one-quantitative-predictor"><i class="fa fa-check"></i><b>10.5</b> Logistic regression with one quantitative predictor</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>10.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.5.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-8"><i class="fa fa-check"></i><b>10.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.5.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-0"><i class="fa fa-check"></i><b>10.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="10.5.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1"><i class="fa fa-check"></i><b>10.5.4</b> Interpreting the model</a></li>
<li class="chapter" data-level="10.5.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification"><i class="fa fa-check"></i><b>10.5.5</b> Using logistic models to understand classification</a></li>
<li class="chapter" data-level="10.5.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-question"><i class="fa fa-check"></i><b>10.5.6</b> Answering our research question</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#measuring-sensitivity-and-bias"><i class="fa fa-check"></i><b>10.6</b> Measuring sensitivity and bias</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-6"><i class="fa fa-check"></i><b>10.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.6.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-9"><i class="fa fa-check"></i><b>10.6.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.6.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#fitting-and-interpreting-the-model-9"><i class="fa fa-check"></i><b>10.6.3</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="10.6.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>10.6.4</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#exercises-9"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
<li class="chapter" data-level="10.8" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#references-7"><i class="fa fa-check"></i><b>10.8</b> References</a></li>
<li class="chapter" data-level="10.9" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#plot-code-9"><i class="fa fa-check"></i><b>10.9</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><i class="fa fa-check"></i><b>11</b> Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#chapter-pre-cap-10"><i class="fa fa-check"></i><b>11.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#models-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.2</b> Models with multiple quantitative predictors</a></li>
<li class="chapter" data-level="11.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#interactions-between-quantitative-predictors"><i class="fa fa-check"></i><b>11.3</b> Interactions between quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#centering-quantitative-predictors-when-including-interactions"><i class="fa fa-check"></i><b>11.3.1</b> Centering quantitative predictors when including interactions</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-7"><i class="fa fa-check"></i><b>11.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="11.3.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-description-1"><i class="fa fa-check"></i><b>11.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="11.3.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-the-model-2"><i class="fa fa-check"></i><b>11.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="11.3.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#advantages-of-bayesian-multilevel-models-for-large-models"><i class="fa fa-check"></i><b>11.3.5</b> Advantages of Bayesian multilevel models for large models</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-BANOVA"><i class="fa fa-check"></i><b>11.4</b> Bayesian Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#getting-the-standard-deviations-from-our-models-manually"><i class="fa fa-check"></i><b>11.4.1</b> Getting the standard deviations from our models ‘manually’</a></li>
<li class="chapter" data-level="11.4.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#using-the-banova-function"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>banova</code> function</a></li>
<li class="chapter" data-level="11.4.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-comparing-the-reduced-model"><i class="fa fa-check"></i><b>11.4.3</b> Fitting and comparing the reduced model</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#a-logistic-regression-model-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.5</b> A logistic regression model with multiple quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-8"><i class="fa fa-check"></i><b>11.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="11.5.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#description-of-the-model-10"><i class="fa fa-check"></i><b>11.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="11.5.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-the-model-and-applying-a-bayesian-anova"><i class="fa fa-check"></i><b>11.5.3</b> Fitting and the model and applying a Bayesian ANOVA</a></li>
<li class="chapter" data-level="11.5.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c12-2d-categorization"><i class="fa fa-check"></i><b>11.5.4</b> Categorization in two dimensions</a></li>
<li class="chapter" data-level="11.5.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#model-selection-and-misspecification"><i class="fa fa-check"></i><b>11.5.5</b> Model selection and misspecification</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#exercises-10"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#references-8"><i class="fa fa-check"></i><b>11.7</b> References</a></li>
<li class="chapter" data-level="11.8" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#plot-code-10"><i class="fa fa-check"></i><b>11.8</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial and Ordinal regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#chapter-pre-cap-11"><i class="fa fa-check"></i><b>12.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="12.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logits-and-the-softmax-function"><i class="fa fa-check"></i><b>12.2.1</b> Multinomial logits and the softmax function</a></li>
<li class="chapter" data-level="12.2.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#comparison-to-logistic-regression"><i class="fa fa-check"></i><b>12.2.2</b> Comparison to logistic regression</a></li>
<li class="chapter" data-level="12.2.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-9"><i class="fa fa-check"></i><b>12.2.3</b> Data and research questions</a></li>
<li class="chapter" data-level="12.2.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-our-model-3"><i class="fa fa-check"></i><b>12.2.4</b> Description of our model</a></li>
<li class="chapter" data-level="12.2.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-the-model-3"><i class="fa fa-check"></i><b>12.2.5</b> Fitting the model</a></li>
<li class="chapter" data-level="12.2.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>12.2.6</b> Interpreting the model</a></li>
<li class="chapter" data-level="12.2.7" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-multinomial-territorial-maps"><i class="fa fa-check"></i><b>12.2.7</b> Multinomial models and territorial maps</a></li>
<li class="chapter" data-level="12.2.8" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#refitting-the-model-without-speaker-random-effects"><i class="fa fa-check"></i><b>12.2.8</b> Refitting the model without speaker random effects</a></li>
<li class="chapter" data-level="12.2.9" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-2"><i class="fa fa-check"></i><b>12.2.9</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Ordinal (logistic) regression</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-cumulative-density"><i class="fa fa-check"></i><b>12.3.1</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="12.3.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-10"><i class="fa fa-check"></i><b>12.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="12.3.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-the-model-11"><i class="fa fa-check"></i><b>12.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="12.3.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-and-interpreting-the-model-10"><i class="fa fa-check"></i><b>12.3.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="12.3.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#listener-specific-discrimination-terms"><i class="fa fa-check"></i><b>12.3.5</b> Listener-specific discrimination terms</a></li>
<li class="chapter" data-level="12.3.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-3"><i class="fa fa-check"></i><b>12.3.6</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#exercises-11"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
<li class="chapter" data-level="12.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#references-9"><i class="fa fa-check"></i><b>12.5</b> References</a></li>
<li class="chapter" data-level="12.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#plot-code-11"><i class="fa fa-check"></i><b>12.6</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><i class="fa fa-check"></i><b>13</b> Writing up experiments: An investigation of the perception of apparent speaker characteristics from speech acoustics</a>
<ul>
<li class="chapter" data-level="13.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#fundamental-frequency-and-voice-pitch"><i class="fa fa-check"></i><b>13.1.1</b> Fundamental frequency and voice pitch</a></li>
<li class="chapter" data-level="13.1.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-fundamental-frequency-between-speakers"><i class="fa fa-check"></i><b>13.1.2</b> Variation in fundamental frequency between speakers</a></li>
<li class="chapter" data-level="13.1.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#voice-resonance-and-vocal-tract-length"><i class="fa fa-check"></i><b>13.1.3</b> Voice resonance and vocal-tract length</a></li>
<li class="chapter" data-level="13.1.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-estimating-vtl"><i class="fa fa-check"></i><b>13.1.4</b> Estimating vocal-tracts length from speech</a></li>
<li class="chapter" data-level="13.1.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-vocal-tract-length-between-speakers"><i class="fa fa-check"></i><b>13.1.5</b> Variation in vocal-tract length between speakers</a></li>
<li class="chapter" data-level="13.1.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-perception-of-chars"><i class="fa fa-check"></i><b>13.1.6</b> Perception of age, gender and size</a></li>
<li class="chapter" data-level="13.1.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#category-dependent-behavior"><i class="fa fa-check"></i><b>13.1.7</b> Category-dependent behavior</a></li>
<li class="chapter" data-level="13.1.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-current-experiment"><i class="fa fa-check"></i><b>13.1.8</b> The current experiment</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#methods"><i class="fa fa-check"></i><b>13.2</b> Methods</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#participants"><i class="fa fa-check"></i><b>13.2.1</b> Participants</a></li>
<li class="chapter" data-level="13.2.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-stimuli"><i class="fa fa-check"></i><b>13.2.2</b> Stimuli</a></li>
<li class="chapter" data-level="13.2.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#procedure"><i class="fa fa-check"></i><b>13.2.3</b> Procedure</a></li>
<li class="chapter" data-level="13.2.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#data-screening"><i class="fa fa-check"></i><b>13.2.4</b> Data screening</a></li>
<li class="chapter" data-level="13.2.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#loading-the-data-and-packages"><i class="fa fa-check"></i><b>13.2.5</b> Loading the data and packages</a></li>
<li class="chapter" data-level="13.2.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-height"><i class="fa fa-check"></i><b>13.2.6</b> Statistical Analysis: Apparent height</a></li>
<li class="chapter" data-level="13.2.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-gender"><i class="fa fa-check"></i><b>13.2.7</b> Statistical Analysis: Apparent gender</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-height-judgments"><i class="fa fa-check"></i><b>13.3</b> Results: Apparent height judgments</a></li>
<li class="chapter" data-level="13.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-height"><i class="fa fa-check"></i><b>13.4</b> Discussion: Apparent height</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#age-dependent-use-of-vtl-cues-on-apparent-height"><i class="fa fa-check"></i><b>13.4.1</b> Age-dependent use of VTL cues on apparent height</a></li>
<li class="chapter" data-level="13.4.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-effect-for-apparent-gender-on-apparent-height"><i class="fa fa-check"></i><b>13.4.2</b> The effect for apparent gender on apparent height</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-height-judgments"><i class="fa fa-check"></i><b>13.5</b> Conclusion: Apparent height judgments</a></li>
<li class="chapter" data-level="13.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.6</b> Results: Apparent gender judgments</a></li>
<li class="chapter" data-level="13.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.7</b> Discussion: Apparent gender judgments</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#effect-of-apparent-age-on-the-perception-of-femaleness"><i class="fa fa-check"></i><b>13.7.1</b> Effect of apparent age on the perception of femaleness</a></li>
<li class="chapter" data-level="13.7.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#between-listener-variation-in-gender-perception"><i class="fa fa-check"></i><b>13.7.2</b> Between-listener variation in gender perception</a></li>
<li class="chapter" data-level="13.7.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#beyond-gross-acoustic-cues-in-gender-perception"><i class="fa fa-check"></i><b>13.7.3</b> Beyond gross acoustic cues in gender perception</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-gender"><i class="fa fa-check"></i><b>13.8</b> Conclusion: Apparent gender</a></li>
<li class="chapter" data-level="13.9" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#next-steps"><i class="fa fa-check"></i><b>13.9</b> Next steps</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#research-design-variable-selection-etc."><i class="fa fa-check"></i><b>13.9.1</b> Research design, variable selection, etc.</a></li>
<li class="chapter" data-level="13.9.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#non-linear-models"><i class="fa fa-check"></i><b>13.9.2</b> Non-linear models</a></li>
<li class="chapter" data-level="13.9.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#other-data-distributions"><i class="fa fa-check"></i><b>13.9.3</b> Other data distributions</a></li>
<li class="chapter" data-level="13.9.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#multivariate-analyses"><i class="fa fa-check"></i><b>13.9.4</b> Multivariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#references-10"><i class="fa fa-check"></i><b>13.10</b> References</a></li>
<li class="chapter" data-level="13.11" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#plot-code-12"><i class="fa fa-check"></i><b>13.11</b> Plot Code</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/santiagobarreda/bmmrmd" target="blank">Book GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression-and-signal-detection-theory-models" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Logistic regression and signal detection theory models<a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-and-signal-detection-theory-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>To this point we’ve only discussed the prediction of quantitative variables that can take on a large number of values and are measured on an interval (or ratio) scale. In this chapter we begin talking about the prediction of categorical variables, variables that take on a (usually small) number of discrete values. For now, we will focus on dichotomous (i.e. binary) outcomes, however, the ideas presented will be extended to the prediction of ordinal (ordered categories such as 1st, 2nd, 3rd), and multinomial (unordered categories such as English, French and Spanish) data in chapter 12. In the second half of the chapter, we will discuss how logistic regression models can be used to investigate the discrimination of categories, and response bias, using signal detection theory.</p>
<div id="chapter-pre-cap-9" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Chapter pre-cap<a href="logistic-regression-and-signal-detection-theory-models.html#chapter-pre-cap-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter introduces linear models for the prediction of dichotomous variables, that is, variables that can take on one of two possible discrete values. First, we introduce dichotomous data and the Bernoulli and binomial distributions. After that, link functions and the generalized linear model are discussed. We present logistic regression, discuss logits and their characteristics, and introduce the inverse logit function, the link function for logistic regression. Following this, we fit a multilevel logistic regression model with a single quantitative predictor, and use this model to understand the predictions made by our model across the stimulus space, resulting in territorial maps. Finally, we introduce the use of logistic regression to fit a signal detection theory model, estimating response bias and sensitivity using a multilevel logistic regression model.</p>
</div>
<div id="c10-dichotomous" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Dichotomous variables and data<a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The models we fit in chapter 9 featured linear relationships between our predictor and the expected value of the dependent variable (<span class="math inline">\(\mu\)</span>). For a model predicting apparent height using speaker vocal-tract length (VTL), like the ones we fit in chapter 9, this means the <span class="math inline">\(\mu\)</span> parameter of a normal distribution slides along straight lines as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-1">(10.1)</a>.</p>
<p><span class="math display" id="eq:10-1">\[
\begin{equation}
\begin{split}
\mathrm{height}_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\
\mu_{[i]} = \mathrm{Intercept} + VTL \cdot \mathrm{vtl}_{[i]}
\end{split}
\tag{10.1}
\end{equation}
\]</span></p>
<p>We can see an example of a linear relationship in the top left plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-1">10.1</a> where expected apparent height (<span class="math inline">\(\mu\)</span>) varies along a line as a function of vocal-tract length (VTL). In this model, the observations we collect are assumed to be normally, and independently, distributed around the expected value (the line). Importantly, our predicted values <em>directly</em> model the values we’re interested in. What we mean by this is that the values along our line are the actual apparent heights we expect to observe for a given value of VTL.</p>
<div class="figure"><span style="display:block;" id="fig:F10-1"></span>
<img src="_main_files/figure-html/F10-1-1.jpeg" alt="(top left) Average apparent height for each speaker against speaker VTL. Point color represents veridical speaker category. (top right) Individual gender identifications plotted according to speaker VTL. Female responses were given a value of 1, male responses 0. (bottom left) Probability of a female response as a function of speaker VTL. (bottom right) Logit of the probability of a female response as a function of speaker VTL." width="4800" />
<p class="caption">
Figure 10.1: (top left) Average apparent height for each speaker against speaker VTL. Point color represents veridical speaker category. (top right) Individual gender identifications plotted according to speaker VTL. Female responses were given a value of 1, male responses 0. (bottom left) Probability of a female response as a function of speaker VTL. (bottom right) Logit of the probability of a female response as a function of speaker VTL.
</p>
</div>
<p>Unlike quantitative variables, <strong>dichotomous</strong> variables can only take on two different values, and these are not measured on an interval scale. We can easily think of many examples of this kind of data: A response to an item on a test that is wrong or right, a person who is either male or female, or someone who is an adult or a child. None of these examples are meant to suggest that reality is this simple. For example, males and females are not two discrete and internally homogeneous classes that fully explain variation in human sex or gender. Obviously, variation in human age and development is more complicated than adult or child. And, in fact, in real life it is common that situations or questions arise that cannot simply be labelled as ‘wrong’ or ‘right’. Despite this, things like gender and age can be <em>coded</em> as dichotomous variables so that they have only two possible values within the context of our model.</p>
<p>The decision to represent these groups of speakers using binary variables is just as artificial as assuming that apparent height and VTL relate perfectly along a line, assuming that our errors are normally distributed and so on. Despite this, we can use our models to look for statistical associations between the dependent variable and our predictors <em>given the structure of our model</em>, that is, based on the simplifying assumptions we have made in order to turn an infinitely complicated reality into a regression model with a small number of parameters.</p>
<p>When your variable has only two categorical outcomes (within the context of your model), you need to find a way to represent these numerically. The way this is usually done is by coding one category as 1 (a ‘success’) and the other as 0 (a ‘failure’). The designation of one category as 1 and the other as 0 will not affect your analysis in any meaningful way (it will flip the sign of most coefficients) and can be based on what ‘makes sense’ given the analysis at hand. In our case we will define a new variable called <code>Female</code> and assign 1 to cases where listeners identified a speaker as female, thereby associating female responses with ‘successes’. Keep in mind, this variable does not reflect whether the speaker <em>was</em> female but that the listener <em>thought</em> the speaker was female.</p>
<p>The distribution of dichotomous female (1) and male (0) responses with respect to speaker VTL is presented in the top right plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-1">10.1</a>. Plotting ones and zeros against speaker VTL is not very informative, in addition, using a line to directly predict dichotomous data leads to strange outcomes. For example, our line predicts all sorts of values between 0 and 1 that our variable can never actually have. It also suggests a continuous, gradual change in the value of our dependent variable with respect to VTL, which is also impossible.</p>
<p>We can make the situation a bit better by finding the average value of our dichotomous variable for each speaker, where female responses equal 1 and male responses equal 0 (bottom left plot in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-1">10.1</a>). When we do this, we obtain the probability (<span class="math inline">\(p\)</span>) of observing a female response for each speaker. For example, imagine you are playing basketball and keep track of 1000 free throws over several practices. Imagine you sink 753 of these shots, and let’s assume that this represents your actual ability fairly well. You consider these 1000 shots observations of the variable “successful free throw” which equals 1 when you make it and 0 when you don’t. If you add up all your made shots (753) and divide by the total number of observations (1000), you can conclude that there is a 0.753 (753/1000) probability of you sinking a free throw (i.e. (<span class="math inline">\(p=0.753\)</span>)).</p>
<p>Since the <span class="math inline">\(p\)</span> parameter is the ‘average’ made shot, this seems similar to the <span class="math inline">\(\mu\)</span> parameter in a normal distribution. However, the <span class="math inline">\(p\)</span> parameter can’t be used with a normal distribution to generate dichotomous data. A normal distribution generates continuous variation symmetrically around its mean. Instead, we need a distribution that takes a parameter like <span class="math inline">\(p\)</span> and generates a variable with only two possible values. The two distributions most commonly used to model dichotomous variables are the <em>Bernoulli</em> distribution and the <em>binomial</em> distribution.</p>
<p>The <strong>Bernoulli distribution</strong> generates individual dichotomous outcomes. This distribution has only one parameter: The probability of a success (<span class="math inline">\(p\)</span>). The <span class="math inline">\(p\)</span> parameter also represents the <em>expected value</em> of the Bernoulli distribution. Imagine you have ten observations from a Bernoulli variable and you observe 6 ones and 4 zeros. This suggests a probability of observing a success, <span class="math inline">\(p\)</span>, of 0.6 (6/10). The expected value (<span class="math inline">\(\mathbb{E}\)</span>) of a discrete variable is the sum of the values it can take times the probability of each value. For a Bernoulli variable this is relatively simple to calculate as it involves the addition of only two values. In <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-2">(10.2)</a>, we see that to find the expected value of a Bernoulli variable <span class="math inline">\(y\)</span>, we multiply the probability of a success (<span class="math inline">\(p\)</span>) times one, the probability of a failure (<span class="math inline">\(1-p\)</span>) times zero, and add the two. Since the probability of failures is multiplied by zero, the expected value of the Bernoulli variable is simply equal to <span class="math inline">\(p\)</span>.</p>
<p><span class="math display" id="eq:10-2">\[
\begin{equation}
\begin{split}
\mathbb{E}(y) = \sum_{i=1}^{2} y_{[i]} P(y_{[i]}) \\
\mathbb{E}(y) = (1 \cdot p)+(0 \cdot (1-p)) \\
\mathbb{E}(y) = p
\end{split}
\tag{10.2}
\end{equation}
\]</span></p>
<p>Below, we use the <code>rbernoulli</code> function to generate random Bernoulli variables. First we generate a single Bernoulli variable (a <strong>Bernoulli trial</strong>) and then ten variables with the same probability of success.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a single trial, probability of 0.5</span></span>
<span id="cb375-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-2" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">rbernoulli</span> (<span class="dv">1</span>,.<span class="dv">5</span>)</span>
<span id="cb375-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0</span></span>
<span id="cb375-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb375-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ten single trials, probability of 0.5</span></span>
<span id="cb375-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-6" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">rbernoulli</span> (<span class="dv">10</span>,.<span class="dv">5</span>)</span>
<span id="cb375-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb375-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 0 0 0 0 0 0 1 0 1 0</span></span></code></pre></div>
<p>The <strong>binomial distribution</strong> generates a <em>set</em> (batch) of dichotomous outcomes. This distribution has two parameters: The probability of a success (<span class="math inline">\(p\)</span>) and the number of trials (<span class="math inline">\(n\)</span>). The Bernoulli distribution is a special case of the binomial distribution where <span class="math inline">\(n\)</span> is always one. If you were playing basketball and you took 5 free throws and made 3 (3/5=0.6), then the <span class="math inline">\(p\)</span> parameter suggested by your data is 0.6 and the <span class="math inline">\(n\)</span> parameter is 5. If you use this distribution, you are treating all 5 trials as a single observation. This means that your data is 3 (out of five) and not 0, 1, 1, 0, and 1 (or whatever). In this case, another 5 shots would constitute one more ‘observation’ summarized by the total number of successes. The expected value of the binomial distribution is <span class="math inline">\(np\)</span> since a single observation of a binomial variable is equal to <span class="math inline">\(n\)</span> Bernoulli trials, and the expected value of a single Bernoulli trial is <span class="math inline">\(p\)</span>.</p>
<p>Below we generate random binomial variables using the <code>rbinom</code> (random binomial) function, which takes parameters in this order: <code>number of observations, size, probability of success</code>. We can compare the data generated by the Bernoulli and the binomial distributions. In the top row we get a single number, the total number of successes in the trials. We don’t get any information about what happened on any individual trial. In the bottom row we do get information about what happens on each individual trial.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a single batch of 10 trials, probability of 0.5</span></span>
<span id="cb376-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span> (<span class="dv">1</span>,<span class="dv">10</span>,.<span class="dv">5</span>)</span>
<span id="cb376-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6</span></span>
<span id="cb376-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb376-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ten individual trials, probability of 0.5</span></span>
<span id="cb376-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-6" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">rbernoulli</span> (<span class="dv">10</span>,.<span class="dv">5</span>)</span>
<span id="cb376-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb376-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 1 1 0 0 0 0 0 0 0 1</span></span></code></pre></div>
<p>So, when we have a dichotomous dependent variable and are interested in predicting individual trials, we use a Bernoulli distribution and our data model is <span class="math inline">\(y \sim \mathrm{Bernoulli} (p)\)</span>. When our dependent variable consists of groups of Bernoulli trials treated as a single unit we can use a binomial distribution. If this is the case our data model looks like this <span class="math inline">\(y \sim \mathrm{Binomial} (p, n)\)</span>, and keep in mind <span class="math inline">\(n\)</span> is <em>data</em> (i.e. is provided by the researcher) and not an parameter that is estimated by the model.</p>
<p>Before moving on we want to note that unlike the normal distribution, the Bernoulli and binomial distributions do not generate individual data points near their expected value <span class="math inline">\(p\)</span>. Instead, they generate sequences of 1s and 0s whose sample estimate of <span class="math inline">\(p\)</span> is expected to be close to its underlying value <em>hypothetically</em> given a large enough sample. For example, below we generate sequences of a dichotomous variable with a true <span class="math inline">\(p\)</span> of 0.5. In each case, the estimate of <span class="math inline">\(p\)</span> gets closer to the true value as the length of the sample gets longer, but it is never exactly equal to the true parameter value of 0.5.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbernoulli</span> (<span class="dv">10</span>,.<span class="dv">5</span>)) <span class="co"># the mean of 10 observations</span></span>
<span id="cb377-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.3</span></span>
<span id="cb377-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbernoulli</span> (<span class="dv">100</span>,.<span class="dv">5</span>))  <span class="co"># the mean of 100 observations</span></span>
<span id="cb377-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.47</span></span>
<span id="cb377-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbernoulli</span> (<span class="dv">1000</span>,.<span class="dv">5</span>))  <span class="co"># the mean of 1000 observations</span></span>
<span id="cb377-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.488</span></span>
<span id="cb377-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbernoulli</span> (<span class="dv">100000</span>,.<span class="dv">5</span>))  <span class="co"># the mean of 100000 observations</span></span>
<span id="cb377-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb377-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4999</span></span></code></pre></div>
</div>
<div id="generalizing-our-linear-models" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Generalizing our linear models<a href="logistic-regression-and-signal-detection-theory-models.html#generalizing-our-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The bottom left plot in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-1">10.1</a> shows the probability of observing a classification of ‘female’ for each speaker as a function of their vocal-tract length (VTL). We can see that the perception of femaleness is negatively related to speaker VTL: As VTL increases the probability of observing a female response decreases. Despite this, we can’t directly model variation in <span class="math inline">\(p\)</span> using lines as shown in the figure. There are many reasons for this but the main one is that using lines to model <span class="math inline">\(p\)</span> causes problems near 0 and 1. For example, imagine a predictor is associated with an increase in probability of 0.2 per unit change in the predictor. If the Intercept is 0.7, a two unit increase in the predictor is associated with a probability of 1.1, which is not possible. This suggests that a curve with a constant slope (e.g. a line) is fundamentally ill-suited to predict a bounded variable like probability.</p>
<p>The above seems problematic, after all, so far this book has been about linear regression models and we’re saying lines are no good for modeling variation in probabilities. The solution to this is to remember that our regression models consist of a bunch of components stuck together. Early on we discussed two general parts: The random component and the systematic component (see section <a href="fitting-bayesian-regression-models-with-brms.html#c3-what-is-reg">3.2</a>). The systematic component predicts variation in expected values using shapes like lines and planes. The random component specifies how our data randomly varies around the expected value. We can add a third component to this called a <strong>link function</strong>, a function that takes some parameter you’re interested in (e.g. <span class="math inline">\(p\)</span>) and transforms it so that the <em>transformed</em> value can be modeled along straight lines. When you rely on link functions, modeling becomes a three step process like this:</p>
<ol style="list-style-type: decimal">
<li><p>Predict variation in expected values (<span class="math inline">\(\theta\)</span>) along straight lines (or related shapes), for example <span class="math inline">\(\theta = a + b \cdot x\)</span>. This is the systematic component, and <span class="math inline">\(\theta\)</span> is linearly related to the predictor <span class="math inline">\(x\)</span>.</p></li>
<li><p>Transform <span class="math inline">\(\theta\)</span> using a link function. For example, given the link function <span class="math inline">\(f\)</span> we can transform our expected value <span class="math inline">\(\theta\)</span> to <span class="math inline">\(p\)</span> like so: <span class="math inline">\(p = f(\theta)\)</span>. The transformed parameter (<span class="math inline">\(p\)</span>) may no longer be linearly related to the dependent variable <span class="math inline">\(x\)</span> or the linear predictor <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Use the <em>transformed</em> parameter (<span class="math inline">\(p\)</span>) in the data generating distribution. For example <span class="math inline">\(y \sim \mathrm{Bernoulli}(p)\)</span>. This is the random component.</p></li>
</ol>
<p>Our link function literally <em>links</em> our systematic and random components: The random component does not <em>directly</em> use the information in the systematic component. The models we’ve been fitting so far featured what’s known as the <strong>identity</strong> link function. The identity link function is basically a function that does nothing, the input equals the output like <span class="math inline">\(\mu = f(\mu)\)</span>. Using the identity link function is equivalent to modeling <span class="math inline">\(\mu\)</span> directly in our prediction equation. This works for the mean parameter of the normal distribution because it can actually vary continuously from positive to negative infinity as lines do. However, it doesn’t work for the Bernoulli or binomial distributions since <span class="math inline">\(p\)</span> must fall between 0 and 1. This suggests that we’ll need to include a function (<span class="math inline">\(f\)</span>) that maps some continuous predicted value (<span class="math inline">\(\theta\)</span>) to a probability (<span class="math inline">\(p\)</span>), as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-3">(10.3)</a>.</p>
<p><span class="math display" id="eq:10-3">\[
\begin{equation}
\begin{split}
\mathrm{Female}_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = f(\theta_{[i]}) \\
\theta_{[i]} = \mathrm{Intercept} + VTL \cdot \mathrm{vtl}_{[i]} \\
\end{split}
\tag{10.3}
\end{equation}
\]</span></p>
<p>The presentation in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-3">(10.3)</a> separates our model into a random component, a link function, and a systematic component. Each of these components play a crucial role in our ability to build regression models that predict dichotomous variables. The random component is the distribution underlying the data-level error in our dependent variable. This component helps us account for the fact that our observations are not perfectly predictable given our independent variables, and that they can only equal 0 or 1. The systematic component allows us to make predictions about variability in the dependent variable using combinations of our predictors. The link function lets us use lines to model variation in parameters even when these may not vary in an unbounded manner.</p>
<p>This division of regression models into three discrete components is referred to as the <strong>generalized linear model</strong>. The generalized linear model allows the user to change the random component and/or link function as appropriate to model a wide range of data. It’s called the ‘generalized’ linear model because it represents the extrapolation of principles related to the prediction of quantitative variables coming from a normal distribution to the general case. For more information on the generalized linear model see Kruschke (2014) for a Bayesian introduction, or McCullagh and Nelder (2019) for a more advanced, and classic, treatment.</p>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Logistic Regression<a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Logistic regression</strong> is an approach to regression for predicting dichotomous dependent variables. Since it is extremely common, we’re going to focus on modeling Bernoulli data (i.e. individual dichotomous variables) in this chapter. However, most of the information presented here directly applies to binomial regression, and binomial regression is simple to carry out using <code>brms</code>.</p>
<p>Logistic regression assumes a Bernoulli distribution for the random component, and the <em>inverse logit</em> function (<span class="math inline">\(\mathrm{logit}^{-1}\)</span>, discussed below) as its link function. The systematic component still consists of linear combinations of our independent variables, i.e., adding them up after multiplying by their respective model coefficients, as it has in previous chapters. A general description of logistic regression with a single quantitative predictor is given in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-4">(10.4)</a>.</p>
<p><span class="math display" id="eq:10-4">\[
\begin{equation}
\begin{split}
y_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \mathrm{logit}^{-1}(z_{[i]}) \\
z_{[i]} = Intercept + \beta \cdot \mathrm{x}_{[i]} \\
\end{split}
\tag{10.4}
\end{equation}
\]</span></p>
<p>The information in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-4">(10.4)</a> says the following: Our data (<span class="math inline">\(y\)</span>) is a Bernoulli-distributed dichotomous variable with a <span class="math inline">\(p\)</span> parameter that varies on each trial. The <span class="math inline">\(p\)</span> parameter is the output of the inverse logit (<span class="math inline">\(\mathrm{logit}^{-1}\)</span>) applied to our linear predictor <span class="math inline">\(z\)</span>. The linear predictor is the sum of an intercept and product of the independent variable (<span class="math inline">\(\mathrm{x}\)</span>) and its slope parameter (<span class="math inline">\(\beta\)</span>). We noted above that <span class="math inline">\(p\)</span> is the expected value of the Bernoulli distribution. Thus, we can see that logistic regression predicts expected values (<span class="math inline">\(p\)</span>) just like our regressions in previous chapters (e.g. <span class="math inline">\(\mu\)</span> for a normal model). However, logistic regression doesn’t model <span class="math inline">\(p\)</span> directly but instead models the <em>logit</em> of the expected value, <span class="math inline">\(z\)</span>.</p>
<div id="logits" class="section level3 hasAnchor" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Logits<a href="logistic-regression-and-signal-detection-theory-models.html#logits" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Logits</strong> are log-odds, the logarithm of the odds of a success. The odds of a success are defined as:</p>
<p><span class="math display" id="eq:10-5">\[
\begin{equation}
\mathrm{odds}_{\mathrm{success}} = \frac{N_{\mathrm{success}}}{N_{\mathrm{failures}}}
\tag{10.5}
\end{equation}
\]</span></p>
<p>Where <span class="math inline">\(N_{\mathrm{success}}\)</span> is the number of observed (or expected) successes and <span class="math inline">\(N_{\mathrm{failures}}\)</span> is the observed (or expected) number of failures. For example, odds of 3/1 indicate that success is three times as likely as a failure. We can also define odds by expressing them in terms of the probability of a success, <span class="math inline">\(p\)</span>, and the probability of a failure, <span class="math inline">\(1-p\)</span>, as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-7">(10.6)</a>.</p>
<p><span class="math display" id="eq:10-7">\[
\begin{equation}
\mathrm{odds} = p / (1-p)
\tag{10.6}
\end{equation}
\]</span></p>
<p>We can turn odds into probabilities with the following calculation. For example, odds of 3/1 imply a probability of 0.25 (3 / (3+1)).</p>
<p><span class="math display" id="eq:10-6">\[
\begin{equation}
\mathrm{P}(\mathrm{success}) = \frac{N_{\mathrm{success}}}{N_{\mathrm{failures}} + N_{\mathrm{success}}} = \frac{N_{\mathrm{success}}}{N_{\mathrm{total}}}
\tag{10.7}
\end{equation}
\]</span></p>
<p>Odds are still bounded by zero on the lower end so they still can’t be modeled using lines, however, if we take the <em>logarithm</em> of the odds we get a <strong>logit</strong>, the log odds. Unlike odds, logits <em>can</em> take on values from positive to negative infinity. This is because, for variable x, logarithms represent the values of x between 0 and 1 with values of log(x) from <span class="math inline">\(-\infty\)</span> to 0. Values of x from 1 to <span class="math inline">\(+\infty\)</span> are represented by values of log(x) from 0 to <span class="math inline">\(+\infty\)</span> (see section <a href="probabilities-likelihood-and-inference.html#c2-logarithms">2.7.2</a>). You can see this relationship by running the code below.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this plot compares x and log (x). Note that log(x)&lt;0 when x&lt;1.</span></span>
<span id="cb378-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb378-2" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> (<span class="fu">log</span>(x), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>),<span class="at">n=</span><span class="dv">1000</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb378-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb378-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb378-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb378-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">v=</span><span class="dv">0</span>, <span class="at">lty=</span><span class="dv">3</span>,<span class="at">col=</span><span class="st">&quot;grey&quot;</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<p>We can calculate the logit of a probability in either of the two following ways:</p>
<p><span class="math display" id="eq:10-8">\[
\begin{equation}
\begin{split}
\mathrm{logit}(p) = \log (p / (1-p)) \\
\mathrm{logit}(p) = \log (p) - \log(1-p)
\end{split}
\tag{10.8}
\end{equation}
\]</span></p>
<p>Equation <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-8">(10.8)</a> is sometimes called the <strong>logit function</strong> because it turns probabilities into logits. The <code>bmmb</code> package contains a function that carries out this transformation (<code>logit</code>). We can see below that this function takes the second approach to calculate logits.</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span>logit</span>
<span id="cb379-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-2" aria-hidden="true" tabindex="-1"></a><span class="do">## function (p) </span></span>
<span id="cb379-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-3" aria-hidden="true" tabindex="-1"></a><span class="do">## {</span></span>
<span id="cb379-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-4" aria-hidden="true" tabindex="-1"></a><span class="do">##     p[p == 1] = 0.99</span></span>
<span id="cb379-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     p[p == 0] = 0.01</span></span>
<span id="cb379-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     log(p) - log(1 - p)</span></span>
<span id="cb379-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-7" aria-hidden="true" tabindex="-1"></a><span class="do">## }</span></span>
<span id="cb379-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-8" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;bytecode: 0x0000014a3b357268&gt;</span></span>
<span id="cb379-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb379-9" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;environment: namespace:bmmb&gt;</span></span></code></pre></div>
<p>Note that when p is equal to 1 or 0, the function arbitrarily changes those values to 0.99 and 0.01. This is because the logarithm of zero is undefined, and so the logit of probabilities of 0 and 1 cannot be calculated. To accommodate these values, the <code>logit</code> function sets extreme (but manageable) values for probabilities of 0 and 1. The effect of this can be seen in the bottom right plot of figure 10.1. Several speakers classified as female 100% or 0% of the time appear along horizontal lines at the bottom and top of the y axis range.</p>
</div>
<div id="c10-inverse-logit" class="section level3 hasAnchor" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> The inverse logit link function<a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Logistic regression models express expected values in <em>logits</em>. The lines represented by these models describe continuous changes in logits as a function of the predictors, and intercepts represent shifts in the values of logits across different situations. However, our data generating distribution (Bernoulli) requires a parameter <span class="math inline">\(p\)</span> that is bounded by 0 and 1. This means that the link function for logistic regression needs to convert logits to probabilities. The function that does this is called the <strong>logistic function</strong>, the <strong>antilogit function</strong>, or the <strong>inverse logit function</strong>. We will refer to it as the inverse logit function to highlight the fact that it is the inverse of the logit, that it <em>undoes</em> the logit transform.</p>
<p>The inverse logit function is presented in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-9">(10.9)</a>, for logit (i.e. log odds) values, <span class="math inline">\(z\)</span>, ranging continuously from positive to negative infinity. The symbol <span class="math inline">\(e\)</span> represents the mathematical constant (Euler’s number, 2.718…) that serves as the base for natural logarithms and for the exponential function so that <span class="math inline">\(\exp(z)=e^z\)</span>.</p>
<p><span class="math display" id="eq:10-9">\[
\begin{equation}
P(y=1)=\mathrm{logit}^{-1}(z) = \frac{e^{z}}{1 + e^{z}}
\tag{10.9}
\end{equation}
\]</span></p>
<p>The inverse logit function may seem inscrutable, but it is in fact very scrutable. First, note its structural similarity to equation <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-6">(10.7)</a> which describes the conversion of counts of successes and failures to probabilities. Recall that <span class="math inline">\(z\)</span> is the <em>log-odds</em> of observing a ‘success’. This means that <span class="math inline">\(e^z\)</span> is the <em>odds</em> of observing a success (since exponentiation ‘undoes’ log transforms). The odds are the ratio of the number of successes to the number of failures observed (or expected) for some number of trials. As a result, <span class="math inline">\(e^z\)</span> can be <em>thought of</em> as the expected number of successes relative to some number of failures. How many failures?</p>
<p>In equation <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-6">(10.7)</a> we see that the left term in the denominator represents the number of failures. Based on this, it seems as though in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-9">(10.9)</a> we’re fixing the expected ‘count’ of failures at 1, and this is effectively what the inverse logit function does. This is necessary because for <span class="math inline">\(n\)</span> trials if you have <span class="math inline">\(S\)</span> successes then you <em>must</em> have <span class="math inline">\(n-S\)</span> failures. This means that the odds (or probabilities) of successes <em>and</em> failures cannot both be independently estimated. We ran into similar constraints when we discussed contrasts in chapter 5, and our discussion of why all levels of a factor cannot be independently estimated. To resolve this issue, logistic regression only estimates the odds of successes and not of failures. However, we still need a number of failures in order to turn odds into probabilities. To resolve this issue, the inverse logit function fixes the number of failures to 1 for all cases.</p>
<p>Thinking of <span class="math inline">\(e^z\)</span> as the expected number of successes relative to 1 expected failure may help interpret logits and understand the inverse logit function. For example, since <span class="math inline">\(e^0=1\)</span> a logit of 0 means we expect 1 success compared to 1 failure. This results in <span class="math inline">\(p=0.5\)</span> for a logit of 0, as shown in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-10">(10.10)</a>.</p>
<p><span class="math display" id="eq:10-10">\[
\begin{equation}
\frac{e^{0}}{1 + e^{0}}=\frac{1}{1 + 1}=0.5
\tag{10.10}
\end{equation}
\]</span></p>
<p>When <span class="math inline">\(z\)</span> is a negative value, <span class="math inline">\(e^{z}\)</span> is bounded by 0 and 1. This means that a success is less likely than a failure in these cases since we’ve fixed the number of failures at 1. For example if <span class="math inline">\(z=-3\)</span> then <span class="math inline">\(e^{z}=e^{-3}=0.05\)</span>. This means that when <span class="math inline">\(z&lt;0\)</span>, we expect probabilities between 0.0 and 0.5, as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-11">(10.11)</a>.</p>
<p><span class="math display" id="eq:10-11">\[
\begin{equation}
\frac{e^{-3}}{1 + e^{-3}}=\frac{0.05}{1 + 0.05}=0.0474
\tag{10.11}
\end{equation}
\]</span></p>
<p>On the other hand, when <span class="math inline">\(z\)</span> is positive, <span class="math inline">\(e^{z}\)</span> will be a number greater than 1, meaning success is more likely than failure. Thus, for positive values of <span class="math inline">\(z\)</span> we expect probabilities between 0.5 and 1 as in:</p>
<p><span class="math display" id="eq:10-12">\[
\begin{equation}
\frac{e^3}{1 + e^{3}}=\frac{20.1}{1 + 20.1}=0.953
\tag{10.12}
\end{equation}
\]</span></p>
<p>Notice also that the inverse logit function can’t actually generate values of 1 or 0. This is because no matter how large the value of <span class="math inline">\(z\)</span> gets, the denominator will always be 1 greater. This sets the upper range of the inverse logit at 1. On the other hand, no matter how negative the value of <span class="math inline">\(z\)</span> gets, exponentiating the value will <em>always</em> result in a number greater than 0. This is because the exponential function can only generate positive, non-zero values. As a result, <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-12">(10.12)</a> will also always provide values greater than 0.</p>
<p>In Figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-2">10.2</a> we draw a line with a slope of one and an intercept of 0 (i.e., y=x). We can imagine that this line defines expected values in <em>logits</em> as a function of some predictor x. In the middle plot, we’ve applied the inverse logit function to the line so that <span class="math inline">\(\mathrm{logit}^{-1}(y) = x\)</span>. This results in what’s known as a <strong>sigmoid curve</strong> relating our x and y variables. If we apply the logit function to this sigmoid curve, the result is a return of our original line again. Thus, we can see that the logit and inverse logit transformations can be applied over and over to go back and forth between a probability or logit representation of the same information.</p>
<div class="figure"><span style="display:block;" id="fig:F10-2"></span>
<img src="_main_files/figure-html/F10-2-1.jpeg" alt="(left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y-axis as logits. (middle) The result of applying the inverse logit function to every point of the line in the left plot. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line." width="4800" />
<p class="caption">
Figure 10.2: (left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y-axis as logits. (middle) The result of applying the inverse logit function to every point of the line in the left plot. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line.
</p>
</div>
<p>In chapter 9 we mentioned that when you do a linear regression you effectively model the data as being generated by distributions that slide along lines, ‘producing’ data along the way (see figure <a href="quantitative-predictors-and-their-interactions-with-factors.html#fig:F9-1">9.1</a>). Logistic regression leads to an analogous but slightly different interpretation. In logistic regression you model the data as a Bernoulli distribution sliding along a sigmoid curve (middle plot, figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-2">10.2</a>), generating a ratio of successes to failures based on the value of <span class="math inline">\(p\)</span> specified by the curve.</p>
<p>The second and third lines in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-4">(10.4)</a> represent the model link function and systematic component, respectively. We can skip the middleman (<span class="math inline">\(z\)</span>) and just put the prediction equation directly inside the inverse logit function as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-13">(10.13)</a>.</p>
<p><span class="math display" id="eq:10-13">\[
\begin{equation}
p = \mathrm{logit}^{-1}(\mathrm{Intercept} + \beta \cdot x_{[i]}) =  \frac{e^{(\mathrm{Intercept} + \beta \cdot x_{[i]})}}{1 + e^{(\mathrm{Intercept} + \beta \cdot x_{[i]})}}
\tag{10.13}
\end{equation}
\]</span></p>
<p>We can then take the inverse logit function and put it directly inside the Bernoulli distribution, as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-14">(10.14)</a>.</p>
<p><span class="math display" id="eq:10-14">\[
\begin{equation}
F \sim \mathrm{Bernoulli} \left (\frac{e^{(\mathrm{Intercept} + \beta \cdot x_{[i]})}}{1 + e^{(\mathrm{Intercept} + \beta \cdot x_{[i]})}} \right )
\tag{10.14}
\end{equation}
\]</span></p>
<p>There’s no particular reason to do this, apart from the fact that it illustrates that our link function really does <em>link</em> our prediction equation (<span class="math inline">\(Intercept + VTL \cdot \mathrm{vtl}\)</span>) and our data distribution (Bernoulli).</p>
</div>
<div id="building-intuitions-about-logits-and-the-inverse-logit-function" class="section level3 hasAnchor" number="10.4.3">
<h3><span class="header-section-number">10.4.3</span> Building intuitions about logits and the inverse logit function<a href="logistic-regression-and-signal-detection-theory-models.html#building-intuitions-about-logits-and-the-inverse-logit-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The logit and inverse logit functions are <strong>non linear</strong> functions. In the simplest sense this means that the relationships they form between x and y variables, when plotted, do not form straight lines. In our case, this means that when we compare logits and their associated probabilities we will not see a straight line. The sigmoid curve in the left plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-3">10.3</a> reveals a changing relationship between logits and probabilities: The slope is very shallow for large negative logit values but increases in value as the curve approaches 0. The slope then begins to decrease in value as the logits become large again. This behavior allows the sigmoid curve in the figure to extend from positive infinity to negative infinity without ever crossing 1 and 0: As logit values become arbitrarily large in magnitude, the slope of the sigmoid curve becomes arbitrarily small in magnitude to the point where it is effectively parallel to the x axis on both ends of the curve.</p>
<div class="figure"><span style="display:block;" id="fig:F10-3"></span>
<img src="_main_files/figure-html/F10-3-1.jpeg" alt="(left) A sigmoid curve expressing the probability associated with each logit value along the x axis. Horizontal lines are placed every 0.1 from 0.1 to 0.9 probability. (right) A line relating some predictor to logits. Horizontal lines are placed every 0.1 from 0.1 to 0.9 probability." width="4800" />
<p class="caption">
Figure 10.3: (left) A sigmoid curve expressing the probability associated with each logit value along the x axis. Horizontal lines are placed every 0.1 from 0.1 to 0.9 probability. (right) A line relating some predictor to logits. Horizontal lines are placed every 0.1 from 0.1 to 0.9 probability.
</p>
</div>
<p>In the right plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-3">10.3</a> we see the relationship between the logit of the probability and our predictor. We can see the consequences of the changing relationship between logits and probabilities by looking at the horizontal lines in the plot. These lines represent equal differences in probability (0.1 change per line). The lines are closer together when logits have a small magnitude and further when logits are large. This is a consequence of the non-linearity of the transforms: A one unit change in probabilities/logits does not always map to the same size of change in the other value. In general, small changes when logits are close to zero (i.e., when <span class="math inline">\(p\)</span> is close to 0.5) map to large changes in probabilities, while larger changes in logits with large magnitudes (i.e., when <span class="math inline">\(p\)</span> is close to 0 or 1) tend to map to small changes in probabilities. As noted above, this is reflected in the changing slope of the sigmoid curve in the left plot.</p>
<p>The top row below contains a sequence of probabilities and the bottom row shows equivalent logits. Notice that the difference between 0.5 and 0.6 is 0.41 logits, but the difference between 0.8 and 0.9 is 0.81 logits. Meanwhile the difference between 0.9 and 1 is infinity.</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span> ( (<span class="fu">seq</span> (<span class="fl">0.1</span>,.<span class="dv">9</span>,.<span class="dv">1</span>)), </span>
<span id="cb380-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb380-2" aria-hidden="true" tabindex="-1"></a>        <span class="fu">round</span> ( <span class="fu">logit</span> (<span class="fu">seq</span> (<span class="fl">0.1</span>,<span class="fl">0.9</span>,.<span class="dv">1</span>)) , <span class="dv">2</span>) )</span>
<span id="cb380-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb380-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1]  [,2]  [,3]  [,4] [,5] [,6] [,7] [,8] [,9]</span></span>
<span id="cb380-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb380-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]  0.1  0.20  0.30  0.40  0.5 0.60 0.70 0.80  0.9</span></span>
<span id="cb380-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb380-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] -2.2 -1.39 -0.85 -0.41  0.0 0.41 0.85 1.39  2.2</span></span></code></pre></div>
<p>We can think about the cause of this behavior by thinking about some situation where you’re trying to change a probability. Imagine that you keep track of your free throws in basketball practice. You sink 500/1000 free throws, giving you a 0.5 probability of success. Now imagine you take a further 100 shots and sink them all. Now your probability of success is 600/1100, meaning your amazing streak has increased your probability to 0.54. However, suppose that you had been a 900/1000 shooter, a probability of 0.9 of success. If you had the same streak of 100 made baskets you would only increase your probability to 0.901 (1000/1100). This change is <em>forty</em> times smaller than the change when you begin at 0.5 (0.001/0.040). Thus, we can see that ‘the same’ increase in made baskets results in a large increase in one probability (0.5 to 0.54, almost 10%) and a minuscule change in another (0.9 to 0.901, about 0.1%). Basically, as you approach 0 and 1 it gets harder to make large changes in your probabilities, and this is reflected in the logits of those probabilities.</p>
<p>Here are some useful things to keep in mind when interpreting logits, in no particular order:</p>
<ul>
<li><p>A probability of 0.5 is 0 logits. Positive logits means more likely to be a success, negative logits means more likely to be a failure.</p></li>
<li><p>-3 and 3 are 4.7% and 95.2%. Basically -3 and 3 logits are useful bounds for “very likely 1” and “very likely 0”.</p></li>
<li><p>Since a logit of 3 translates to a <span class="math inline">\(p\)</span> of about 0.95, all of the space between +3 and infinity logits represents the probability space between 0.95 and 1, while logits between 0 and 3 represent the space from 0.5 to 0.95.</p></li>
<li><p>Logits far beyond 3 might not have much practical significance. A logit of 4 is a probability of 0.982 and a logit of 6 is 0.997. For many purposes, probabilities of 0.95, 0.98, and 0.99 are nearly interchangeable. Also, it is very difficult to distinguish 95%, 98%, and 99% in practice since you will be observing very few cases of 0.</p></li>
<li><p>Effects can be considered important or not based on how far they get you along -3 to 3 (or -4 to 4). Basically, anything in the +1 range is very likely to matter, while effects smaller than 0.1 or so are likely having only a small influence on outcomes.</p></li>
</ul>
<p>Here is one final thing to keep in mind about logits: You <strong>must</strong> combine your model parameters as logits <strong>before</strong> transforming them into probabilities. This is due to the fact that the inverse logit function is non-linear, and will be discussed in more detail in section <a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1">10.5.4</a>.</p>
</div>
</div>
<div id="logistic-regression-with-one-quantitative-predictor" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Logistic regression with one quantitative predictor<a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-with-one-quantitative-predictor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter we tried to predict apparent height using speaker VTL, apparent age, and apparent gender. Here, we’re going to predict the perception of femaleness, i.e., the identification of the speaker as female, based on speaker VTL an apparent age and using a logistic regression.</p>
<div id="data-and-research-questions-5" class="section level3 hasAnchor" number="10.5.1">
<h3><span class="header-section-number">10.5.1</span> Data and research questions<a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below we load the necessary R packages and the data. We create a new variable called <code>Female</code> which will be our dependent variable. This variable equals 1 when listeners indicated hearing a female speaker and 0 when listeners indicated a male speaker. We’ll predict this using a single quantitative predictor, speaker VTL, which we also center below.</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb381-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb381-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-3" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&#39;contr.sum&#39;</span>,<span class="st">&#39;contr.sum&#39;</span>))</span>
<span id="cb381-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb381-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-7" aria-hidden="true" tabindex="-1"></a><span class="co"># our dependent variable</span></span>
<span id="cb381-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-8" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>Female <span class="ot">=</span> <span class="fu">as.numeric</span> (exp_data<span class="sc">$</span>G <span class="sc">==</span> <span class="st">&#39;f&#39;</span>)</span>
<span id="cb381-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-10" aria-hidden="true" tabindex="-1"></a><span class="co"># make a copy of vtl</span></span>
<span id="cb381-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-11" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl_original <span class="ot">=</span> exp_data<span class="sc">$</span>vtl</span>
<span id="cb381-12"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-12" aria-hidden="true" tabindex="-1"></a><span class="co"># center vtl</span></span>
<span id="cb381-13"><a href="logistic-regression-and-signal-detection-theory-models.html#cb381-13" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl <span class="ot">=</span> exp_data<span class="sc">$</span>vtl <span class="sc">-</span> <span class="fu">mean</span> (exp_data<span class="sc">$</span>vtl)</span></code></pre></div>
<p>The variables from our data frame that will be used in this model are:</p>
<ul>
<li><code>L</code>: A number from 1-15 indicating which <em>listener</em> responded to the trial.</li>
<li><code>S</code>: A number from 1-139 indicating which <em>speaker</em> produced the trial stimulus.</li>
<li><code>vtl</code>: An estimate of the speaker’s <em>vocal-tract length</em> in centimeters.</li>
<li><code>G</code>: The <em>apparent gender</em> of the speaker indicated by the listener, <code>f</code> (female) or <code>m</code> (male).</li>
<li><code>A</code>: The <em>apparent age</em> of the speaker indicated by the listener, <code>a</code> (adult) or <code>c</code> (child).</li>
</ul>
<p>We saw in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-1">10.1</a> that VTL is negatively related to the probability that a speaker will be identified as female. In addition, it seems that the relationship between speaker VTL and the perception of female speakers may differ based on the apparent age of the speaker. We would like to know:</p>
<p>(Q1) What is the relationship between speaker VTL and the perception of femaleness?</p>
<p>(Q2) Does the relationship between VTL and apparent speaker gender vary in an age-dependent manner?</p>
</div>
<div id="description-of-the-model-8" class="section level3 hasAnchor" number="10.5.2">
<h3><span class="header-section-number">10.5.2</span> Description of the model<a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to answer the questions posed above, our model needs to represent the linear relationship between VTL and the logit of the probability of observing a female classification. It also needs to allow this linear relationship to vary between apparent children and adults. The formula for our model needs to look like this:</p>
<p><code>Female ~ vtl*A + (vtl*A|L) + (1|S)</code></p>
<p>The formula above tells our model to predict perceived femaleness using speaker VTL (centered as in the previous chapter), information about whether the speaker was identified as a child or an adult, and age-dependent use of VTL. Since we include the interaction between apparent age and speaker VTL, we are effectively estimating two lines relating speaker VTL and apparent gender: One for apparent adults and another for apparent children. Our formula also includes by-listener effects for all predictors and by-speaker intercepts. This model would be relatively ‘simple’ at this point if it were dealing with normally distributed data. The prior specification we’re going to use looks like this:</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb382-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb382-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb382-2" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb382-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb382-3" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb382-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb382-4" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>))</span></code></pre></div>
<p>You’ll notice that we’re using priors with a standard deviation of 3 across the board. This is because logit values of 3, 6, and 9 correspond to probabilities of 0.9526, 0.9975, and 0.9999 (<code>inverse_logit(c(3,6,9))</code>) respectively. So, a prior with a standard deviation of 3 suggests we expect group differences, or a one-unit change in VTL, have the potential to change probabilities from 50% to 95% (a difference of 3 logits). We think it’s plausible that apparent age may, for example, change an expected probability from 50% to 95%. In terms of continuous predictors like VTL, the important thing to keep in mind is that the slope estimated for a predictor depends on the unit of measurement for that predictor. For example, below we see the average VTL in centimeters for each speaker category.</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb383-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span> (exp_data<span class="sc">$</span>vtl_original, exp_data<span class="sc">$</span>C_v, mean)</span>
<span id="cb383-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb383-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     b     g     m     w </span></span>
<span id="cb383-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb383-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 12.32 11.78 15.08 13.00</span></span></code></pre></div>
<p>We can see that the average difference between men and women is about 2 cm. If we assume that people at the averages are classified correctly more often than not, this means that the logit of the probability of observing a female response for an adult with a 15 cm VTL may be about -3, and the logit of the same for a 13 cm VTL may be about 3. Thus, we might expect a slope of about 3 logits per 1 cm change in VTL with respect to the perception of female speakers. However, what if we had measured VTL in meters? Then the difference in VTL would be only 0.02, meaning that the model slope would now have to be 300 in order to change from -3 to 3 in only 0.02 units of change (<span class="math inline">\(300 = 6/0.02\)</span>). Based on this we can see that it’s important to think about the amount of variation there is in your quantitative predictors and how this might relate to the probabilities you are modeling when setting your priors.</p>
<p>Our full model specification, omitting the deterministic construction of the covariance matrix <span class="math inline">\(\mathrm{\Sigma}\)</span> (as usual), is:</p>
<p><span class="math display" id="eq:10-13">\[
\begin{equation}
\begin{split}
\mathrm{Female}_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \mathrm{logit}^{-1} (z_{[i]}) \\
z_{[i]} = a_{[i]} + b_{[i]} \cdot \mathrm{vtl}_{[i]}  \\
a_{[i]} = \mathrm{Intercept} + A + A \colon L_{[\mathsf{L}_{[i]}]} + L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\
b_{[i]} =  VTL + VTL \colon A + VTL \colon L_{[\mathsf{L}_{[i]}]} + VTL \colon A \colon L_{[\mathsf{L}_{[i]}]}  \\ \\
\textrm{Priors:} \\
S_{[\bullet]} \sim \mathrm{Normal}(0,\sigma_{S}) \\
\begin{bmatrix} L_{[\bullet]} \\ A \colon L_{[\bullet]} \\ VTL \colon L_{[\bullet]} \\ VTL \colon A \colon L_{[\bullet]} \\ \end{bmatrix}   
\sim \mathrm{MVNormal} \left(\, \begin{bmatrix} 0\\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \mathrm{\Sigma} \right) \\ \\
\mathrm{Intercept} \sim \mathrm{t}(3, 0, 3) \\
A, VTL, VTL \colon A \sim \mathrm{t}(3, 0, 3) \\
\sigma_{S}, \sigma_{L}, \sigma_{A \colon L}, \sigma_{VTL \colon L} , \sigma_{VTL  \colon A \colon L}  \sim \mathrm{t}(3, 0, 3) \\ R \sim \mathrm{LKJCorr} (2)
\end{split}
\tag{10.13}
\end{equation}
\]</span></p>
<p>The main differences compared to our previous models are the lack of terms related to <span class="math inline">\(\sigma\)</span>, the inclusion of a link function, and the reliance on a Bernoulli rather than the normal or t distributions at the data level. In plain English, this model could be read like:</p>
<blockquote>
<p>We’re treating our femaleness judgments (1 or 0 for female or male) as coming from a Bernoulli distribution with a probability (<span class="math inline">\(p\)</span>) that varies from trial to trial. The <em>logit of the probability</em> (<span class="math inline">\(z\)</span>) varies along lines. The lines are specified by intercepts (<span class="math inline">\(a\)</span>) and slopes (<span class="math inline">\(b\)</span>) that vary from trial to trial, and there is a single continuous predictor (speaker VTL). The intercept of these lines varies based on an overall intercept, an effect for apparent age (<span class="math inline">\(A\)</span>), listener-specific effects for apparent age (<span class="math inline">\(A \colon L\)</span>), listener-specific deviations (<span class="math inline">\(L\)</span>), and speaker-specific deviations (S). The slope of these lines varies based on an overall slope (<span class="math inline">\(VTL\)</span>, the main effect), deviations based on apparent age (<span class="math inline">\(VTL \colon A\)</span>), listener-specific deviations (<span class="math inline">\(VTL \colon L\)</span>), and listener-specific interactions between apparent age and VTL (<span class="math inline">\(A \colon VTL \colon L\)</span>)). The speaker intercept (<span class="math inline">\(S\)</span>) terms were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The listener random effects were drawn from a multivariate normal distribution with means of zero and a covariance matrix estimated from the data. All other effects (e.g., the Intercept, VTL, A, etc.) were treated as ‘fixed’ and drawn from prior distributions appropriate for their expected range of values.</p>
</blockquote>
</div>
<div id="c10-fitting-0" class="section level3 hasAnchor" number="10.5.3">
<h3><span class="header-section-number">10.5.3</span> Fitting the model<a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-0" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’re going to fit the model outlined above. Below is the function call we need to run the model described in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-13">(10.13)</a>:</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb384-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-2" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">=</span> <span class="fu">c</span>(brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb384-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-3" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb384-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-4" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb384-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-5" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>))</span>
<span id="cb384-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb384-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-7" aria-hidden="true" tabindex="-1"></a>model_gender_vtl <span class="ot">=</span></span>
<span id="cb384-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span> (Female <span class="sc">~</span> vtl<span class="sc">*</span>A <span class="sc">+</span> (vtl<span class="sc">*</span>A<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data=</span>exp_data, <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, </span>
<span id="cb384-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb384-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">family=</span><span class="st">&quot;bernoulli&quot;</span>, <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter=</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>,<span class="at">prior=</span>priors)</span></code></pre></div>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb385-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb385-2" aria-hidden="true" tabindex="-1"></a>model_gender_vtl <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;10_model_gender_vtl.RDS&#39;</span>)</span></code></pre></div>
<p>However, we’re going to (retroactively) pause before continuing in order to update our discussion of prior predictive checks. In section <a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-prior-prediction">8.3.1</a> we discussed these in the context of quantitative variables. In these cases, our predictions take on a range of values and can be summarized effectively using histograms and plots showing the density of the distribution. In contrast, our dichotomous variables only take on values of 1 and 0, and so histograms and densities of these no longer provide as much useful information.
Instead of considering the distribution of values of the dependent variable directly, we can consider prior predictions as logits, allowing us to use densities or histograms just as we did for quantitative variables.</p>
<p>We can look at the location of the logit predictions to see how expected outcomes vary across conditions or overall. A concentration around 0 means that predictions are balanced across successes and failures, whereas positive or negative predictions indicate one or the other category is more likely. We can also look at the variation of predictions about their average, and in particular, in the magnitude of the logits being predicted. Predicted logits with very large magnitudes indicate near certainty in the outcome, whereas predictions concentrated near zero indicate that either category is equally likely. We can use the code below to run our prior predictive check and then use the <code>p_check</code> function to get and plot our prior predictions (not shown here).</p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-1" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">=</span> <span class="fu">c</span>(brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb386-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-2" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb386-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-3" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb386-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-4" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>))</span>
<span id="cb386-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb386-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-6" aria-hidden="true" tabindex="-1"></a>model_gender_vtl_priors <span class="ot">=</span></span>
<span id="cb386-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span> (Female <span class="sc">~</span> vtl<span class="sc">*</span>A <span class="sc">+</span> (vtl<span class="sc">*</span>A<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data=</span>exp_data, <span class="at">chains=</span><span class="dv">4</span>, </span>
<span id="cb386-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">cores=</span><span class="dv">4</span>, <span class="at">family=</span><span class="st">&quot;bernoulli&quot;</span>, <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>,</span>
<span id="cb386-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">prior =</span> priors, <span class="at">sample_prior =</span> <span class="st">&quot;only&quot;</span>)</span>
<span id="cb386-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb386-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb386-11" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">p_check</span> (model_gender_vtl_priors)</span></code></pre></div>
<p>Our prior predictions indicate some logits with very high values (&gt;40), with the bulk of variation seemingly between -20 and 20 logits. This indicates that in some cases, speakers would be identified as female with a probability of 0.000000002 (since <span class="math inline">\(\mathrm{logit}^{-1}(-20)=0.000000002\)</span>), or about once every 500 million trials. This might seem crazy but it might not be. For example, consider someone with a very deep voice and a deep resonance like James Earl Jones (a.k.a Mufasa in the <em>Lion King</em> and Darth Vader in <em>Star Wars</em>). You ask: What’s the probability that this voice would be identified as an adult female by a random listener, knowing nothing about the speaker other than a single example of their voice? In some cases, you might reasonably think that this might <em>never</em> happen, other than perhaps due to accidental responses. But what if you <em>had</em> to put a number on never? Well in that case logits as large as 20, or a one in half billion chance, may not seem so crazy.</p>
<p>Although we’ve spent the last paragraph defending these priors, they probably are too wide, just not excessively so. If our model has problems converging or has any other pathologies (e.g., divergent transitions, see section <a href="fitting-bayesian-regression-models-with-brms.html#c3-checking-convergence">3.6</a>), we may want to reconsider these priors.</p>
</div>
<div id="c10-fitting-1" class="section level3 hasAnchor" number="10.5.4">
<h3><span class="header-section-number">10.5.4</span> Interpreting the model<a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we inspect the model summary:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-1" aria-hidden="true" tabindex="-1"></a><span class="fu">short_summary</span>(model_gender_vtl)</span>
<span id="cb387-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula:  Female ~ vtl * A + (vtl * A | L) + (1 | S)</span></span>
<span id="cb387-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb387-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Group-Level Effects:</span></span>
<span id="cb387-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-5" aria-hidden="true" tabindex="-1"></a><span class="do">## ~L (Number of levels: 15)</span></span>
<span id="cb387-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                       Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb387-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-7" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)             0.55      0.27     0.08     1.14</span></span>
<span id="cb387-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-8" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(vtl)                   1.04      0.31     0.53     1.78</span></span>
<span id="cb387-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-9" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(A1)                    0.95      0.31     0.44     1.61</span></span>
<span id="cb387-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-10" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(vtl:A1)                0.43      0.24     0.04     0.96</span></span>
<span id="cb387-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-11" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,vtl)       -0.06      0.32    -0.66     0.57</span></span>
<span id="cb387-12"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-12" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,A1)        -0.22      0.32    -0.78     0.44</span></span>
<span id="cb387-13"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-13" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(vtl,A1)              -0.46      0.24    -0.82     0.09</span></span>
<span id="cb387-14"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-14" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,vtl:A1)    -0.26      0.37    -0.83     0.55</span></span>
<span id="cb387-15"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-15" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(vtl,vtl:A1)           0.24      0.34    -0.48     0.78</span></span>
<span id="cb387-16"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-16" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(A1,vtl:A1)           -0.26      0.33    -0.81     0.48</span></span>
<span id="cb387-17"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb387-18"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ~S (Number of levels: 139)</span></span>
<span id="cb387-19"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-19" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb387-20"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-20" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)      1.2      0.16     0.91     1.55</span></span>
<span id="cb387-21"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-21" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb387-22"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects:</span></span>
<span id="cb387-23"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-23" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb387-24"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept     0.86      0.31     0.27     1.49</span></span>
<span id="cb387-25"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-25" aria-hidden="true" tabindex="-1"></a><span class="do">## vtl          -3.59      0.39    -4.41    -2.88</span></span>
<span id="cb387-26"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-26" aria-hidden="true" tabindex="-1"></a><span class="do">## A1            2.65      0.35     2.01     3.39</span></span>
<span id="cb387-27"><a href="logistic-regression-and-signal-detection-theory-models.html#cb387-27" aria-hidden="true" tabindex="-1"></a><span class="do">## vtl:A1       -1.76      0.28    -2.34    -1.24</span></span></code></pre></div>
<p>We see that it looks just like all our previous models except for two main differences:</p>
<ol style="list-style-type: decimal">
<li><p>All our parameters, including means, errors, and credible intervals, are now expressed in logits.</p></li>
<li><p>The absence of the <code>Family-Specific</code> parameter section of the model where <code>sigma</code> and <code>nu</code> (i.e., <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\nu\)</span>) were usually found.</p></li>
</ol>
<p>In chapter 9 we talked about the geometry of models that include a single quantitative predictor, a categorical predictor with several levels, and the interaction of the two. Essentially, these models result in a set of lines, one overall ‘main effects’ line, and another line for each level of the categorical predictor interacting with the quantitative predictor. Since our model includes effects for apparent age (<code>A1</code>), VTL (<code>vtl</code>), and their interaction (<code>vtl:A1</code>), our model can be thought of as three lines relating VTL to the logit of the probability of observing a female response: The overall (main effects) line, the line for apparent adult speakers, and the line for apparent child speakers. We can recover the parameters for these three lines by adding the appropriate model coefficients together using the hypothesis function. Since <code>A1</code> (and related parameters) represent the adult group and we are using sum coding, the effect for children is represented by subtracting, rather than adding, the relevant parameters (i.e. <code>Intercept - A1</code> to find the child intercept).</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-1" aria-hidden="true" tabindex="-1"></a>gender_vtl_hypothesis <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb388-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-2" aria-hidden="true" tabindex="-1"></a>  model_gender_vtl,</span>
<span id="cb388-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;Intercept = 0&quot;</span>,           <span class="co"># overall intercept</span></span>
<span id="cb388-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-4" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;Intercept + A1 = 0&quot;</span>,      <span class="co"># adult intercept</span></span>
<span id="cb388-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-5" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;Intercept - A1 = 0&quot;</span>,      <span class="co"># child intercept</span></span>
<span id="cb388-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-6" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;vtl = 0&quot;</span>,                 <span class="co"># overall slope</span></span>
<span id="cb388-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-7" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;vtl + vtl:A1 = 0&quot;</span>,        <span class="co"># adult slope</span></span>
<span id="cb388-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-8" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;vtl - vtl:A1 = 0&quot;</span>) )      <span class="co"># child slope</span></span>
<span id="cb388-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb388-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-10" aria-hidden="true" tabindex="-1"></a>gender_vtl_hypothesis</span>
<span id="cb388-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-11" aria-hidden="true" tabindex="-1"></a><span class="do">##    Estimate Est.Error    Q2.5   Q97.5         hypothesis</span></span>
<span id="cb388-12"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-12" aria-hidden="true" tabindex="-1"></a><span class="do">## H1   0.8563    0.3084  0.2672  1.4934    (Intercept) = 0</span></span>
<span id="cb388-13"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-13" aria-hidden="true" tabindex="-1"></a><span class="do">## H2   3.5035    0.5022  2.5716  4.5727 (Intercept+A1) = 0</span></span>
<span id="cb388-14"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-14" aria-hidden="true" tabindex="-1"></a><span class="do">## H3  -1.7909    0.4337 -2.6816 -0.9702 (Intercept-A1) = 0</span></span>
<span id="cb388-15"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-15" aria-hidden="true" tabindex="-1"></a><span class="do">## H4  -3.5911    0.3868 -4.4145 -2.8822          (vtl) = 0</span></span>
<span id="cb388-16"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-16" aria-hidden="true" tabindex="-1"></a><span class="do">## H5  -5.3493    0.5664 -6.5654 -4.3457   (vtl+vtl:A1) = 0</span></span>
<span id="cb388-17"><a href="logistic-regression-and-signal-detection-theory-models.html#cb388-17" aria-hidden="true" tabindex="-1"></a><span class="do">## H6  -1.8328    0.3681 -2.5856 -1.1353   (vtl-vtl:A1) = 0</span></span></code></pre></div>
<p>These parameters can be used to plot lines predicting the logit of the probability of a female response given speaker VTL. The left plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a> presents the overall ‘main effects’ line and the age-dependent lines, compared to our data. We can see that our age-dependent lines follow the data much better than the single average line does. In the right plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a> we see what happens when we apply the inverse logit transform on the lines (and points) in the left plot of the figure. The result is a set of sigmoid curves representing expected variation in the <span class="math inline">\(p\)</span> parameter of a Bernoulli distribution as a function of speaker VTL. These curves are a better fit for our probabilities than the lines we originally used in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-1">10.1</a>, and also do not ever result in values below zero or above one.</p>
<div class="figure"><span style="display:block;" id="fig:F10-4"></span>
<img src="_main_files/figure-html/F10-4-1.jpeg" alt="(left) Lines indicating the linear relationship between VTL and the logit of the probability of a female response. Point colors indicate modal speaker category classification. (right) Same as the left plot but indicating probabilities on the y axis." width="4800" />
<p class="caption">
Figure 10.4: (left) Lines indicating the linear relationship between VTL and the logit of the probability of a female response. Point colors indicate modal speaker category classification. (right) Same as the left plot but indicating probabilities on the y axis.
</p>
</div>
<p>Let’s discuss the values of our fixed effects parameters. The most important thing to remember when interpreting the coefficients of a logistic model is that positive coefficients push us towards a 1 response (‘female’), while negative values push us towards a 0 response (‘male’). A predicted value of exactly 0 means the outcome is 50/50. The model intercept is the value of the line when VTL=0. Since we centered our VTL predictor, our positive intercept suggests a speaker with an average VTL (13.4 cm) was more likely to be classified as female (with a probability of 0.67 (<code>inverse_logit(0.71)</code>). The effect for perceived adultness (<code>A1</code>) is positive indicating that a speaker with an average VTL is more likely to be identified as a woman when the speaker is also thought to be an adult. The negative effect for VTL tells us that as VTL increases, we are <em>less</em> likely to observe a ‘female’ response and <em>more</em> likely to observe a ‘male’ response. The interaction between VTL and age is negative, meaning this slope is even more negative when the speaker is thought to be an adult. This is evident in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a> where the line for apparent adults has a much steeper slope than the line for apparent children. In addition, the magnitude of the slope increases/decreases by more than 50% since the VTL parameter is -3.6 and the interaction is -1.76.</p>
<p>We can interpret our model entirely in the logit space, focusing on the geometry of our models and interpreting our intercept and slope terms just as we did above and in the previous chapter. However, if we want to think of our model parameters in terms of probabilities (rather than logits), we need to combine parameters as logits first, and then carry out the inverse logit function on these values. It is absolutely essential that the operations be done in this order because the inverse logit function is not an <strong>additive</strong> function. Earlier we noted that the logit (and inverse logit) are non linear. One of the characteristics of non-linear transformations is that they are generally not additive. A function, <span class="math inline">\(f(x)\)</span>, is additive if the following property holds:</p>
<p><span class="math display" id="eq:10-14">\[
\begin{equation}
f(x+y) = f(x) + f(y)
\tag{10.14}
\end{equation}
\]</span></p>
<p>A function is additive if it preserves the operation of addition. This means that adding two things and putting them into the function provides the same results as passing them individually through the function and adding them up after. For example imagine our function is <span class="math inline">\(f(x)=x \cdot 2\)</span>. We put in <span class="math inline">\(3+2\)</span> (i.e. <span class="math inline">\(5\)</span>) and get <span class="math inline">\(10\)</span> out. We then put in <span class="math inline">\(3\)</span> and <span class="math inline">\(2\)</span> individually, get <span class="math inline">\(6\)</span> and <span class="math inline">\(4\)</span> out, add them up and get <span class="math inline">\(10\)</span> again. Based on this we say that our function appears to be <em>additive</em>. As noted above, the inverse logit function is not. For example, let’s consider the meaning of the <code>Intercept</code> and <code>A1</code> coefficients:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model intercept</span></span>
<span id="cb389-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-2" aria-hidden="true" tabindex="-1"></a><span class="fu">inverse_logit</span> (<span class="fl">0.86</span>)</span>
<span id="cb389-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.7027</span></span>
<span id="cb389-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb389-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model A1, adultness, term</span></span>
<span id="cb389-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-6" aria-hidden="true" tabindex="-1"></a><span class="fu">inverse_logit</span> (<span class="fl">2.65</span>)</span>
<span id="cb389-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb389-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.934</span></span></code></pre></div>
<p>The intercept reflects the logit of the probability of a female response, overall, when VTL was 0. This intercept is not a difference or a parameter indicating a rate of change. As a result, the probability of 0.7 (<code>inverse_logit (0.86)</code>) <em>can</em> be interpreted in isolation. But what does 0.934 represent? Nothing useful, actually. The <code>A1</code> parameter reflects the difference in logits (2.65) between perceiving a female speaker at 0 VTL overall compared to when listeners indicated hearing an adult. However, since this is a <em>difference</em> it needs to be combined with a baseline number in order to be interpreted. For example if I say “that group was 3 seconds faster”, you need to know “faster than what?” in order to know the speed of the group. So, in order to get the log odds (and probability) of observing a female response given some apparent age, we need to combine the <code>Intercept</code> and <code>A1</code> parameters, and then turn this into a probability using the inverse logit function. Since the inverse logit transform is not additive, we will not get the same output if we combine these parameters and then transform versus transform and then combine. See <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-15">(10.15)</a> and compare to <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-14">(10.14)</a>.</p>
<p><span class="math display" id="eq:10-15">\[
\begin{equation}
\mathrm{logit}^{-1}(\mathrm{Intercept}+A1) \; \neq \; \mathrm{logit}^{-1}(\mathrm{Intercept}) +
\mathrm{logit}^{-1}(A1)
\tag{10.15}
\end{equation}
\]</span></p>
<p>Since they are not equal, only one can be correct. Our model combines parameters as logits, and therefore the appropriate way to consider our parameters is by also combining them as logits. So, in order to turn the <code>A1</code> effect into a probability it first needs to be combined with the intercept as a logit, and <em>then</em> converted using the inverse logit function. As seen below, converting to a probability and then combining can lead to strange outcomes: 1.64 is not even a valid probability. When we combine first and then convert, we see that the result is a reasonable probability that matches what we see in the right plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a> (i.e. 0.97 is a plausible value for the adult line at x=0).</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="co"># intercept + adult (bad)</span></span>
<span id="cb390-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-2" aria-hidden="true" tabindex="-1"></a><span class="fu">inverse_logit</span> (<span class="fl">0.86</span>) <span class="sc">+</span> <span class="fu">inverse_logit</span> (<span class="fl">2.65</span>)</span>
<span id="cb390-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1.637</span></span>
<span id="cb390-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb390-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-5" aria-hidden="true" tabindex="-1"></a><span class="co"># intercept + adult (good)</span></span>
<span id="cb390-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-6" aria-hidden="true" tabindex="-1"></a><span class="fu">inverse_logit</span> (<span class="fl">0.86</span> <span class="sc">+</span> <span class="fl">2.65</span>)</span>
<span id="cb390-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb390-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.971</span></span></code></pre></div>
<p>The same reasoning applies to the interpretation of slopes in logistic regression models. Our slope tells us that for every unit change in VTL we expect a 3.6 logit increase/decrease in our expected values. We cannot convert this to probabilities and then multiply this by VTL to predict probabilities. For example, the difference from 0 to 3.6 logits results in a change from 0.5 to 0.97 in probability. We may be inclined to say that our VTL slope suggests an increase of 0.47 in probabilities (0.97-0.5) for every unit change in VTL. Obviously, this will not work since it suggests a difference in probability of 1.41 for a VTL difference of 3 cm. As a result, slopes in logistic models can only be interpreted as reflecting consistent changes in the <em>logit</em> of a probability, and not in the probability itself.</p>
<p>Think of it this way. You can move along lines in the logit space (the left plot in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a>), and you can use the inverse logit to calculate the probability associated with any given value along this line. However, the slope of the curve relating our predictor to probabilities (the right plot in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a>) does <em>not have a constant slope</em>. You can’t find the slope of this curve for one fixed position and just move along it in a straight line. Obviously, if you did this you would immediately diverge from the curve in the right plot in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a> because that line does not have a constant slope. So, when you want to interpret slopes, just remember that your model only thinks of these as lines in the logit space, not in the probability space.</p>
</div>
<div id="c10-classification" class="section level3 hasAnchor" number="10.5.5">
<h3><span class="header-section-number">10.5.5</span> Using logistic models to understand classification<a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our models so far have presented us with the y-intercepts of our lines, the value of the y axis at x=0. However, when modeling in logits we might also be interested in the <strong>x intercepts</strong> of our lines. The x intercept is the value of x where <em>y is equal to zero</em>, the point where our line crosses the horizontal x axis. Why do we care about this? Consider the prediction lines in the left plot in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a>. When these lines have positive values we know a female response is expected and when these lines have a negative value we know that a male response is expected. Imagine a vertical line placed at the x axis intercept. This line would represent the <strong>category boundary</strong> between our two possible outcomes along the x axis (according to our model). Crossing the x intercept in one direction along the x axis means one category is more likely, and crossing in the other direction means the other category is now more likely.</p>
<p>We can find the x intercept by setting y=0 in our prediction equation and solving for (isolating) x, as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-16">(10.16)</a>. For complicated prediction equations (and even for simple ones), you can rely on algebra solving websites easily found on the internet. Below, we see that when y is equal to zero, the value of x is equal to the negative intercept (<span class="math inline">\(a\)</span>) divided by the slope (<span class="math inline">\(b\)</span>).</p>
<p><span class="math display" id="eq:10-16">\[
\begin{equation}
\begin{split}
y = a + b \cdot x \\
0 = a + b \cdot x \\
-a = b \cdot x \\
-a/b = x
\end{split}
\tag{10.16}
\end{equation}
\]</span></p>
<p>We can use the equation above to calculate our predicted boundary between male and female classifications along the VTL dimension. For example, based on the numbers in our model summary above, we expect that the overall category boundary is at <span class="math inline">\(-(a/b) = -(0.86 / -3.59) = -0.24\)</span>. However, remember that to do arithmetic operations on, or otherwise combine, our parameters, we have to use the original samples and not the summaries (see section <a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-manipulating-random-effects">5.8.2</a> for a discussion on this). We will present three ways to do this, in order of decreasing difficulty/tediousness. The first is by directly combining the fixed effects samples as necessary. First we get the unsummarized fixed effects samples:</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb391-1" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">=</span> <span class="fu">fixef</span> (model_gender_vtl, <span class="at">summary =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Then we divide the column representing the overall intercept by the column representing the overall slope. The result is a vector of individual samples from the posterior distribution of the x intercept of the line, i.e. the category boundary along the VTL dimension. We refer to specific columns by using the same names seen in the print statement for the fixed effects.</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate overall boundary = -a/b</span></span>
<span id="cb392-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb392-2" aria-hidden="true" tabindex="-1"></a>boundary <span class="ot">=</span> <span class="sc">-</span>samples[,<span class="st">&quot;Intercept&quot;</span>] <span class="sc">/</span> samples[,<span class="st">&quot;vtl&quot;</span>]</span></code></pre></div>
<p>To find the x intercept for the lines for apparent adults and children, we combine parameters in the same way as when we found the age-dependent lines above. However, this time we also relate these parameter estimates as a fraction as seen below.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-1" aria-hidden="true" tabindex="-1"></a><span class="co"># same but for adults</span></span>
<span id="cb393-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-2" aria-hidden="true" tabindex="-1"></a>boundary_adults <span class="ot">=</span> <span class="sc">-</span>(samples[,<span class="st">&quot;Intercept&quot;</span>] <span class="sc">+</span> samples[,<span class="st">&quot;A1&quot;</span>]) <span class="sc">/</span> </span>
<span id="cb393-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-3" aria-hidden="true" tabindex="-1"></a>  (samples[,<span class="st">&quot;vtl&quot;</span>] <span class="sc">+</span> samples[,<span class="st">&quot;vtl:A1&quot;</span>])</span>
<span id="cb393-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb393-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-5" aria-hidden="true" tabindex="-1"></a><span class="co"># now for children</span></span>
<span id="cb393-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-6" aria-hidden="true" tabindex="-1"></a>boundary_children <span class="ot">=</span> <span class="sc">-</span>(samples[,<span class="st">&quot;Intercept&quot;</span>] <span class="sc">-</span> samples[,<span class="st">&quot;A1&quot;</span>]) <span class="sc">/</span> </span>
<span id="cb393-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb393-7" aria-hidden="true" tabindex="-1"></a>  (samples[,<span class="st">&quot;vtl&quot;</span>] <span class="sc">-</span> samples[,<span class="st">&quot;vtl:A1&quot;</span>])</span></code></pre></div>
<p>The result of the above combinations is a set of vectors. Each of these vectors represents the posterior distribution of the category boundary (i.e. the x intercept) given the structure of our model and our model parameters. We can stick the vectors representing boundaries together and summarize them as seen below. We see that the overall boundary is close to what we predicted using the fixed effect summaries above.</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-1" aria-hidden="true" tabindex="-1"></a>boundaries_1 <span class="ot">=</span> <span class="fu">posterior_summary</span> (</span>
<span id="cb394-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span> (boundary, boundary_adults, boundary_children)) </span>
<span id="cb394-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb394-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-4" aria-hidden="true" tabindex="-1"></a>boundaries_1</span>
<span id="cb394-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-5" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Estimate Est.Error     Q2.5   Q97.5</span></span>
<span id="cb394-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-6" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary            0.2397   0.08613  0.07438  0.4104</span></span>
<span id="cb394-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-7" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary_adults     0.6558   0.07169  0.51829  0.7958</span></span>
<span id="cb394-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb394-8" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary_children  -0.9925   0.23018 -1.48021 -0.5908</span></span></code></pre></div>
<p>Our boundaries are expressed relative to a mean VTL of zero. If we want them expressed relative to the true mean we need to add it back in. Below we add the original mean VTL back to our summary (excluding the standard error), and we see the category boundaries relative to values of VTL in the range of our real speakers. This might seem to violate our many (repeated) warnings about summarizing posterior distributions before combining them. Why does this work? Because we are just adding a single number to all our observations, and not a set of values that are possibly correlated with our posterior samples. Adding a number to all samples raises/lowers the sample mean and all the quantiles exactly by that number, but otherwise has no effect on our summaries. Further, adding a single number to all values has no effect on their posterior standard deviation, which means that we don’t need to worry about misrepresenting the amount of uncertainty in our estimates.</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="co"># omit standard error column</span></span>
<span id="cb395-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb395-2" aria-hidden="true" tabindex="-1"></a>boundaries_1[,<span class="sc">-</span><span class="dv">2</span>] <span class="sc">+</span> <span class="fl">13.4</span>   <span class="co"># i.e. mean (exp_data$vtl_original)</span></span>
<span id="cb395-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb395-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Estimate  Q2.5 Q97.5</span></span>
<span id="cb395-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb395-4" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary             13.64 13.47 13.81</span></span>
<span id="cb395-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb395-5" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary_adults      14.06 13.92 14.20</span></span>
<span id="cb395-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb395-6" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary_children    12.41 11.92 12.81</span></span></code></pre></div>
<p>The second way to find boundaries is to use the line parameters we calculated in the previous section. We get the samples from our hypothesis object using the <code>attr</code> function as seen below. The individual samples underlying each hypothesis are stored as attributes so that these are not printed out every time the summary is.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb396-1" aria-hidden="true" tabindex="-1"></a>line_parameters <span class="ot">=</span> <span class="fu">attr</span> (gender_vtl_hypothesis, <span class="st">&quot;samples&quot;</span>)</span></code></pre></div>
<p>Since we know the first second and third hypotheses represented the overall, adult, and child intercepts, and the fourth, fifth, and sixth hypothesis represented the overall, adult, and child slopes, we can find the boundaries as seen below.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate boundary = -a/b</span></span>
<span id="cb397-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb397-2" aria-hidden="true" tabindex="-1"></a>boundary <span class="ot">=</span> <span class="sc">-</span>line_parameters[,<span class="dv">1</span>] <span class="sc">/</span> line_parameters[,<span class="dv">4</span>]</span>
<span id="cb397-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb397-3" aria-hidden="true" tabindex="-1"></a>boundary_adults <span class="ot">=</span> <span class="sc">-</span>line_parameters[,<span class="dv">2</span>] <span class="sc">/</span> line_parameters[,<span class="dv">5</span>]</span>
<span id="cb397-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb397-4" aria-hidden="true" tabindex="-1"></a>boundary_children <span class="ot">=</span> <span class="sc">-</span>line_parameters[,<span class="dv">3</span>] <span class="sc">/</span> line_parameters[,<span class="dv">6</span>]</span></code></pre></div>
<p>This process results in identical outcomes to our previous approach.</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-1" aria-hidden="true" tabindex="-1"></a>boundaries_2 <span class="ot">=</span> <span class="fu">posterior_summary</span> (</span>
<span id="cb398-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span> (boundary, boundary_adults, boundary_children)) </span>
<span id="cb398-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb398-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-4" aria-hidden="true" tabindex="-1"></a>boundaries_2[,<span class="sc">-</span><span class="dv">2</span>] <span class="sc">+</span> <span class="fl">13.4</span></span>
<span id="cb398-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-5" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Estimate  Q2.5 Q97.5</span></span>
<span id="cb398-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-6" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary             13.64 13.47 13.81</span></span>
<span id="cb398-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-7" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary_adults      14.06 13.92 14.20</span></span>
<span id="cb398-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb398-8" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary_children    12.41 11.92 12.81</span></span></code></pre></div>
<p>The final way to find the boundaries is to just find the age-dependent parameters, <em>and</em> the ratio of these, directly in <code>hypothesis</code> (or <code>short_hypothesis</code>). We can also add the average VTL value back in at the same time for good measure.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-1" aria-hidden="true" tabindex="-1"></a>boundaries_3 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb399-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-2" aria-hidden="true" tabindex="-1"></a>  model_gender_vtl,</span>
<span id="cb399-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;-(Intercept) / (vtl) + 13.4= 0&quot;</span>,                   <span class="co"># overall boundary</span></span>
<span id="cb399-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;-(Intercept + A1) / (vtl + vtl:A1) + 13.4 = 0&quot;</span>,    <span class="co"># adult boundary</span></span>
<span id="cb399-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;-(Intercept - A1) / (vtl - vtl:A1)  + 13.4 = 0&quot;</span>))  <span class="co"># child boundary</span></span>
<span id="cb399-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb399-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-7" aria-hidden="true" tabindex="-1"></a>boundaries_3[,<span class="sc">-</span><span class="dv">5</span>]</span>
<span id="cb399-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-8" aria-hidden="true" tabindex="-1"></a><span class="do">##    Estimate Est.Error  Q2.5 Q97.5</span></span>
<span id="cb399-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-9" aria-hidden="true" tabindex="-1"></a><span class="do">## H1    13.64   0.08613 13.47 13.81</span></span>
<span id="cb399-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-10" aria-hidden="true" tabindex="-1"></a><span class="do">## H2    14.06   0.07169 13.92 14.20</span></span>
<span id="cb399-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb399-11" aria-hidden="true" tabindex="-1"></a><span class="do">## H3    12.41   0.23018 11.92 12.81</span></span></code></pre></div>
<p>If the last way is so quick and easy, why bother learning how to combine and summarize the samples by hand? The problem is if you only know how to use a helper function (like <code>hypothesis</code>), you may become reliant on it for your work, and you can be limited by what it can and can’t do. If it changes or goes away, your ability to do what you need to do may suffer. If you know what <code>hypothesis</code> is doing, and know how to do it yourself, you can use it when it does what you want, but you also have the ability to do things independently when you want or need to.</p>
<p>The boundaries calculated above are presented with our data in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-5">10.5</a>. The right plot below can be thought of as representing a <strong>stimulus space</strong>, a space indicating variation in our experimental stimuli along one or more dimensions. We say <em>a</em> stimulus space and not <em>the</em> stimulus space because there may be many ways to consider variation in the stimuli, and this is just one of them. In this case, the only dimension we’re considering is VTL and all of our speakers can be placed along this dimension. In the right plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-5">10.5</a> the y axis doesn’t matter since this is a unidimensional representation.</p>
<div class="figure"><span style="display:block;" id="fig:F10-5"></span>
<img src="../wrong_plots/Figure 10.5.jpg" alt="(left) Points represent individual speakers in our data based on their VTL and the logit of the probability that they were identified as female. Lines indicate the overall relationship (black), the relationship expected for apparent children (green), and that expected for apparent adults (pink). Vertical lines indicate each line's x-intercept. Point colors indicate veridical speaker categories. (right) Territorial maps implied by each line presented in the left plot. Each map divides the VTL dimension into 'territories' associated with male and female responses."  />
<p class="caption">
Figure 10.5: (left) Points represent individual speakers in our data based on their VTL and the logit of the probability that they were identified as female. Lines indicate the overall relationship (black), the relationship expected for apparent children (green), and that expected for apparent adults (pink). Vertical lines indicate each line’s x-intercept. Point colors indicate veridical speaker categories. (right) Territorial maps implied by each line presented in the left plot. Each map divides the VTL dimension into ‘territories’ associated with male and female responses.
</p>
</div>
<p>We can divide the stimulus space into different regions associated with different expected outcomes of our dependent variable. We do this by finding the boundary and then finding which ‘region’ of the space corresponds to each response category. In our case, since the VTL slope is negative for all of our groups, we know that larger VTL values correspond to more male responses. As a result, VTL values greater than the boundary must correspond to the male category. Figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-5">10.5</a> show three divisions of our stimulus space based on our three boundaries we calculated above: One for apparent children, one for apparent adults, and an overall boundary. We can see in each case that the category boundary falls at the x intercept corresponding to each line in the left plot of the same figure. Nearey (1990, 1992, 1997) promoted the use of figures of this kind for perceptual modeling, and referred to these as <strong>territorial maps</strong>. Territorial maps divide a stimulus space into regions (or territories) based on the most probable outcome in that region, given the model. These maps have a very simple and useful interpretation: Stimuli falling in a category’s ‘territory’ is expected to be identified as a member of that category.</p>
<p>We can think about the characteristics of our fixed effects parameters, and our age-dependent lines, in terms of what they mean for the classification of speakers as female or male. It can be useful to think about category boundaries when interpreting logits because our model parameters can be interpreted as shifts in these boundaries. For example, our model intercept estimate was 0.86, and the effect for perceived adultness (<code>A1</code>) was 2.65. If we consider lines with fixed slopes, the effect of shifts in the y intercept on classification can be understood in terms of the slope of the line. Since our line has a downward (negative) slope along VTL, raising the <span class="math inline">\(y\)</span>-intercept has the effect of moving our x intercept to the ‘right’ towards higher values of VTL. In other words, the positive intercept shift associated with perceived adultness increases the category boundary between apparent male and female speakers along the VTL dimension. If the slope of our line had been positive the associations would be reversed: Positive y intercept shifts would lead to a ‘leftward’ motion of the category boundary.</p>
<p>Increasing the magnitude of our slopes (positive or negative) does not <em>necessarily</em> affect the location of the category boundary. Sometimes there can be compensatory change in the y intercept such that the x intercept remains unchanged. In general, we can say that two lines with unequal slopes but equal x intercepts differ in terms of how ‘categorical’ or ‘fuzzy’ classifications tend to be. This is because a steeper slope gets from high probabilities to low probabilities (or vice versa) faster, and therefore has a smaller ambiguous region relative to a slope with a smaller magnitude. For example, the points representing women and men are further apart along VTL than boys and girls. This means they are more separable along this dimension, and this is represented in the model by the steeper slope for speakers judged to be adults.
When lines differ in both slopes and intercepts, the effect on classification boundaries needs to be considered on a case by case basis. However, in general it is quite straightforward: One only needs to imagine the effects on our lines and the locations where they will cross zero. In figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-4">10.4</a>, we can see that the overall effect of apparent adultness is to increase the boundary between female and male responses along speaker VTL.</p>
</div>
<div id="answering-our-research-question" class="section level3 hasAnchor" number="10.5.6">
<h3><span class="header-section-number">10.5.6</span> Answering our research question<a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-question" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can answer our research questions based on our model above:</p>
<p>(Q1) What is the relationship between speaker VTL and the perception of femaleness?</p>
<p>(Q2) Does the relationship between VTL and apparent speaker gender vary in an age-dependent manner?</p>
<p>Speaker VTL is negatively related to the perception of femaleness with a slope of -3.59 logits per unit change in cm (s.d. = 0.39, 95% C.I = [-4.41, -2.88])). This effect increased by about 50% when listeners thought the speaker was an adult and decreased by about 50% when listeners thought the speaker was a child (mean = 2.65, s.d. = 0.35, 95% C.I = [2.01, 3.39]). Our results do indicate that the relationship between VTL and apparent femaleness varies as a function of the apparent age of the speaker. These differences can be understood in terms of the information presented in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-5">10.5</a> and, in particular, the territorial maps presented in the right plot. In the plot we see that when listeners thought the speaker was a child, the boundary between male and female speakers is at a lower value of VTL (12.4 cm based on our calculations above), which makes sense given that children are smaller overall. When the listener thinks the speaker is an adult, a higher value of VTL (14.0 cm) is required to predict a male response.</p>
<p>We think this model reflects behavior that ‘makes sense’ for human listeners. Research suggests that listeners more or less ‘know’ how different sorts of people sound as evidenced by their ability to classify speakers by age, size, and gender. If this is true, listeners need to know that younger speakers have shorter VTLs than older speakers. Necessarily, if there is a category boundary between male and female responses for both adults and children, the one between boys and girls <em>must</em> be lower along the VTL dimension than the boundary between men and women. In the absence of this, the model would not result in the accurate classification of stimuli. For example, the overall category boundary in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-5">10.5</a> would result in the expectation of female classifications for <em>all</em> the boys in our sample since they all fall to the left of this line (i.e. in the ‘female’ territory in the overall territorial map).</p>
</div>
</div>
<div id="measuring-sensitivity-and-bias" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Measuring sensitivity and bias<a href="logistic-regression-and-signal-detection-theory-models.html#measuring-sensitivity-and-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Previously we considered the probability that a listener would respond female based on speaker vocal-tract length (VTL) and apparent age. What our model didn’t really tell us much about was the extent to which listeners were able to correctly identify male and female voices. To do that, we have to carry out an analysis using principles developed in signal detection theory. Whole books can (and have) been written on this topic, and we are only going to deal with it very superficially here. Our intention is to show how <em>sensitivity</em> and <em>bias</em> (to be discussed momentarily) can be estimated with signal detection theory models implemented using logistic regression. The implementation used here is described in DeCarlo (1998), and a thorough introduction to detection theory can be found in Green and Swets (1966) or Wickens (2001).</p>
<p>Imagine we want to know how well listeners can identify cases when the speaker is actually female. To do this we might just calculate the percent of trials in which a speaker was female and was also identified as being female. However, imagine a listener identifies 100% of male and female speakers as female. If we only measure accuracy on female speaker trials this speaker would appear to perform perfectly. However, obviously, if they also think 100% of male speakers sound female then they show no ability to distinguish male and female speakers. Clearly, we need a measure that considers whether listeners can detect when female speakers are <em>not</em> there, in addition to being able to identify when they are.</p>
<p>First, we need to define some terminology. Consider the general case where the listener is trying to identify some signal or characteristic (e.g., femaleness), and the signal is either present or it is not. We define the <strong>hit rate</strong> (<span class="math inline">\(\mathrm{H}\)</span>) as the probability that a listener will say the speaker is female when they are. The <strong>false alarm rate</strong> (<span class="math inline">\(\mathrm{FA}\)</span>) is the probability that the listener will identify the speaker as female when they were not. We present this below for our data where <span class="math inline">\(Female\)</span> is our vector indicating whether the listener indicated hearing a male (0) or female speaker (1), and <span class="math inline">\(G_v\)</span> is a vector (<code>G_v</code>) that indicates whether the speaker’s veridical gender was male (<code>m</code>) or female (<code>f</code>).</p>
<p><span class="math display" id="eq:10-17">\[
\begin{equation}
\begin{split}
\mathrm{H} = P(Female=1 | G_v= \mathrm{f} \,) \\
\mathrm{FA} = P(Female=1 | G_v= \mathrm{m} \, )
\end{split}
\tag{10.17}
\end{equation}
\]</span></p>
<p>The lines above say: <span class="math inline">\(H\)</span> is equal to the probability the <span class="math inline">\(Female\)</span> equals 1 given that the veridical gender is female, and <span class="math inline">\(FA\)</span> is equal to the probability the <span class="math inline">\(Female\)</span> equals 1 given that the veridical gender is male. We can calculate the overall hit and false alarm rates for our data with the code below:</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb400-1" aria-hidden="true" tabindex="-1"></a>H <span class="ot">=</span> <span class="fu">mean</span>(exp_data<span class="sc">$</span>Female[exp_data<span class="sc">$</span>G_v <span class="sc">==</span> <span class="st">&quot;f&quot;</span>])</span>
<span id="cb400-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb400-2" aria-hidden="true" tabindex="-1"></a>FA <span class="ot">=</span> <span class="fu">mean</span>(exp_data<span class="sc">$</span>Female[exp_data<span class="sc">$</span>G_v <span class="sc">==</span> <span class="st">&quot;m&quot;</span>])</span></code></pre></div>
<p>Or more simply using:</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb401-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span>(exp_data<span class="sc">$</span>Female, exp_data<span class="sc">$</span>G_v, mean)</span>
<span id="cb401-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb401-2" aria-hidden="true" tabindex="-1"></a><span class="do">##      f      m </span></span>
<span id="cb401-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb401-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.8219 0.1694</span></span></code></pre></div>
<p><strong>Sensitivity</strong> is the ability to discriminate categories and is a function of the difference between a listener’s hit rate and their false alarm rate. If the hit rate is 1 and the false alarm rate is 0, this listener exhibits perfect discrimination: They can identify all females as females and identify <em>no</em> males as female. If a person has a hit rate of 0.5 and a false alarm rate of 0.5 it means they show no sensitivity at all, their hit and false alarm rates are equal. This person would perform as well as someone who was not even listening to the stimuli. However, a person with a hit rate of 0.9 and a false alarm rate of 0.9 <em>also</em> shows no sensitivity, even though they are identifying 90% of women as women. The reason for this is that they are identifying 90% of <em>people</em>, including men, as women.</p>
<p>There are potentially many ways to measure sensitivity. For example, we could just subtract the hit rate from the false alarm rate. However, this is not the best idea for many of the same reasons that we do not base our linear models directly on probabilities. One of the most common measures of sensitivity is <span class="math inline">\(d&#39;\)</span> (‘d-prime’), which is calculated as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-17">(10.17)</a>.</p>
<p><span class="math display" id="eq:10-18">\[
\begin{equation}
d&#39; = z(H) - z(FA)
\tag{10.18}
\end{equation}
\]</span></p>
<p>Where <span class="math inline">\(z(\cdot)\)</span> is a function that converts a proportion (or probability) to a z-score (a standard normal variable, see section <a href="probabilities-likelihood-and-inference.html#c2-standard-normal">2.5.4</a>). Models estimating <span class="math inline">\(d&#39;\)</span> can be implemented with Probit regression, which is basically analogous to logistic regression save for the fact that they rely on the cumulative Gaussian link function (see section <a href="multinomial-and-ordinal-regression.html#c12-cumulative-density">12.3.1</a>) rather than the inverse logit link function. The inverse logit function effectively performs the same function as <span class="math inline">\(z(\cdot)\)</span> in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-17">(10.17)</a>, meaning that logistic regression can be used to estimate <span class="math inline">\(d\)</span> (rather than <span class="math inline">\(d&#39;\)</span>). We define <span class="math inline">\(d\)</span> as the difference between the logit of the hit rate and the logit of the false alarm rate as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-19">(10.19)</a>.</p>
<p><span class="math display" id="eq:10-19">\[
\begin{equation}
d = \mathrm{logit}(H) - \mathrm{logit}(FA)
\tag{10.19}
\end{equation}
\]</span></p>
<p>When hit rates and false alarm rates are balanced around probabilities of 0.5, this is equivalent to balancing out around values of 0 logits. When hits and false alarms balance out like this, this means that errors were equally likely to occur in response to both male and female speakers. For example, a hit rate of 0.9 means listeners made a mistake on 10% of female trials, and a false alarm rate of 0.1 indicates the same error rate for male trials. But what if they don’t balance out?</p>
<p>Imagine a situation where a listener identifies 100% of females as female (hit rate = 1) and 50% of men as females (false alarm rate = 0.5). This seems to suggest that they make no mistakes on female trials but are only performing at chance for male trials. How could this be possible? In order to do well on female trials they need to know the speaker is female (obviously). But if they know when speakers are female, they must also know when the speaker is <em>not</em> male. So, why do they perform so poorly on the male trials? Rather than indicating differential performance across the two categories, a lack of balance across hits and false alarms indicates <strong>response bias</strong>, the tendency to select one category more than another. In this case, the listener does not show an increased ability to identify female speakers but rather a bias towards identifying speakers as female. A common way to measure bias is using what is called a <strong>criterion</strong>, defined as the negative of the average of the transformed hit and false alarm rates (<a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-20">(10.20)</a>). Since we are using logits we call this <span class="math inline">\(c&#39;\)</span> to distinguish it from the <span class="math inline">\(c\)</span> criterion measured using a probit model (DeCarlo 1998).</p>
<p><span class="math display" id="eq:10-20">\[
\begin{equation}
c&#39; = -\frac{1}{2} \, [\mathrm{logit}(H) + \mathrm{logit}(FA)]
\tag{10.20}
\end{equation}
\]</span></p>
<p>For historical reasons, in signal detection theory a negative criterion (negative bias) is associated with more positive responses and a positive criterion (positive bias) is associated with more negative responses. Note however that the calculation of <span class="math inline">\(c&#39;\)</span> in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-17">(10.17)</a> includes negating the mean of the logit of the hit and false alarm rates. As a result, this definition of bias involves a double negative which unnecessarily complicates things for many purposes: A higher average hit/false alarm rate is <em>negatively</em> related to the criterion, which is itself <em>negatively</em> related to outcomes. We think it’s important to be aware of this convention but do not feel bound to follow it. As a result, we’re going to divert from detection theory somewhat and simply define a bias measure <span class="math inline">\(b\)</span> as the negative of the criterion, as seen in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-21">(10.21)</a>.</p>
<p><span class="math display" id="eq:10-21">\[
\begin{equation}
b = -c&#39; = \frac{1}{2} \, [\mathrm{logit}(H) - \mathrm{logit}(FA)]
\tag{10.21}
\end{equation}
\]</span></p>
<p>When defined in this way, we see that increasing values of <span class="math inline">\(b\)</span> (positive bias) reflect the tendency for both hits and false alarm rates to increase, while negative values of <span class="math inline">\(b\)</span> (negative bias) reflects the tendency of both hits and false alarm rates to decrease. For our data, a positive bias would indicate an increased tendency to identify speakers as female (since this was the variable coded with a 1) while a negative bias would indicate an increased probability of a male response. As an example, let’s consider the detection of femaleness in our speaker’s voices. We’re going to consider how sensitivity and bias varies according to the veridical (not apparent) age of the speakers. Below we divide our data into trials involving (veridical) children and trials involving (veridical) adult speakers.</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb402-1" aria-hidden="true" tabindex="-1"></a><span class="co"># adult speaker data</span></span>
<span id="cb402-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb402-2" aria-hidden="true" tabindex="-1"></a>adults <span class="ot">=</span> exp_data[exp_data<span class="sc">$</span>A_v <span class="sc">==</span> <span class="st">&quot;a&quot;</span>,]</span>
<span id="cb402-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb402-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb402-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb402-4" aria-hidden="true" tabindex="-1"></a><span class="co"># child speaker data</span></span>
<span id="cb402-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb402-5" aria-hidden="true" tabindex="-1"></a>children <span class="ot">=</span> exp_data[exp_data<span class="sc">$</span>A_v <span class="sc">==</span> <span class="st">&quot;c&quot;</span>,]</span></code></pre></div>
<p>We can find the hit and false alarm rates by finding the average of our <code>Female</code> variable independently for veridical male and female speakers. We see these values below for adults, children, and overall.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-1" aria-hidden="true" tabindex="-1"></a><span class="co"># hit and false alarm rate, overall</span></span>
<span id="cb403-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span> (exp_data<span class="sc">$</span>Female, exp_data<span class="sc">$</span>G_v, mean)</span>
<span id="cb403-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      f      m </span></span>
<span id="cb403-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.8219 0.1694</span></span>
<span id="cb403-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb403-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-6" aria-hidden="true" tabindex="-1"></a><span class="co"># hit and false alarm rate, for adult</span></span>
<span id="cb403-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-7" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span> (adults<span class="sc">$</span>Female, adults<span class="sc">$</span>G_v, mean)</span>
<span id="cb403-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-8" aria-hidden="true" tabindex="-1"></a><span class="do">##       f       m </span></span>
<span id="cb403-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.86111 0.02667</span></span>
<span id="cb403-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb403-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-11" aria-hidden="true" tabindex="-1"></a><span class="co"># hit and false alarm rate, for children</span></span>
<span id="cb403-12"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-12" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span> (children<span class="sc">$</span>Female, children<span class="sc">$</span>G_v, mean)</span>
<span id="cb403-13"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-13" aria-hidden="true" tabindex="-1"></a><span class="do">##      f      m </span></span>
<span id="cb403-14"><a href="logistic-regression-and-signal-detection-theory-models.html#cb403-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.7228 0.4074</span></span></code></pre></div>
<p>Figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-6">10.6</a> presents these probabilities in addition to their logit values. Across all speakers, we see a balanced (zero bias) case, with hits and false alarms equally spaced around 0 logits. For veridical adults, the false alarm rate is lower than the hit rate is high. This indicates a negative bias meaning that listeners were more likely to respond male than female for adult speakers. We also see that hits and false alarm rates are more separated for adults, indicating a higher value of <span class="math inline">\(d\)</span> (i.e. sensitivity). For children, we see largely the opposite pattern: Hits were more likely than false alarms indicating a positive bias (and more female responses). In addition, the distance between hits and false alarm rates was much smaller than for adults, indicating a substantially reduced sensitivity and ability to discriminate male and female speakers.</p>
<div class="figure"><span style="display:block;" id="fig:F10-6"></span>
<img src="_main_files/figure-html/F10-6-1.jpeg" alt="(left) Hits and false alarm rates for the detection of speaker 'femaleness', averaged across all listeners. (right) Same as in the left plot, except as logits of the rates. The black point represents the average of hits and false alarms (the bias, $b$) and the distance between the green and red points reflects sensitivity ($d$)." width="4800" />
<p class="caption">
Figure 10.6: (left) Hits and false alarm rates for the detection of speaker ‘femaleness’, averaged across all listeners. (right) Same as in the left plot, except as logits of the rates. The black point represents the average of hits and false alarms (the bias, <span class="math inline">\(b\)</span>) and the distance between the green and red points reflects sensitivity (<span class="math inline">\(d\)</span>).
</p>
</div>
<p>When considered as in the right plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-6">10.6</a>, the estimation of hit rates, false alarm rates, and their average, is very similar to the two-group models described in chapter 5. In chapter 5 we estimated the effect of apparent adultness on apparent height. In that model we had two ‘groups’ in our data: Trials where listeners indicated hearing an adult and trials where listeners indicated hearing a child. So. our model predicted apparent height given the apparent age of the speaker. Since we used sum coding, the intercept in our model was the average of the two group means and the effect for age was equal to 1/2 the distance between the group means.</p>
<p>Here, we are predicting the probability of observing a female response and we have two groups: Trials where the speaker was <em>actually</em> a female and trials where they were not. Our model predicts the logit of the probability of a female response given the veridical gender of the speaker. The probability of a female response when the speaker is actually female corresponds to the hit rate, i.e. <span class="math inline">\(P(Female=1 | G_v= \mathrm{f} \,)\)</span>, as in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-17">(10.17)</a>. So, one of the groups in our two-group models is estimating the logit of the hit rate. Conversely, since the probability of a female response when the speaker is actually male corresponds to the false alarm rate, that means that the probability of a female response for actual male speakers represents our false alarm rate (i.e. <span class="math inline">\(P(Female=1 | G_v= \mathrm{f} \,)\)</span>. As a result, the other group in our two-group model estimates the false alarm rate. Since we are using sum coding, the intercept in our model is the average of the two group means and so is a measure of <em>bias</em> (since it is equal to <span class="math inline">\(b\)</span> and <span class="math inline">\(-c&#39;\)</span>, see <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-20">(10.20)</a> and <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-21">(10.21)</a>). The effect for our group predictor, in this case the group separation based on veridical gender, will equal 1/2 the distance between the hit and false alarm rates. This means that our predictor in this case will equal <span class="math inline">\(d/2\)</span>, meaning that the effect for veridical gender is related to <em>sensitivity</em>.</p>
<p>Before continuing, we want to highlight an extremely important property of these models. Any parameter interacting with the predictor indicating <em>veridical category</em>, in this case <code>G_v</code>, reflects variation in the separation of hit and false alarm rates. Therefore, any parameters that make the <code>G_v</code> effect change in our models reflect variation in sensitivity. In contrast, any predictor that does <em>not</em> interact with veridical category cannot affect the separation between hit and false alarm rates across different conditions. Instead, these effects simply move hit and false alarm rates together in the same direction, but do not affect the separation between them. As a result, such effects can only affect response bias, the overall tendency to report one category or the other.</p>
<div id="data-and-research-questions-6" class="section level3 hasAnchor" number="10.6.1">
<h3><span class="header-section-number">10.6.1</span> Data and research questions<a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’re going to keep working with the data we loaded above, which we reload below for convenience. We also center our VTL predictor and create a new predictor representing veridical gender. To represent veridical gender in our model we’re going to do something new, were going to directly specify one of the ‘secret’ numerical variables our models have been relying on all along (for example, see sections <a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-description-1">5.7.1</a>, <a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-description-1">7.2.2</a>).</p>
<p>When you have a categorical predictor with only two levels, the coefficient representing one of the levels cannot be estimated because it is the negative of the other coefficient. When using sum coding, this is represented numerically using a predictor that equals 1 for one group (the estimated parameter) and -1 for the other group. Below, we add a variable representing veridical speaker femaleness (<code>F_v</code>) with values of 1 (for female) and -1 (for male).</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb404-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb404-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-3" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&#39;contr.sum&#39;</span>,<span class="st">&#39;contr.sum&#39;</span>))</span>
<span id="cb404-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb404-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-6" aria-hidden="true" tabindex="-1"></a><span class="co"># our dependent variable</span></span>
<span id="cb404-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-7" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>Female <span class="ot">=</span> <span class="fu">as.numeric</span> (exp_data<span class="sc">$</span>G <span class="sc">==</span> <span class="st">&#39;f&#39;</span>)</span>
<span id="cb404-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-9" aria-hidden="true" tabindex="-1"></a><span class="co"># make a copy of vtl</span></span>
<span id="cb404-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-10" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl_original <span class="ot">=</span> exp_data<span class="sc">$</span>vtl</span>
<span id="cb404-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-11" aria-hidden="true" tabindex="-1"></a><span class="co"># center vtl</span></span>
<span id="cb404-12"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-12" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl <span class="ot">=</span> exp_data<span class="sc">$</span>vtl <span class="sc">-</span> <span class="fu">mean</span> (exp_data<span class="sc">$</span>vtl)</span>
<span id="cb404-13"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-14"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-14" aria-hidden="true" tabindex="-1"></a><span class="co"># create veridical gender predictor </span></span>
<span id="cb404-15"><a href="logistic-regression-and-signal-detection-theory-models.html#cb404-15" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>F_v <span class="ot">=</span> <span class="fu">ifelse</span> (exp_data<span class="sc">$</span>G_v<span class="sc">==</span><span class="st">&quot;f&quot;</span>, <span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p>We add the <code>F_v</code> numerical values ‘by hand’, rather than using a factor as we’ve usually done, for two reasons. First, as will be discussed below, we need the category coded with a 1 in our predictor (the estimated category) to be the same category coded with a 1 in the dependent variable. Creating a vector that we know equals 1 for a specific category and -1 for the other is a simple way to be sure of this. Second, we do this as a pedagogical device to highlight the fact that, for factors with two levels, a categorical predictor is effectively a ‘slope’ indicating the difference between groups expected for a 1 unit change in our fake predictor. In our case our groups have values of -1 and 1 resulting in a difference of 2 between groups for our fake predictor. This is why, in the two group case, the difference between groups is equal to twice the value of the estimated parameter (i.e. the real difference is two fake units). In any case, representing a factor with two levels using -1 for one group and 1 for the other has the same effects as sum coding that factor, and we will simply be treating the <code>F_v</code> as a sum coded factor in the discussion below.</p>
<p>The variables from our data frame involved in this analysis are:</p>
<ul>
<li><code>L</code>: A number from 1-15 indicating which <em>listener</em> responded to the trial.</li>
<li><code>S</code>: A number from 1-139 indicating which <em>speaker</em> produced the trial stimulus.</li>
<li><code>G</code>: The <em>apparent gender</em> of the speaker indicated by the listener, <code>f</code> (female) or <code>m</code> (male).</li>
<li><code>G_v</code>: The <em>veridical gender</em> of the speaker indicated by the listener, <code>f</code> (female) or <code>m</code> (male).</li>
<li><code>A_v</code>: The <em>veridical age</em> of the speaker indicated by the listener, <code>a</code> (adult) or <code>c</code> (child).</li>
</ul>
<p>We’re going to model the ability of listeners to discriminate the gender of speakers from their voices. We would like to know:</p>
<p>(Q1) How different is listeners’ ability to discriminate the gender of children and adults?</p>
<p>(Q2) Is response bias different for children and for adults?</p>
</div>
<div id="description-of-the-model-9" class="section level3 hasAnchor" number="10.6.2">
<h3><span class="header-section-number">10.6.2</span> Description of the model<a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our model formula should minimally be:</p>
<p><code>Female ~ F_v + (F_v|L) + (1|S)</code></p>
<p>Which predicts the response (female = 1, male = 0) as a function of the actual speaker gender (female = 1, male = -1). Again, note the similarity to the two group models we fit in chapter 5. For this model, the intercept would reflect response bias and the <code>F_v</code> parameter would reflect sensitivity. The model formula we’re actually going to use looks like this:</p>
<p><code>Female ~ F_v * A_v + (F_v * A_v|L) + (1|S)</code></p>
<p>Which we can expand to:</p>
<p><code>Female ~ F_v + A_v + F_v:A_v + (F_v + A_v + F_v:A_v|L) + (1|S)</code></p>
<p>Where <code>A_v</code> represents veridical adultness. As noted above, all predictors in these sorts of models that do not interact with <code>F_v</code> represent bias terms. Thus, the model intercept represents overall bias and the <code>A_v</code> term reflects changes in bias as a function of the veridical adultness of the speaker. All model parameters that <em>do</em> interact with <code>F_v</code> reflect sensitivity. So, <code>F_v</code> represents overall sensitivity and the <code>F_v:A_v</code> interaction represents variation in sensitivity dependent on veridical age. We’re going to keep using the same priors we fit for our last logistic model:</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb405-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb405-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb405-2" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb405-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb405-3" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb405-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb405-4" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>))</span></code></pre></div>
<p>Our full model specification is:</p>
<p><span class="math display" id="eq:10-22">\[
\begin{equation}
\begin{split}
Female_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \mathrm{logistic} (z_{[i]}) \\
z_{[i]} = b_{[i]} + d_{[i]}  \\
b_{[i]} = \mathrm{Intercept} + A_v + A_v \colon L_{[\mathsf{L}_{[i]}]} + L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\
d_{[i]} =  F_v + F_v \colon A_v + F_v \colon L_{[\mathsf{L}_{[i]}]} + F_v \colon A_v \colon L_{[\mathsf{L}_{[i]}]}  \\ \\
\textrm{Priors:} \\
S_{[\bullet]} \sim \mathrm{Normal}(0,\sigma_{S}) \\
\begin{bmatrix} L_{[\bullet]} \\ A_v \colon L_{[\bullet]} \\ F_v \colon L_{[\bullet]} \\ A \colon F_v \colon L_{[\bullet]} \\ \end{bmatrix}
\sim \mathrm{MVNormal} \left(\, \begin{bmatrix} 0\\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \mathrm{\Sigma} \right) \\ \\
Intercept \sim \mathrm{t}(3, 0, 3) \\
A, VTL, A \colon VTL \sim \mathrm{t}(3, 0, 3) \\
\sigma_{L}, \sigma_{A_v \colon L}, \sigma_{F_v \colon L} , \sigma_{A_v  \colon F_v \colon L}, \sigma_{S} \sim \mathrm{t}(3, 0, 3) \\
R \sim \mathrm{LKJCorr} (2)
\end{split}
\tag{10.22}
\end{equation}
\]</span></p>
<p>In chapter 9 we discussed how the model representation above separates our line parameters into those affecting slopes and those affecting intercepts. In <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-22">(10.22)</a> we can instead see a segregation of parameters into those affecting biases (<span class="math inline">\(b_{[i]}= \dots\)</span>) and those affecting sensitivity (<span class="math inline">\(d_{[i]}= \dots\)</span>). Our plain English model description is now:</p>
<blockquote>
<p>We’re treating our femaleness judgments (1 or 0 for female or male) as coming from a Bernoulli distribution with a probability that varies trial to trial. The <em>logit of the probability</em> (<span class="math inline">\(z\)</span>) varies based on the sum of parameters related to response bias (<span class="math inline">\(b\)</span>) and discrimination (<span class="math inline">\(d\)</span>). The expected bias for a given situation is equal to the sum of the overall intercept, an effect for veridical age (<span class="math inline">\(A_v\)</span>), listener-specific effects for veridical age (<span class="math inline">\(A_v \colon L\)</span>), listener-specific deviations (<span class="math inline">\(L\)</span>), and speaker-specific deviations (S). The expected discrimination in a given situation varies based on an overall discrimination (<span class="math inline">\(F_v\)</span>, the main effect), deviations based on veridical age (<span class="math inline">\(F_v \colon A_v\)</span>), listener-specific deviations (<span class="math inline">\(F_v \colon L\)</span>), and listener-specific interactions between veridical age and veridical femaleness (<span class="math inline">\(F_v \colon A_v \colon L\)</span>)). The speaker intercept (<span class="math inline">\(S\)</span>) terms were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The listener random effects were drawn from a multivariate normal distribution with means of zero and a covariance matrix estimated from the data. All other effects (e.g., the Intercept, VTL, A, etc.) were treated as ‘fixed’ and drawn from prior distributions appropriate for their expected range of values.</p>
</blockquote>
</div>
<div id="fitting-and-interpreting-the-model-9" class="section level3 hasAnchor" number="10.6.3">
<h3><span class="header-section-number">10.6.3</span> Fitting and interpreting the model<a href="logistic-regression-and-signal-detection-theory-models.html#fitting-and-interpreting-the-model-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below is the function call we need to fit the model described in <a href="logistic-regression-and-signal-detection-theory-models.html#eq:10-22">(10.22)</a>. Note that it is simply a logistic regression model, just one with a specific structure as discussed above.</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb406-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-2" aria-hidden="true" tabindex="-1"></a>model_gender_dt <span class="ot">=</span></span>
<span id="cb406-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span> (Female <span class="sc">~</span> F_v<span class="sc">*</span>A_v <span class="sc">+</span> (F_v<span class="sc">*</span>A_v<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data=</span>exp_data, </span>
<span id="cb406-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, <span class="at">family=</span><span class="st">&quot;bernoulli&quot;</span>, </span>
<span id="cb406-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>,  </span>
<span id="cb406-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb406-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-7" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb406-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-8" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb406-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb406-9" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>)))</span></code></pre></div>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb407-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb407-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb407-2" aria-hidden="true" tabindex="-1"></a>model_gender_dt <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;10_model_gender_dt.RDS&#39;</span>)</span></code></pre></div>
<p>We are mainly interested in the fixed effects and combinations of these:</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb408-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span> (model_gender_dt)</span>
<span id="cb408-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb408-2" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error    Q2.5   Q97.5</span></span>
<span id="cb408-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb408-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept  -0.3514    0.2634 -0.8774  0.1545</span></span>
<span id="cb408-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb408-4" aria-hidden="true" tabindex="-1"></a><span class="do">## F_v         2.2522    0.1851  1.9038  2.6424</span></span>
<span id="cb408-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb408-5" aria-hidden="true" tabindex="-1"></a><span class="do">## A_v1       -0.7712    0.2255 -1.2340 -0.3309</span></span>
<span id="cb408-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb408-6" aria-hidden="true" tabindex="-1"></a><span class="do">## F_v:A_v1    1.3446    0.1764  1.0086  1.7019</span></span></code></pre></div>
<p>We can find the age-specific intercept terms, representing the bias, and double the age-specific <code>F_v</code> effects, representing sensitivity, as seen below.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-1" aria-hidden="true" tabindex="-1"></a>gender_dt_hypothesis <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb409-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-2" aria-hidden="true" tabindex="-1"></a>  model_gender_dt,</span>
<span id="cb409-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;Intercept = 0&quot;</span>,           <span class="co"># overall bias</span></span>
<span id="cb409-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-4" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;Intercept + A_v1 = 0&quot;</span>,    <span class="co"># adult bias</span></span>
<span id="cb409-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-5" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;Intercept - A_v1 = 0&quot;</span>,    <span class="co"># child bias</span></span>
<span id="cb409-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-6" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;2*(F_v) = 0&quot;</span>,             <span class="co"># overall sensitivity</span></span>
<span id="cb409-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-7" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;2*(F_v + F_v:A_v1) = 0&quot;</span>,  <span class="co"># adult sensitivity</span></span>
<span id="cb409-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb409-8" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;2*(F_v - F_v:A_v1) = 0&quot;</span>)) <span class="co"># child sensitivity</span></span></code></pre></div>
<p>We plot these biases and sensitivities in the left plot of figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-7">10.7</a>. In the same figure we also compare listener-dependent biases and sensitivities for veridical adults and children. Since we’ve covered how to easily recover these ‘random effects’ previously (e.g. in section <a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-inspecting-random-effects">4.6.1</a>) we only provide a single example below.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb410-1" aria-hidden="true" tabindex="-1"></a><span class="co"># listener-dependent biases for veridical adults</span></span>
<span id="cb410-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb410-2" aria-hidden="true" tabindex="-1"></a>biases_adult <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb410-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb410-3" aria-hidden="true" tabindex="-1"></a>  model_gender_dt,</span>
<span id="cb410-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb410-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;Intercept+A_v1 = 0&quot;</span>),<span class="at">group=</span><span class="st">&quot;L&quot;</span>, <span class="at">scope=</span><span class="st">&quot;coef&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:F10-7"></span>
<img src="_main_files/figure-html/F10-7-1.jpeg" alt="(left) Average bias and sensitivity and the 'simple effects' of bias and sensitivity across levels of veridical adultness. (middle) Bias and sensitivity for adult speakers, presented individually for each listener. (right) Bias and sensitivity for child speakers, presented individually for each listener." width="4800" />
<p class="caption">
Figure 10.7: (left) Average bias and sensitivity and the ‘simple effects’ of bias and sensitivity across levels of veridical adultness. (middle) Bias and sensitivity for adult speakers, presented individually for each listener. (right) Bias and sensitivity for child speakers, presented individually for each listener.
</p>
</div>
</div>
<div id="answering-our-research-questions-1" class="section level3 hasAnchor" number="10.6.4">
<h3><span class="header-section-number">10.6.4</span> Answering our research questions<a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-questions-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following verbal description of the results in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-7">10.7</a> are based on the biases and sensitivities reconstructed in <code>gender_dt_hypothesis</code>, presented in table <a href="logistic-regression-and-signal-detection-theory-models.html#tab:T10-1">10.1</a>. Our model suggests a bias towards male responses for adults , and perhaps a slight bias towards female responses for children. Speaker femaleness appeared to be discriminable for both adults and children, although sensitivity was substantially larger for adults. Based on figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-7">10.7</a> We can also say that all listeners were able to discriminate male and female adult speakers, and most (but not all) were able to do this for children.</p>
<table>
<caption><span id="tab:T10-1">Table 10.1: </span>Posterior means, standard errors, and 2.5% and 97.5% quantiles for bias and sensitivity under different conditions.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Est.Error</th>
<th align="right">Q2.5</th>
<th align="right">Q97.5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Overall Bias</td>
<td align="right">-0.35</td>
<td align="right">0.26</td>
<td align="right">-0.88</td>
<td align="right">0.15</td>
</tr>
<tr class="even">
<td align="left">Adult Bias</td>
<td align="right">-1.12</td>
<td align="right">0.34</td>
<td align="right">-1.81</td>
<td align="right">-0.49</td>
</tr>
<tr class="odd">
<td align="left">Child Bias</td>
<td align="right">0.42</td>
<td align="right">0.36</td>
<td align="right">-0.28</td>
<td align="right">1.11</td>
</tr>
<tr class="even">
<td align="left">Overall Sensitivity</td>
<td align="right">4.50</td>
<td align="right">0.37</td>
<td align="right">3.81</td>
<td align="right">5.28</td>
</tr>
<tr class="odd">
<td align="left">Adult Sensitivity</td>
<td align="right">7.19</td>
<td align="right">0.57</td>
<td align="right">6.13</td>
<td align="right">8.39</td>
</tr>
<tr class="even">
<td align="left">Child Sensitivity</td>
<td align="right">1.82</td>
<td align="right">0.44</td>
<td align="right">0.94</td>
<td align="right">2.72</td>
</tr>
</tbody>
</table>
<p>We can potentially understand the difference in sensitivity between veridical children and veridical adults if we assume that: 1) Speaker VTL is a useful clue in the discrimination of male and female speakers, and 2) The distribution of VTL between our speaker categories in the whole population is roughly the same as in our data. Consider the distribution of speaker VTL across boys and girls, and women and men in figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-8">10.8</a>. Imagine a listener encounters a speaker with a VTL of 15 cm. This value is typical for an adult male but <em>extremely</em> unlikely for an adult female. As a result, a speaker with a VTL of 15 cm is very likely to be an male. Because of the lack of overlap along VTL for adult males and females, there are many values of VTL that are plausible for adult males that are implausible for adult females. As a result, adult men and women will be easier to discriminate along the VTL dimension. In contrast, the boy and girl distributions largely overlap. This means a VTL of 12 cm could plausibly be either a boy or a girl, and the same could be said for most VTL values for boys and girls. As a result, it will clearly be more difficult to distinguish the gender of children along the VTL dimension and we can reasonably expect sensitivity to be smaller than it is for adults.</p>
<div class="figure"><span style="display:block;" id="fig:F10-8"></span>
<img src="_main_files/figure-html/F10-8-1.jpeg" alt="Distribution of boys, girls, men, and women in our speaker data according to their vocal-tract length." width="4800" />
<p class="caption">
Figure 10.8: Distribution of boys, girls, men, and women in our speaker data according to their vocal-tract length.
</p>
</div>
<p>We can also potentially explain the changes in response bias that we saw in our results based on figure <a href="logistic-regression-and-signal-detection-theory-models.html#fig:F10-8">10.8</a>. Below we see a table showing categorizations for adult males and females.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-1" aria-hidden="true" tabindex="-1"></a><span class="fu">xtabs</span> ( <span class="sc">~</span> adults<span class="sc">$</span>C_v <span class="sc">+</span> adults<span class="sc">$</span>C)</span>
<span id="cb411-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-2" aria-hidden="true" tabindex="-1"></a><span class="do">##           adults$C</span></span>
<span id="cb411-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-3" aria-hidden="true" tabindex="-1"></a><span class="do">## adults$C_v   b   g   m   w</span></span>
<span id="cb411-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-4" aria-hidden="true" tabindex="-1"></a><span class="do">##          b   0   0   0   0</span></span>
<span id="cb411-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-5" aria-hidden="true" tabindex="-1"></a><span class="do">##          g   0   0   0   0</span></span>
<span id="cb411-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-6" aria-hidden="true" tabindex="-1"></a><span class="do">##          m  31   0 626  18</span></span>
<span id="cb411-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb411-7" aria-hidden="true" tabindex="-1"></a><span class="do">##          w  97 109   3 511</span></span></code></pre></div>
<p>We see that men are rarely confused with girls or women (18 out of 675 trials, 2.7%). In contrast, women are confused with males in 15% (100/675) of cases, and specifically confused with boys in 14% (97/675) of cases. This is likely because adult males tend to have substantially longer VTLs than women and girls, but the VTLs of adult females and younger boys overlap somewhat. Recall that a response bias is simply an increase in responses for that category overall. Since men and women are balanced in our sample, if adult males tend to be identified as male almost always, and adult females are sometimes confused with younger males, this will necessarily result in a ‘bias’ towards male responses.</p>
</div>
</div>
<div id="exercises-9" class="section level2 hasAnchor" number="10.7">
<h2><span class="header-section-number">10.7</span> Exercises<a href="logistic-regression-and-signal-detection-theory-models.html#exercises-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The analyses in the main body of the text all involve only the unmodified ‘actual’ resonance level (in <code>exp_data</code>). Responses for the stimuli with the simulate ‘big’ resonance are reserved for exercises throughout. You can get the ‘big’ resonance in the <code>exp_ex</code> data frame, or all data in the <code>exp_data_all</code> data frame.</p>
<p>Fit and interpret one of the suggested models:</p>
<ol style="list-style-type: decimal">
<li><p>Easy: Analyze the (pre-fit) model that’s exactly like <code>model_gender_vtl</code>, except using the data in <code>exp_ex</code> (<code>bmmb::get_model("10_model_gender_vtl_ex.RDS")</code>).</p></li>
<li><p>Easy: Analyze the (pre-fit) model that’s exactly like <code>model_gender_dt</code>, except using the data in <code>exp_ex</code> (<code>bmmb::get_model("10_model_gender_dt_ex.RDS")</code>).</p></li>
<li><p>Medium: Fit models like the ones in this chapter, but include comparisons across resonance levels. For example, the model formula for a model could be <code>Female ~ vtl*A+R+A:R + (vtl*A+R+A:R|L) + (1|S)</code>. This model includes an effect for resonance level, in addition to an interaction between resonance and apparent age.</p></li>
<li><p>Hard: Fit models like the ones in this chapter, but predict apparent age instead of apparent gender.</p></li>
</ol>
<p>In any case, describe the model, present and explain the results, and include some figures.</p>
</div>
<div id="references-7" class="section level2 hasAnchor" number="10.8">
<h2><span class="header-section-number">10.8</span> References<a href="logistic-regression-and-signal-detection-theory-models.html#references-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>DeCarlo, L. T. (1998). Signal detection theory and generalized linear models. Psychological Methods, 3(2), 186.</p>
<p>Gelman, A., Hill, J., &amp; Yajima, M. (2012). Why we (usually) don’t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2), 189–211.</p>
<p>Green, D. M., &amp; Swets, J. A. (1966). Signal detection theory and psychophysics (Vol. 1, pp. 1969-2012). New York: Wiley.</p>
<p>Kruschke, J. (2014). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.</p>
<p>McCullagh, P., &amp; Nelder, J. A. (2019). Generalized linear models. Routledge.</p>
<p>Nearey, T. M. (1990). The segment as a unit of speech perception. Journal of Phonetics.</p>
<p>Nearey, T. M. (1992). Context effects in a double-weak theory of speech perception. Language and
Speech, 35(1-2), 153-171.</p>
<p>Nearey, T. M. (1997). Speech perception as pattern recognition. The Journal of the Acoustical Society of America, 101(6), 3241-3254.</p>
<p>Wickens, T. D. (2001). Elementary signal detection theory. Oxford university press.</p>
</div>
<div id="plot-code-9" class="section level2 hasAnchor" number="10.9">
<h2><span class="header-section-number">10.9</span> Plot Code<a href="logistic-regression-and-signal-detection-theory-models.html#plot-code-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-2"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-2" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-3"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-3" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.1</span></span>
<span id="cb412-4"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-4" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-5"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-6"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-7"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb412-8"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-8" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb412-9"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-9" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&#39;contr.sum&#39;</span>,<span class="st">&#39;contr.sum&#39;</span>))</span>
<span id="cb412-10"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-11"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-11" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">table</span> (exp_data<span class="sc">$</span>S, exp_data<span class="sc">$</span>C_v)</span>
<span id="cb412-12"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-12" aria-hidden="true" tabindex="-1"></a>mod_cat <span class="ot">=</span> <span class="fu">apply</span> (tab, <span class="dv">1</span>,which.max)</span>
<span id="cb412-13"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-14"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-15"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-15" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl <span class="ot">=</span> exp_data<span class="sc">$</span>vtl <span class="sc">-</span> <span class="fu">mean</span> (exp_data<span class="sc">$</span>vtl)</span>
<span id="cb412-16"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-17"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-17" aria-hidden="true" tabindex="-1"></a>aggd <span class="ot">=</span> <span class="fu">aggregate</span> (<span class="fu">cbind</span> ( height, A<span class="sc">==</span><span class="st">&quot;a&quot;</span>, G<span class="sc">==</span><span class="st">&quot;f&quot;</span>, vtl,f0, vtl) <span class="sc">~</span> S <span class="sc">+</span> C_v, </span>
<span id="cb412-18"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-18" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> exp_data, <span class="at">FUN =</span> mean)</span>
<span id="cb412-19"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-20"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-20" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="fl">1.5</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb412-21"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (aggd<span class="sc">$</span>vtl, aggd<span class="sc">$</span>height, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)][<span class="fu">factor</span>(aggd<span class="sc">$</span>C_v)], </span>
<span id="cb412-22"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-22" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),  <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">130</span>,<span class="dv">185</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb412-23"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Apparent height (inches)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-24"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-24" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb412-25"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-25" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="fu">lm</span>(aggd<span class="sc">$</span>height<span class="sc">~</span>aggd<span class="sc">$</span>vtl)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb412-26"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-26" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (aggd<span class="sc">$</span>vtl, aggd<span class="sc">$</span>height, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb412-27"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][aggd<span class="sc">$</span>group])</span>
<span id="cb412-28"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-29"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-29" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (.<span class="dv">8</span>,<span class="dv">165</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb412-30"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-30" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">16</span>,<span class="at">pt.cex=</span><span class="dv">2</span>)</span>
<span id="cb412-31"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-31" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (exp_data<span class="sc">$</span>vtl, exp_data<span class="sc">$</span>G<span class="sc">==</span><span class="st">&#39;f&#39;</span>, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)][<span class="fu">factor</span>(aggd<span class="sc">$</span>C_v)], <span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb412-32"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-32" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),  <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">1</span>,<span class="fl">1.1</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb412-33"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-33" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;G == &#39;f&#39;&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-34"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-34" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb412-35"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-35" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="fu">lm</span>(aggd[,<span class="dv">5</span>]<span class="sc">~</span>aggd<span class="sc">$</span>vtl)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb412-36"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-36" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (aggd<span class="sc">$</span>vtl, aggd[,<span class="dv">5</span>], <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb412-37"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-37" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][aggd<span class="sc">$</span>group])</span>
<span id="cb412-38"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-38" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span>.<span class="dv">5</span>)</span>
<span id="cb412-39"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-39" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span> (<span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb412-40"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-41"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-41" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (aggd<span class="sc">$</span>vtl, aggd[,<span class="dv">5</span>], <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)][<span class="fu">factor</span>(aggd<span class="sc">$</span>C_v)], </span>
<span id="cb412-42"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-42" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),  <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">1</span>,<span class="fl">1.1</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb412-43"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-43" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;P(G  = &#39;f&#39;)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-44"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-44" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb412-45"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-45" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="fu">lm</span>(aggd[,<span class="dv">5</span>]<span class="sc">~</span>aggd<span class="sc">$</span>vtl)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb412-46"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-46" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (aggd<span class="sc">$</span>vtl, aggd[,<span class="dv">5</span>], <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb412-47"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-47" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][aggd<span class="sc">$</span>group])</span>
<span id="cb412-48"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-48" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span>.<span class="dv">5</span>)</span>
<span id="cb412-49"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-50"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-50" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (aggd<span class="sc">$</span>vtl, <span class="fu">logit</span>(aggd[,<span class="dv">5</span>]), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)][<span class="fu">factor</span>(aggd<span class="sc">$</span>C_v)], </span>
<span id="cb412-51"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-51" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),  <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">6.1</span>,<span class="fl">6.1</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb412-52"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-52" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Logit (P(G  = &#39;f&#39;))&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-53"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-53" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb412-54"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-54" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="fu">lm</span>(<span class="fu">logit</span>(aggd[,<span class="dv">5</span>])<span class="sc">~</span>aggd<span class="sc">$</span>vtl)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb412-55"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-55" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (aggd<span class="sc">$</span>vtl, <span class="fu">logit</span>(aggd[,<span class="dv">5</span>]), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb412-56"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-56" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][aggd<span class="sc">$</span>group])</span>
<span id="cb412-57"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-57" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>)</span>
<span id="cb412-58"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-59"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-59" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">1</span>,<span class="at">text=</span><span class="st">&quot;Centered VTL (cm)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">line =</span> <span class="fl">1.5</span>, <span class="at">cex=</span><span class="fl">0.9</span>)</span>
<span id="cb412-60"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-61"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-61" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-62"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-62" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.2</span></span>
<span id="cb412-63"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-63" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-64"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-65"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-65" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,.<span class="dv">01</span>)</span>
<span id="cb412-66"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-66" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x</span>
<span id="cb412-67"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-68"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-68" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb412-69"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-69" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,y, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">4</span>, <span class="at">col=</span>bmmb<span class="sc">::</span><span class="st">&quot;deepgreen&quot;</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="at">main =</span> <span class="st">&quot;y = x&quot;</span>,</span>
<span id="cb412-70"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-70" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;Predictor&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Logits&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-71"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-71" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">v=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">2</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-72"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-73"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-73" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,bmmb<span class="sc">::</span><span class="fu">inverse_logit</span> (y), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">4</span>, <span class="at">col=</span>bmmb<span class="sc">::</span><span class="st">&quot;darkorange&quot;</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>), </span>
<span id="cb412-74"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-74" aria-hidden="true" tabindex="-1"></a>      <span class="at">main =</span> <span class="st">&quot;y = inverse logit ( x )&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Predictor&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Probability&quot;</span>,</span>
<span id="cb412-75"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-75" aria-hidden="true" tabindex="-1"></a>      <span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-76"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-76" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,.<span class="dv">5</span>),<span class="at">v=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">2</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-77"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-78"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-78" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,bmmb<span class="sc">::</span><span class="fu">logit</span>(bmmb<span class="sc">::</span><span class="fu">inverse_logit</span> (y)), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">4</span>, </span>
<span id="cb412-79"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-79" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>bmmb<span class="sc">::</span><span class="st">&quot;lavender&quot;</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>,</span>
<span id="cb412-80"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-80" aria-hidden="true" tabindex="-1"></a>      <span class="at">main =</span> <span class="st">&quot;y = logit ( inverse logit ( x ) )&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Predictor&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Logits&quot;</span>)</span>
<span id="cb412-81"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-81" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">v=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">2</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-82"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-83"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-83" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-84"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-84" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.3</span></span>
<span id="cb412-85"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-85" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-86"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-87"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-87" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="fl">3.5</span>,<span class="fl">3.5</span>,.<span class="dv">01</span>)</span>
<span id="cb412-88"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-88" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x</span>
<span id="cb412-89"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-90"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-90" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb412-91"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-92"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-92" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,<span class="fu">inverse_logit</span> (y), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span>darkorange, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),</span>
<span id="cb412-93"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-93" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">&quot;Logit&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Probability&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">yaxs=</span><span class="st">&#39;i&#39;</span>,<span class="at">xaxs=</span><span class="st">&#39;i&#39;</span>)</span>
<span id="cb412-94"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-94" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,.<span class="dv">1</span>),<span class="at">v=</span>(<span class="sc">-</span><span class="dv">9</span><span class="sc">:</span><span class="dv">9</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-95"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-95" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h =</span> <span class="fl">0.5</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb412-96"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-97"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-97" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,y, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span>deepgreen, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),<span class="at">xlab=</span><span class="st">&quot;Predictor&quot;</span>,</span>
<span id="cb412-98"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-98" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;Logit&quot;</span>,<span class="at">yaxs=</span><span class="st">&#39;i&#39;</span>,<span class="at">xaxs=</span><span class="st">&#39;i&#39;</span>)</span>
<span id="cb412-99"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-99" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fu">logit</span>(<span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="fl">0.9</span>,.<span class="dv">1</span>)),<span class="at">v=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">9</span><span class="sc">:</span><span class="dv">9</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-100"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-100" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb412-101"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-102"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-103"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-103" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-104"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-104" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.4</span></span>
<span id="cb412-105"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-105" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-106"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-107"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-107" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">table</span> (exp_data<span class="sc">$</span>S, exp_data<span class="sc">$</span>C)</span>
<span id="cb412-108"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-108" aria-hidden="true" tabindex="-1"></a>mod_cat <span class="ot">=</span> <span class="fu">apply</span> (tab, <span class="dv">1</span>,which.max)</span>
<span id="cb412-109"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-110"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-110" aria-hidden="true" tabindex="-1"></a>aggd <span class="ot">=</span> <span class="fu">aggregate</span> (<span class="fu">cbind</span> ( height, A<span class="sc">==</span><span class="st">&quot;a&quot;</span>, G<span class="sc">==</span><span class="st">&quot;f&quot;</span>, vtl,f0, vtl) <span class="sc">~</span> S <span class="sc">+</span> C_v, </span>
<span id="cb412-111"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-111" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> exp_data, <span class="at">FUN =</span> mean)</span>
<span id="cb412-112"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-112" aria-hidden="true" tabindex="-1"></a>aggd<span class="sc">$</span>C_v <span class="ot">=</span> <span class="fu">factor</span>(aggd<span class="sc">$</span>C_v)</span>
<span id="cb412-113"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-114"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-114" aria-hidden="true" tabindex="-1"></a>cffs <span class="ot">=</span> gender_vtl_hypothesis[,<span class="dv">1</span>]</span>
<span id="cb412-115"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-116"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-116" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb412-117"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-118"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-118" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (aggd<span class="sc">$</span>vtl, <span class="fu">logit</span>(aggd[,<span class="dv">5</span>]), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,</span>
<span id="cb412-119"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-119" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;Logit (P(F==1))&quot;</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)][mod_cat],<span class="at">pch=</span><span class="dv">16</span>,</span>
<span id="cb412-120"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-120" aria-hidden="true" tabindex="-1"></a>      <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl))</span>
<span id="cb412-121"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-121" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb412-122"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-122" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-123"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-123" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb412-124"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-124" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">4</span>)</span>
<span id="cb412-125"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-125" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb412-126"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-126" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">10</span>], <span class="at">lwd=</span><span class="dv">4</span>, <span class="at">lty=</span><span class="dv">1</span>)</span>
<span id="cb412-127"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-127" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb412-128"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-128" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">14</span>], <span class="at">lwd=</span><span class="dv">4</span>, <span class="at">lty=</span><span class="dv">1</span>)</span>
<span id="cb412-129"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-130"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-130" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="fl">0.8</span>,<span class="fl">4.5</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb412-131"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-131" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">16</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-132"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-133"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-133" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (aggd<span class="sc">$</span>vtl, (aggd[,<span class="dv">5</span>]), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,</span>
<span id="cb412-134"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-134" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;P(F==1)&quot;</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)][mod_cat],</span>
<span id="cb412-135"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-135" aria-hidden="true" tabindex="-1"></a>      <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl))</span>
<span id="cb412-136"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-136" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb412-137"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-137" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span>.<span class="dv">50</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb412-138"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-139"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-139" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">inverse_logit</span>(cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl), </span>
<span id="cb412-140"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-140" aria-hidden="true" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">4</span>)</span>
<span id="cb412-141"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-141" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">inverse_logit</span>(cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl), </span>
<span id="cb412-142"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-142" aria-hidden="true" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">10</span>], <span class="at">lwd=</span><span class="dv">4</span>, <span class="at">lty=</span><span class="dv">1</span>)</span>
<span id="cb412-143"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-143" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">inverse_logit</span>(cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (exp_data<span class="sc">$</span>vtl), </span>
<span id="cb412-144"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-144" aria-hidden="true" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">14</span>], <span class="at">lwd=</span><span class="dv">4</span>, <span class="at">lty=</span><span class="dv">1</span>)</span>
<span id="cb412-145"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-146"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-146" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="dv">2</span>,<span class="sc">-</span><span class="dv">1</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb412-147"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-147" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-148"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-149"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-149" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">1</span>, <span class="at">text =</span> <span class="st">&quot;Centered VTL (cm)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="dv">1</span>, <span class="at">line=</span><span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb412-150"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-151"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-151" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="fl">0.8</span>,.<span class="dv">9</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Overall&quot;</span>,<span class="st">&quot;Adult&quot;</span>,<span class="st">&quot;Child&quot;</span>),<span class="at">lwd=</span><span class="dv">5</span>,</span>
<span id="cb412-152"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-152" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">1</span>,bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">14</span>,<span class="dv">10</span>)]), <span class="at">bty=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb412-153"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-154"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-155"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-155" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-156"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-156" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.5</span></span>
<span id="cb412-157"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-157" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-158"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-159"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-160"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-160" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;../wrong_plots/Figure 10.5.jpg&quot;</span>)</span>
<span id="cb412-161"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-162"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-162" aria-hidden="true" tabindex="-1"></a><span class="co"># jpeg (&quot;../wrong_plots/Figure 10.5.jpg&quot;,4800,1800,res=600)</span></span>
<span id="cb412-163"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-163" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-164"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-164" aria-hidden="true" tabindex="-1"></a><span class="co"># tab = table (exp_data$S, exp_data$C_v)</span></span>
<span id="cb412-165"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-165" aria-hidden="true" tabindex="-1"></a><span class="co"># mod_cat = apply (tab, 1,which.max)</span></span>
<span id="cb412-166"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-166" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-167"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-167" aria-hidden="true" tabindex="-1"></a><span class="co"># muvtl = round (mean(exp_data$vtl),1)</span></span>
<span id="cb412-168"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-168" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-169"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-169" aria-hidden="true" tabindex="-1"></a><span class="co"># aggd = aggregate (cbind ( height, A==&quot;a&quot;, G==&quot;f&quot;, vtl,f0, vtl) ~ S + C_v,</span></span>
<span id="cb412-170"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-170" aria-hidden="true" tabindex="-1"></a><span class="co">#                       data = exp_data, FUN = mean)</span></span>
<span id="cb412-171"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-171" aria-hidden="true" tabindex="-1"></a><span class="co"># aggd$C_v = factor(aggd$C_v)</span></span>
<span id="cb412-172"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-172" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-173"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-173" aria-hidden="true" tabindex="-1"></a><span class="co"># cffs = gender_vtl_hypothesis[,1]</span></span>
<span id="cb412-174"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-174" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-175"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-175" aria-hidden="true" tabindex="-1"></a><span class="co"># bounds = boundaries_1[,1]</span></span>
<span id="cb412-176"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-176" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-177"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-177" aria-hidden="true" tabindex="-1"></a><span class="co"># par (mfrow = c(1,2), mar = c(4.1,4.1,1,1))</span></span>
<span id="cb412-178"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-178" aria-hidden="true" tabindex="-1"></a><span class="co"># layout (mat = matrix(c(1,2,1,3,1,4,1,5),4,2,byrow=TRUE))</span></span>
<span id="cb412-179"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-179" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-180"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-180" aria-hidden="true" tabindex="-1"></a><span class="co"># plot (aggd$vtl, bmmb::logit(aggd[,5]), cex =2, ylim = c(-5,5),xlab=&quot;&quot;,</span></span>
<span id="cb412-181"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-181" aria-hidden="true" tabindex="-1"></a><span class="co">#       ylab = &quot;Logit (P(F==1))&quot;, col = bmmb::cols[c(2:5)][mod_cat],pch=16,</span></span>
<span id="cb412-182"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-182" aria-hidden="true" tabindex="-1"></a><span class="co">#       lwd=2, xlim =range (exp_data$vtl),cex.lab = 1.3,cex.axis=1.3,</span></span>
<span id="cb412-183"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-183" aria-hidden="true" tabindex="-1"></a><span class="co">#       xaxt = &#39;n&#39;)</span></span>
<span id="cb412-184"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-184" aria-hidden="true" tabindex="-1"></a><span class="co"># axis (at = -2:2, labels = (-2:2) + muvtl,</span></span>
<span id="cb412-185"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-185" aria-hidden="true" tabindex="-1"></a><span class="co">#       cex.axis = 1.3, side=1)</span></span>
<span id="cb412-186"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-186" aria-hidden="true" tabindex="-1"></a><span class="co"># abline (h=0,lty=3)</span></span>
<span id="cb412-187"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-187" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-188"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-188" aria-hidden="true" tabindex="-1"></a><span class="co"># curve ( (cffs[1] + cffs[4]*x), xlim =range (exp_data$vtl), add = TRUE,</span></span>
<span id="cb412-189"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-189" aria-hidden="true" tabindex="-1"></a><span class="co">#         col = 1, lwd=3)</span></span>
<span id="cb412-190"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-190" aria-hidden="true" tabindex="-1"></a><span class="co"># curve ( (cffs[3] + cffs[6]*x), xlim =range (exp_data$vtl), add = TRUE,</span></span>
<span id="cb412-191"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-191" aria-hidden="true" tabindex="-1"></a><span class="co">#         col = bmmb::cols[10], lwd=4, lty=1)</span></span>
<span id="cb412-192"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-192" aria-hidden="true" tabindex="-1"></a><span class="co"># curve ( (cffs[2] + cffs[5]*x), xlim =range (exp_data$vtl), add = TRUE,</span></span>
<span id="cb412-193"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-193" aria-hidden="true" tabindex="-1"></a><span class="co">#         col = bmmb::cols[14], lwd=4, lty=1)</span></span>
<span id="cb412-194"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-194" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-195"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-195" aria-hidden="true" tabindex="-1"></a><span class="co"># legend (0.8,4.7, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0,</span></span>
<span id="cb412-196"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-196" aria-hidden="true" tabindex="-1"></a><span class="co">#         col = bmmb::cols[2:5], bty=&#39;n&#39;,pch=16,pt.cex=1.5,cex=1.5)</span></span>
<span id="cb412-197"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-197" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-198"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-198" aria-hidden="true" tabindex="-1"></a><span class="co"># abline (v = c(0.24,.655,-.99), col = c(1,bmmb::cols[14],bmmb::cols[10]),</span></span>
<span id="cb412-199"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-199" aria-hidden="true" tabindex="-1"></a><span class="co">#         lwd=2,lty=3)</span></span>
<span id="cb412-200"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-200" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-201"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-201" aria-hidden="true" tabindex="-1"></a><span class="co"># par (mar = c(.5,.5,.5,.5))</span></span>
<span id="cb412-202"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-202" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-203"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-203" aria-hidden="true" tabindex="-1"></a><span class="co"># bound = bounds[3]</span></span>
<span id="cb412-204"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-204" aria-hidden="true" tabindex="-1"></a><span class="co"># plot (0,xlim = c(-2,2),ylim=c(0,1),xaxt=&#39;n&#39;,yaxt=&#39;n&#39;,type=&#39;n&#39;)</span></span>
<span id="cb412-205"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-205" aria-hidden="true" tabindex="-1"></a><span class="co"># rect(-3, -1, bound, 2, col=bmmb::cols[3])</span></span>
<span id="cb412-206"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-206" aria-hidden="true" tabindex="-1"></a><span class="co"># rect(bound, -1, 3, 2, col=bmmb::cols[2])</span></span>
<span id="cb412-207"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-207" aria-hidden="true" tabindex="-1"></a><span class="co"># text (c(-1.5,1.2),c(0.5,0.5), c(&quot;Girl&quot;,&quot;Boy&quot;), cex = 2, col = 0)</span></span>
<span id="cb412-208"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-208" aria-hidden="true" tabindex="-1"></a><span class="co"># abline (v = bounds[3], lwd = 4, col = bmmb::cols[10])</span></span>
<span id="cb412-209"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-209" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-210"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-210" aria-hidden="true" tabindex="-1"></a><span class="co"># bound = bounds[1]</span></span>
<span id="cb412-211"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-211" aria-hidden="true" tabindex="-1"></a><span class="co"># plot (0,xlim = c(-2,2),ylim=c(0,1),xaxt=&#39;n&#39;,yaxt=&#39;n&#39;,type=&#39;n&#39;)</span></span>
<span id="cb412-212"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-212" aria-hidden="true" tabindex="-1"></a><span class="co"># rect(-3, -1, bound, 2, col=bmmb::cols[8])</span></span>
<span id="cb412-213"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-213" aria-hidden="true" tabindex="-1"></a><span class="co"># rect(bound, -1, 3, 2, col=bmmb::cols[7])</span></span>
<span id="cb412-214"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-214" aria-hidden="true" tabindex="-1"></a><span class="co"># text (c(-1.5,1.2),c(0.5,0.5), c(&quot;Female&quot;,&quot;Male&quot;), cex = 2, col = 0)</span></span>
<span id="cb412-215"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-215" aria-hidden="true" tabindex="-1"></a><span class="co"># abline (v = bounds[1], lwd = 3, col = 1)</span></span>
<span id="cb412-216"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-216" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-217"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-217" aria-hidden="true" tabindex="-1"></a><span class="co"># bound = bounds[2]</span></span>
<span id="cb412-218"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-218" aria-hidden="true" tabindex="-1"></a><span class="co"># plot (0,xlim = c(-2,2),ylim=c(0,1),yaxt=&#39;n&#39;,type=&#39;n&#39;,</span></span>
<span id="cb412-219"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-219" aria-hidden="true" tabindex="-1"></a><span class="co">#       cex.axis = 1.3, xaxt = &#39;n&#39;)</span></span>
<span id="cb412-220"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-220" aria-hidden="true" tabindex="-1"></a><span class="co"># rect(-3, -1, bound, 2, col=bmmb::cols[5])</span></span>
<span id="cb412-221"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-221" aria-hidden="true" tabindex="-1"></a><span class="co"># rect(bound, -1, 3, 2, col=bmmb::cols[4])</span></span>
<span id="cb412-222"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-222" aria-hidden="true" tabindex="-1"></a><span class="co"># text (c(-1.5,1.2),c(0.5,0.5), c(&quot;Woman&quot;,&quot;Man&quot;), cex = 2, col = 0)</span></span>
<span id="cb412-223"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-223" aria-hidden="true" tabindex="-1"></a><span class="co"># axis (at = -2:2, labels = (-2:2) + muvtl,</span></span>
<span id="cb412-224"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-224" aria-hidden="true" tabindex="-1"></a><span class="co">#       cex.axis = 1.3, side=1)</span></span>
<span id="cb412-225"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-225" aria-hidden="true" tabindex="-1"></a><span class="co"># abline (v = bounds[2], lwd = 4, col = bmmb::cols[14])</span></span>
<span id="cb412-226"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-226" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-227"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-227" aria-hidden="true" tabindex="-1"></a><span class="co"># mtext (side=1, &quot;Centered vocal-tract length (cm)&quot;, line = 3)</span></span>
<span id="cb412-228"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-228" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb412-229"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-229" aria-hidden="true" tabindex="-1"></a><span class="co"># dev.off()</span></span>
<span id="cb412-230"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-231"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-231" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-232"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-232" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.6</span></span>
<span id="cb412-233"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-233" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-234"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-235"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-236"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-236" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb412-237"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-238"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-238" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">=</span> (<span class="fu">tapply</span> (exp_data<span class="sc">$</span>Female, exp_data<span class="sc">$</span>G_v, mean))</span>
<span id="cb412-239"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-239" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">=</span> (<span class="fu">tapply</span> (adults<span class="sc">$</span>Female, adults<span class="sc">$</span>G_v, mean))</span>
<span id="cb412-240"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-240" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">=</span> (<span class="fu">tapply</span> (children<span class="sc">$</span>Female, children<span class="sc">$</span>G_v, mean))</span>
<span id="cb412-241"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-242"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-242" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), p1, <span class="at">type =</span> <span class="st">&#39;b&#39;</span>, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">0.8</span>,<span class="fl">3.2</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb412-243"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-243" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span> <span class="st">&quot;P(F==1)&quot;</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>,<span class="at">xlab=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb412-244"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-244" aria-hidden="true" tabindex="-1"></a><span class="co">#points (1, mean(p1), cex=1.5,pch=16)</span></span>
<span id="cb412-245"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-245" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), p1, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-246"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-246" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), p2, <span class="at">type =</span> <span class="st">&#39;b&#39;</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-247"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-247" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), p2, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-248"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-248" aria-hidden="true" tabindex="-1"></a><span class="co">#points (2, mean(p2), cex=1.5,pch=16)</span></span>
<span id="cb412-249"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-249" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), p3, <span class="at">type =</span> <span class="st">&#39;b&#39;</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-250"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-250" aria-hidden="true" tabindex="-1"></a><span class="co">#points (3, mean(p3), cex=1.5,pch=16)</span></span>
<span id="cb412-251"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-251" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), p3, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-252"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-252" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h =</span> <span class="fl">0.5</span>)</span>
<span id="cb412-253"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-253" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span> (<span class="at">side=</span><span class="dv">1</span>,<span class="at">at=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;All&quot;</span>,<span class="st">&quot;Adults&quot;</span>,<span class="st">&quot;Chidren&quot;</span>))</span>
<span id="cb412-254"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-255"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-255" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="fl">2.3</span>, <span class="fl">0.9</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;H&quot;</span>,<span class="st">&quot;FA&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>),<span class="at">pch=</span><span class="dv">16</span>,<span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pt.cex=</span><span class="fl">1.3</span>)</span>
<span id="cb412-256"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-257"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-257" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">logit</span> (<span class="fu">tapply</span> (exp_data<span class="sc">$</span>Female, exp_data<span class="sc">$</span>G_v, mean))</span>
<span id="cb412-258"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-258" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">logit</span> (<span class="fu">tapply</span> (adults<span class="sc">$</span>Female, adults<span class="sc">$</span>G_v, mean))</span>
<span id="cb412-259"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-259" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">logit</span> (<span class="fu">tapply</span> (children<span class="sc">$</span>Female, children<span class="sc">$</span>G_v, mean))</span>
<span id="cb412-260"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-261"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-261" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), p1, <span class="at">type =</span> <span class="st">&#39;b&#39;</span>, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">0.8</span>,<span class="fl">3.2</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="fl">2.5</span>),</span>
<span id="cb412-262"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-262" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span> <span class="st">&quot;Logit (P(F==1))&quot;</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>,<span class="at">xlab=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb412-263"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-263" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="dv">1</span>, <span class="fu">mean</span>(p1), <span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-264"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-264" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), p1, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-265"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-265" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), p2, <span class="at">type =</span> <span class="st">&#39;b&#39;</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-266"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-266" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="dv">2</span>, <span class="fu">mean</span>(p2), <span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-267"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-267" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), p2, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-268"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-268" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), p3, <span class="at">type =</span> <span class="st">&#39;b&#39;</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-269"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-269" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="dv">3</span>, <span class="fu">mean</span>(p3), <span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb412-270"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-270" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), p3, <span class="at">col=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb412-271"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-271" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span> (<span class="at">side=</span><span class="dv">1</span>,<span class="at">at=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;All&quot;</span>,<span class="st">&quot;Adults&quot;</span>,<span class="st">&quot;Chidren&quot;</span>))</span>
<span id="cb412-272"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-273"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-273" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h =</span> <span class="dv">0</span>)</span>
<span id="cb412-274"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-275"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-275" aria-hidden="true" tabindex="-1"></a><span class="do">#############################################################################</span></span>
<span id="cb412-276"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-276" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.7</span></span>
<span id="cb412-277"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-277" aria-hidden="true" tabindex="-1"></a><span class="do">#############################################################################</span></span>
<span id="cb412-278"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-279"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-279" aria-hidden="true" tabindex="-1"></a>biases1 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb412-280"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-280" aria-hidden="true" tabindex="-1"></a>  model_gender_dt,</span>
<span id="cb412-281"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-281" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;Intercept+A_v1 = 0&quot;</span>),<span class="at">group=</span><span class="st">&quot;L&quot;</span>, <span class="at">scope=</span><span class="st">&quot;coef&quot;</span>)</span>
<span id="cb412-282"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-282" aria-hidden="true" tabindex="-1"></a>biases2 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb412-283"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-283" aria-hidden="true" tabindex="-1"></a>  model_gender_dt,</span>
<span id="cb412-284"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-284" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;Intercept-A_v1 = 0&quot;</span>),<span class="at">group=</span><span class="st">&quot;L&quot;</span>, <span class="at">scope=</span><span class="st">&quot;coef&quot;</span>)</span>
<span id="cb412-285"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-286"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-286" aria-hidden="true" tabindex="-1"></a>sensitivities1 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb412-287"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-287" aria-hidden="true" tabindex="-1"></a>  model_gender_dt,</span>
<span id="cb412-288"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-288" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;2*(F_v+F_v:A_v1) = 0&quot;</span>),<span class="at">group=</span><span class="st">&quot;L&quot;</span>, <span class="at">scope=</span><span class="st">&quot;coef&quot;</span>)</span>
<span id="cb412-289"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-289" aria-hidden="true" tabindex="-1"></a>sensitivities2 <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">short_hypothesis</span> (</span>
<span id="cb412-290"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-290" aria-hidden="true" tabindex="-1"></a>  model_gender_dt,</span>
<span id="cb412-291"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-291" aria-hidden="true" tabindex="-1"></a>  <span class="at">hypothesis =</span> <span class="fu">c</span>(<span class="st">&quot;2*(F_v-F_v:A_v1) = 0&quot;</span>),<span class="at">group=</span><span class="st">&quot;L&quot;</span>, <span class="at">scope=</span><span class="st">&quot;coef&quot;</span>)</span>
<span id="cb412-292"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-293"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-294"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-294" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="fl">4.2</span>,<span class="dv">1</span>,.<span class="dv">2</span>))</span>
<span id="cb412-295"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-295" aria-hidden="true" tabindex="-1"></a><span class="fu">layout</span> (<span class="at">m=</span><span class="fu">t</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)), <span class="at">widths =</span> <span class="fu">c</span>(.<span class="dv">30</span>,.<span class="dv">4</span>,.<span class="dv">4</span>))</span>
<span id="cb412-296"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-296" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">brmplot</span> (gender_dt_hypothesis[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,], <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">13</span>), <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">7</span>],</span>
<span id="cb412-297"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-297" aria-hidden="true" tabindex="-1"></a>               <span class="at">nudge =</span> <span class="sc">-</span>.<span class="dv">01</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Logits&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-298"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-298" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">brmplot</span> (gender_dt_hypothesis[<span class="dv">4</span><span class="sc">:</span><span class="dv">6</span>,], <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">8</span>], </span>
<span id="cb412-299"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-299" aria-hidden="true" tabindex="-1"></a>               <span class="at">nudge =</span> .<span class="dv">01</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb412-300"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-300" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span> (<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;All&quot;</span>,<span class="st">&quot;Adults&quot;</span>,<span class="st">&quot;Children&quot;</span>),<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-301"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-302"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-302" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,.<span class="dv">1</span>,<span class="dv">1</span>,.<span class="dv">2</span>))</span>
<span id="cb412-303"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-303" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">brmplot</span> (biases1, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">13</span>), <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">7</span>],<span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb412-304"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-304" aria-hidden="true" tabindex="-1"></a>               <span class="at">labels=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, <span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-305"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-305" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">brmplot</span> (sensitivities1, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">8</span>], <span class="at">labels=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb412-306"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-307"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-307" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,.<span class="dv">1</span>,<span class="dv">1</span>,.<span class="dv">2</span>))</span>
<span id="cb412-308"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-308" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">brmplot</span> (biases2, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">13</span>), <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">7</span>],<span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb412-309"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-309" aria-hidden="true" tabindex="-1"></a>               <span class="at">labels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>)</span>
<span id="cb412-310"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-310" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">brmplot</span> (sensitivities2, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="dv">8</span>], <span class="at">pch=</span><span class="dv">16</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb412-311"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-312"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-312" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="dv">5</span>, <span class="dv">11</span>, <span class="at">legend =</span><span class="fu">c</span>(<span class="st">&quot;Sensitivity&quot;</span>, <span class="st">&quot;Bias&quot;</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb412-313"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-313" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> bmmb<span class="sc">::</span>cols[<span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">7</span>)], <span class="at">cex =</span> <span class="fl">1.5</span>, <span class="at">bty=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb412-314"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-315"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-315" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-316"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-316" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 10.8</span></span>
<span id="cb412-317"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-317" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb412-318"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-319"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-319" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">=</span> bmmb<span class="sc">::</span>exp_data</span>
<span id="cb412-320"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-320" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb412-321"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-322"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-322" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (<span class="fu">density</span> (tmp<span class="sc">$</span>vtl[tmp<span class="sc">$</span>C_v<span class="sc">==</span><span class="st">&quot;b&quot;</span>]), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">11</span>,<span class="dv">16</span>),</span>
<span id="cb412-323"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-323" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">1.3</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span>cols[<span class="dv">2</span>],<span class="at">main=</span><span class="st">&quot;&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;Vocal-Tract Length (cm)&quot;</span>)</span>
<span id="cb412-324"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-324" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span> (<span class="fu">density</span> (tmp<span class="sc">$</span>vtl[tmp<span class="sc">$</span>C_v<span class="sc">==</span><span class="st">&quot;b&quot;</span>]),</span>
<span id="cb412-325"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-325" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span>cols[<span class="dv">2</span>])</span>
<span id="cb412-326"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-326" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span> (<span class="fu">density</span> (tmp<span class="sc">$</span>vtl[tmp<span class="sc">$</span>C_v<span class="sc">==</span><span class="st">&quot;g&quot;</span>]),</span>
<span id="cb412-327"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-327" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span>cols[<span class="dv">3</span>])</span>
<span id="cb412-328"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-328" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span> (<span class="fu">density</span> (tmp<span class="sc">$</span>vtl[tmp<span class="sc">$</span>C_v<span class="sc">==</span><span class="st">&quot;w&quot;</span>]),</span>
<span id="cb412-329"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-329" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span>cols[<span class="dv">5</span>])</span>
<span id="cb412-330"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-330" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span> (<span class="fu">density</span> (tmp<span class="sc">$</span>vtl[tmp<span class="sc">$</span>C_v<span class="sc">==</span><span class="st">&quot;m&quot;</span>]),</span>
<span id="cb412-331"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-331" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span>cols[<span class="dv">4</span>])</span>
<span id="cb412-332"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-333"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-333" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="fl">13.7</span>,<span class="fl">1.2</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;boys&quot;</span>,<span class="st">&quot;girls&quot;</span>,<span class="st">&quot;men&quot;</span>,<span class="st">&quot;women&quot;</span>),<span class="at">col=</span>bmmb<span class="sc">::</span>cols[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>],</span>
<span id="cb412-334"><a href="logistic-regression-and-signal-detection-theory-models.html#cb412-334" aria-hidden="true" tabindex="-1"></a>        <span class="at">pch=</span><span class="dv">16</span>,<span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">cex=</span><span class="fl">1.0</span>)</span></code></pre></div>

<div style="page-break-after: always;"></div>
</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="quantitative-predictors-and-their-interactions-with-factors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
