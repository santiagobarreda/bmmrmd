<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Probabilities, likelihood, and inference | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</title>
  <meta name="description" content="Bayesian Models for Repeated Measures" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Probabilities, likelihood, and inference | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Bayesian Models for Repeated Measures" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Probabilities, likelihood, and inference | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  
  <meta name="twitter:description" content="Bayesian Models for Repeated Measures" />
  

<meta name="author" content="Santiago Bareda and Noah Silbert" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-experiments-and-variables.html"/>
<link rel="next" href="fitting-bayesian-regression-models-with-brms.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.amazon.com/Bayesian-Multilevel-Models-Repeated-Measures/dp/1032259639">Bayesian Repeated Measures data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bayesian-multilevel-models-and-repeated-measures-data"><i class="fa fa-check"></i>Bayesian Multilevel models and repeated measures data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-missing-from-this-book"><i class="fa fa-check"></i>What’s missing from this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#statistics-as-procedural-knowledge"><i class="fa fa-check"></i>Statistics as Procedural knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practice-vs-brain-power"><i class="fa fa-check"></i>Practice vs brain power</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#supplemental-resources"><i class="fa fa-check"></i>Supplemental Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#our-target-audience"><i class="fa fa-check"></i>Our target audience</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-self-starter"><i class="fa fa-check"></i>The self-starter</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-convert"><i class="fa fa-check"></i>The convert</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-instructor"><i class="fa fa-check"></i>The instructor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-you-need-installed-to-use-this-book"><i class="fa fa-check"></i>What you need installed to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-go-bayesian"><i class="fa fa-check"></i>Why go Bayesian?</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-brms"><i class="fa fa-check"></i>Why brms?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#it-takes-a-village-of-books"><i class="fa fa-check"></i>It takes a village (of books)</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html"><i class="fa fa-check"></i><b>1</b> Introduction: Experiments and Variables</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#chapter-pre-cap"><i class="fa fa-check"></i><b>1.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-and-effects"><i class="fa fa-check"></i><b>1.2</b> Experiments and effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-and-inference"><i class="fa fa-check"></i><b>1.2.1</b> Experiments and inference</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp"><i class="fa fa-check"></i><b>1.3</b> Our experiment</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-intro"><i class="fa fa-check"></i><b>1.3.1</b> Our experiment: Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-methods"><i class="fa fa-check"></i><b>1.3.2</b> Our experimental methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-research-questions"><i class="fa fa-check"></i><b>1.3.3</b> Our research questions</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-data"><i class="fa fa-check"></i><b>1.3.4</b> Our experimental data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-variables"><i class="fa fa-check"></i><b>1.4</b> Variables</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-pops-and-samps"><i class="fa fa-check"></i><b>1.4.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-dep-and-indep"><i class="fa fa-check"></i><b>1.4.2</b> Dependent and Independent Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-categorical"><i class="fa fa-check"></i><b>1.4.3</b> Categorical variables and ‘factors’</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-quantitative"><i class="fa fa-check"></i><b>1.4.4</b> Quantitative variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-logical"><i class="fa fa-check"></i><b>1.4.5</b> Logical variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting"><i class="fa fa-check"></i><b>1.5</b> Inspecting our data</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-categorical"><i class="fa fa-check"></i><b>1.5.1</b> Inspecting categorical variables</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-quantitative"><i class="fa fa-check"></i><b>1.5.2</b> Inspecting quantitative variables</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-together"><i class="fa fa-check"></i><b>1.5.3</b> Exploring continuous and categorical variables together</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
<li class="chapter" data-level="1.7" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#references"><i class="fa fa-check"></i><b>1.7</b> References</a></li>
<li class="chapter" data-level="1.8" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#plot-code"><i class="fa fa-check"></i><b>1.8</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html"><i class="fa fa-check"></i><b>2</b> Probabilities, likelihood, and inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#chapter-pre-cap-1"><i class="fa fa-check"></i><b>2.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="2.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-data"><i class="fa fa-check"></i><b>2.2</b> Data and research questions</a></li>
<li class="chapter" data-level="2.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-empirical-prob"><i class="fa fa-check"></i><b>2.3</b> Empirical Probabilities</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-conditional"><i class="fa fa-check"></i><b>2.3.1</b> Conditional and marginal probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-joint"><i class="fa fa-check"></i><b>2.3.2</b> Joint probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-theoretical"><i class="fa fa-check"></i><b>2.4</b> Probability distributions</a></li>
<li class="chapter" data-level="2.5" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-normal"><i class="fa fa-check"></i><b>2.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-sample-mean"><i class="fa fa-check"></i><b>2.5.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.5.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-sample-variance"><i class="fa fa-check"></i><b>2.5.2</b> The sample variance (or standard deviation)</a></li>
<li class="chapter" data-level="2.5.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-normal-density"><i class="fa fa-check"></i><b>2.5.3</b> The normal density</a></li>
<li class="chapter" data-level="2.5.4" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-standard-normal"><i class="fa fa-check"></i><b>2.5.4</b> The standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-models-and-inference"><i class="fa fa-check"></i><b>2.6</b> Models and inference</a></li>
<li class="chapter" data-level="2.7" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-likelihoods"><i class="fa fa-check"></i><b>2.7</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods"><i class="fa fa-check"></i><b>2.7.1</b> Characteristics of likelihoods</a></li>
<li class="chapter" data-level="2.7.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-logarithms"><i class="fa fa-check"></i><b>2.7.2</b> A brief aside on logarithms</a></li>
<li class="chapter" data-level="2.7.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods-2"><i class="fa fa-check"></i><b>2.7.3</b> Characteristics of likelihoods, continued</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-inference-and-likelihood"><i class="fa fa-check"></i><b>2.8</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.9" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#exercises-1"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
<li class="chapter" data-level="2.10" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#references-1"><i class="fa fa-check"></i><b>2.10</b> References</a></li>
<li class="chapter" data-level="2.11" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#plot-code-1"><i class="fa fa-check"></i><b>2.11</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html"><i class="fa fa-check"></i><b>3</b> Fitting Bayesian regression models with <em>brms</em></a>
<ul>
<li class="chapter" data-level="3.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#chapter-pre-cap-2"><i class="fa fa-check"></i><b>3.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="3.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-what-is-reg"><i class="fa fa-check"></i><b>3.2</b> What are regression models?</a></li>
<li class="chapter" data-level="3.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-whats-bayes"><i class="fa fa-check"></i><b>3.3</b> What’s ‘Bayesian’ about these models?</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-priors"><i class="fa fa-check"></i><b>3.3.1</b> Prior probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-posterior"><i class="fa fa-check"></i><b>3.3.2</b> Posterior distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-characteristics-posteriors"><i class="fa fa-check"></i><b>3.3.3</b> Posterior distributions and shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-sampling"><i class="fa fa-check"></i><b>3.4</b> Sampling from the posterior using <em>Stan</em> and <em>brms</em></a></li>
<li class="chapter" data-level="3.5" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-estimating"><i class="fa fa-check"></i><b>3.5</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-data-qs-1"><i class="fa fa-check"></i><b>3.5.1</b> Data and Research Questions</a></li>
<li class="chapter" data-level="3.5.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-description-1"><i class="fa fa-check"></i><b>3.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.5.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-errors-and-residuals"><i class="fa fa-check"></i><b>3.5.3</b> Errors and residuals</a></li>
<li class="chapter" data-level="3.5.4" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-model-formula"><i class="fa fa-check"></i><b>3.5.4</b> The model formula</a></li>
<li class="chapter" data-level="3.5.5" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-calling-brm"><i class="fa fa-check"></i><b>3.5.5</b> Fitting the model: Calling the <em>brm</em> function</a></li>
<li class="chapter" data-level="3.5.6" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-interpreting-print"><i class="fa fa-check"></i><b>3.5.6</b> Interpreting the model: The print statement</a></li>
<li class="chapter" data-level="3.5.7" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-seeing-samples"><i class="fa fa-check"></i><b>3.5.7</b> Seeing the samples</a></li>
<li class="chapter" data-level="3.5.8" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-getting-residuals"><i class="fa fa-check"></i><b>3.5.8</b> Getting the residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-checking-convergence"><i class="fa fa-check"></i><b>3.6</b> Checking model convergence</a></li>
<li class="chapter" data-level="3.7" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-specifying-priors"><i class="fa fa-check"></i><b>3.7</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="3.8" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-log-posterior"><i class="fa fa-check"></i><b>3.8</b> The log prior and log posterior densities</a></li>
<li class="chapter" data-level="3.9" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-answering-qs"><i class="fa fa-check"></i><b>3.9</b> Answering our research questions</a></li>
<li class="chapter" data-level="3.10" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-frequentist"><i class="fa fa-check"></i><b>3.10</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-vs-ttest"><i class="fa fa-check"></i><b>3.10.1</b> One-sample t-test vs. intercept-only Bayesian models</a></li>
<li class="chapter" data-level="3.10.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-vs-ols"><i class="fa fa-check"></i><b>3.10.2</b> Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#exercises-2"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
<li class="chapter" data-level="3.12" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#plot-code-2"><i class="fa fa-check"></i><b>3.12</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><i class="fa fa-check"></i><b>4</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#chapter-pre-cap-3"><i class="fa fa-check"></i><b>4.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-multilevel"><i class="fa fa-check"></i><b>4.2</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-levels"><i class="fa fa-check"></i><b>4.2.1</b> Multilevel models and ‘levels’ of variation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-many-levels"><i class="fa fa-check"></i><b>4.3</b> Representing predictors with many levels</a></li>
<li class="chapter" data-level="4.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-strategies"><i class="fa fa-check"></i><b>4.4</b> Strategies for estimating factors with many levels</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-complete-pooling"><i class="fa fa-check"></i><b>4.4.1</b> Complete pooling</a></li>
<li class="chapter" data-level="4.4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-no-pooling"><i class="fa fa-check"></i><b>4.4.2</b> No pooling</a></li>
<li class="chapter" data-level="4.4.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-partial-pooling"><i class="fa fa-check"></i><b>4.4.3</b> (Adaptive) Partial pooling</a></li>
<li class="chapter" data-level="4.4.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#hyperpriors"><i class="fa fa-check"></i><b>4.4.4</b> Hyperpriors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-estimating1"><i class="fa fa-check"></i><b>4.5</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-data-and-qs-1"><i class="fa fa-check"></i><b>4.5.1</b> Data and Research questions</a></li>
<li class="chapter" data-level="4.5.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model"><i class="fa fa-check"></i><b>4.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="4.5.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-fitting-1"><i class="fa fa-check"></i><b>4.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="4.5.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#interpreting-the-model"><i class="fa fa-check"></i><b>4.5.4</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-random-effects"><i class="fa fa-check"></i><b>4.6</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-inspecting-random-effects"><i class="fa fa-check"></i><b>4.6.1</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-simulating"><i class="fa fa-check"></i><b>4.7</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="4.8" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-second-random-effect"><i class="fa fa-check"></i><b>4.8</b> Adding a second random effect</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-updating-model"><i class="fa fa-check"></i><b>4.8.1</b> Updating the model description</a></li>
<li class="chapter" data-level="4.8.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-and-interpreting-the-model"><i class="fa fa-check"></i><b>4.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-investigating-shrinkage"><i class="fa fa-check"></i><b>4.9</b> Investigating ‘shrinkage’</a></li>
<li class="chapter" data-level="4.10" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-answering-question"><i class="fa fa-check"></i><b>4.10</b> Answering our research questions</a></li>
<li class="chapter" data-level="4.11" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-frequentist"><i class="fa fa-check"></i><b>4.11</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-vs-lmer"><i class="fa fa-check"></i><b>4.11.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#exercises-3"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
<li class="chapter" data-level="4.13" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#references-2"><i class="fa fa-check"></i><b>4.13</b> References</a></li>
<li class="chapter" data-level="4.14" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#plot-code-3"><i class="fa fa-check"></i><b>4.14</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html"><i class="fa fa-check"></i><b>5</b> Comparing two groups of observations: Factors and contrasts</a>
<ul>
<li class="chapter" data-level="5.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#chapter-pre-cap-4"><i class="fa fa-check"></i><b>5.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="5.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#comparing-two-groups"><i class="fa fa-check"></i><b>5.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="5.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#distribution-of-repeated-measures-across-factor-levels"><i class="fa fa-check"></i><b>5.3</b> Distribution of repeated measures across factor levels</a></li>
<li class="chapter" data-level="5.4" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-data-and-qs"><i class="fa fa-check"></i><b>5.4</b> Data and research questions</a></li>
<li class="chapter" data-level="5.5" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-two-means"><i class="fa fa-check"></i><b>5.5</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-the-model"><i class="fa fa-check"></i><b>5.5.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>5.5.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-contrasts"><i class="fa fa-check"></i><b>5.6</b> Contrasts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-treatment-coding"><i class="fa fa-check"></i><b>5.6.1</b> Treatment coding</a></li>
<li class="chapter" data-level="5.6.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-sum-coding"><i class="fa fa-check"></i><b>5.6.2</b> Sum coding</a></li>
<li class="chapter" data-level="5.6.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-comparison-sum-treatment"><i class="fa fa-check"></i><b>5.6.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-refittin-sum"><i class="fa fa-check"></i><b>5.7</b> Sum coding and the decomposition of variation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-description-1"><i class="fa fa-check"></i><b>5.7.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.7.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-the-model-1"><i class="fa fa-check"></i><b>5.7.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.7.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>5.7.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-working-with-posteriors"><i class="fa fa-check"></i><b>5.8</b> Inspecting and manipulating the posterior samples</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-using-hypothesis"><i class="fa fa-check"></i><b>5.8.1</b> Using the <em>hypothesis</em> function</a></li>
<li class="chapter" data-level="5.8.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-manipulating-random-effects"><i class="fa fa-check"></i><b>5.8.2</b> Working with the random effects</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-robustness"><i class="fa fa-check"></i><b>5.9</b> Making our models more robust: The (non-standardized) t distribution</a></li>
<li class="chapter" data-level="5.10" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#re-fitting-with-t-distributed-errors."><i class="fa fa-check"></i><b>5.10</b> Re-fitting with t-distributed errors</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#description-of-the-model-1"><i class="fa fa-check"></i><b>5.10.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.10.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-and-interpreting-the-model-1"><i class="fa fa-check"></i><b>5.10.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-simulating"><i class="fa fa-check"></i><b>5.11</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="5.12" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-answering-qs"><i class="fa fa-check"></i><b>5.12</b> Answering our research questions</a></li>
<li class="chapter" data-level="5.13" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-frequentist"><i class="fa fa-check"></i><b>5.13</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#bayesian-multilevel-models-vs.-lmer"><i class="fa fa-check"></i><b>5.13.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#exercises-4"><i class="fa fa-check"></i><b>5.14</b> Exercises</a></li>
<li class="chapter" data-level="5.15" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#plot-code-4"><i class="fa fa-check"></i><b>5.15</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html"><i class="fa fa-check"></i><b>6</b> Variation in parameters (‘random effects’) and model comparison</a>
<ul>
<li class="chapter" data-level="6.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#chapter-pre-cap-5"><i class="fa fa-check"></i><b>6.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="6.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-data-and-qs"><i class="fa fa-check"></i><b>6.2</b> Data and research questions</a></li>
<li class="chapter" data-level="6.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-variation-sources"><i class="fa fa-check"></i><b>6.3</b> Variation in parameters across sources of data</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#description-of-our-model"><i class="fa fa-check"></i><b>6.3.1</b> Description of our model</a></li>
<li class="chapter" data-level="6.3.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-correlations"><i class="fa fa-check"></i><b>6.3.2</b> Correlations between random parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-random-and-mvn"><i class="fa fa-check"></i><b>6.3.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-mvn-priors"><i class="fa fa-check"></i><b>6.3.4</b> Specifying priors for a multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.5" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#updating-our-model-description"><i class="fa fa-check"></i><b>6.3.5</b> Updating our model description</a></li>
<li class="chapter" data-level="6.3.6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-fitting"><i class="fa fa-check"></i><b>6.3.6</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-model-comparison"><i class="fa fa-check"></i><b>6.4</b> Model Comparison</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-in-and-out-prediction"><i class="fa fa-check"></i><b>6.4.1</b> In-sample and out-of-sample prediction</a></li>
<li class="chapter" data-level="6.4.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-adjust"><i class="fa fa-check"></i><b>6.4.2</b> Out-of-sample prediction: Adjusting predictive accuracy</a></li>
<li class="chapter" data-level="6.4.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-crossval"><i class="fa fa-check"></i><b>6.4.3</b> Out-of-sample prediction: Cross validation</a></li>
<li class="chapter" data-level="6.4.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#selecting-a-model"><i class="fa fa-check"></i><b>6.4.4</b> Selecting a model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-answering"><i class="fa fa-check"></i><b>6.5</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-frequentist"><i class="fa fa-check"></i><b>6.6</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-vs-lmer"><i class="fa fa-check"></i><b>6.6.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#references-3"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
<li class="chapter" data-level="6.9" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#plot-code-5"><i class="fa fa-check"></i><b>6.9</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><i class="fa fa-check"></i><b>7</b> Comparing many groups, interactions, and posterior predictive checks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#chapter-pre-cap-6"><i class="fa fa-check"></i><b>7.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="7.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions"><i class="fa fa-check"></i><b>7.2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.2.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-description-1"><i class="fa fa-check"></i><b>7.2.2</b> Description of our model</a></li>
<li class="chapter" data-level="7.2.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-2"><i class="fa fa-check"></i><b>7.2.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-multiple-factors-simultaneously"><i class="fa fa-check"></i><b>7.3</b> Investigating multiple factors simultaneously</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>7.3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.3.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-the-model-2"><i class="fa fa-check"></i><b>7.3.2</b> Description of the model</a></li>
<li class="chapter" data-level="7.3.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-3"><i class="fa fa-check"></i><b>7.3.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-posterior-prediction"><i class="fa fa-check"></i><b>7.4</b> Posterior prediction: Using our models to predict new data</a></li>
<li class="chapter" data-level="7.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-interactions-and-plots"><i class="fa fa-check"></i><b>7.5</b> Interactions and interaction plots</a></li>
<li class="chapter" data-level="7.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>7.6</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>7.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.6.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#model-formulas"><i class="fa fa-check"></i><b>7.6.2</b> Model formulas</a></li>
<li class="chapter" data-level="7.6.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-our-model-1"><i class="fa fa-check"></i><b>7.6.3</b> Description of our model</a></li>
<li class="chapter" data-level="7.6.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-4"><i class="fa fa-check"></i><b>7.6.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="7.6.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-calc-means"><i class="fa fa-check"></i><b>7.6.5</b> Caulculating group means in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#calculating-simple-effects-in-the-presence-of-interactions"><i class="fa fa-check"></i><b>7.6.6</b> Calculating simple effects in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#assessing-model-fit-bayesian-r2"><i class="fa fa-check"></i><b>7.6.7</b> Assessing model fit: Bayesian <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-answering"><i class="fa fa-check"></i><b>7.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="7.8" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.8</b> Factors with more than two levels</a></li>
<li class="chapter" data-level="7.9" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-frequentist"><i class="fa fa-check"></i><b>7.9</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#bayesian-multilevel-models-vs.-lmer-1"><i class="fa fa-check"></i><b>7.9.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#exercises-6"><i class="fa fa-check"></i><b>7.10</b> Exercises</a></li>
<li class="chapter" data-level="7.11" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#references-4"><i class="fa fa-check"></i><b>7.11</b> References</a></li>
<li class="chapter" data-level="7.12" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#plot-code-6"><i class="fa fa-check"></i><b>7.12</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html"><i class="fa fa-check"></i><b>8</b> Varying variances, more about priors, and prior predictive checks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#chapter-pre-cap-7"><i class="fa fa-check"></i><b>8.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>8.2</b> Data and Research questions</a></li>
<li class="chapter" data-level="8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-about-priors"><i class="fa fa-check"></i><b>8.3</b> More about priors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-prior-prediction"><i class="fa fa-check"></i><b>8.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.3.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-specific-priors"><i class="fa fa-check"></i><b>8.3.2</b> More specific priors</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#heteroskedasticity-and-distributional-or-mixture-models"><i class="fa fa-check"></i><b>8.4</b> Heteroskedasticity and distributional (or mixture) models</a></li>
<li class="chapter" data-level="8.5" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-simple-model-error-varies-according-to-a-single-fixed-effect"><i class="fa fa-check"></i><b>8.5</b> A ‘simple’ model: Error varies according to a single fixed effect</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#description-of-our-model-2"><i class="fa fa-check"></i><b>8.5.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.5.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#prior-predictive-checks"><i class="fa fa-check"></i><b>8.5.2</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.5.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-5"><i class="fa fa-check"></i><b>8.5.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-complex-model-error-varies-according-to-fixed-and-random-effects"><i class="fa fa-check"></i><b>8.6</b> A ‘complex’ model: Error varies according to fixed and random effects</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2"><i class="fa fa-check"></i><b>8.6.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.6.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-6"><i class="fa fa-check"></i><b>8.6.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#answering-our-research-questions"><i class="fa fa-check"></i><b>8.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="8.8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability"><i class="fa fa-check"></i><b>8.8</b> Building identifiable and supportable models</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#collinearity"><i class="fa fa-check"></i><b>8.8.1</b> Collinearity</a></li>
<li class="chapter" data-level="8.8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#predictable-values-of-categorical-predictors"><i class="fa fa-check"></i><b>8.8.2</b> Predictable values of categorical predictors</a></li>
<li class="chapter" data-level="8.8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#saturated-and-nearly-saturated-models"><i class="fa fa-check"></i><b>8.8.3</b> Saturated, and ‘nearly-saturated’, models</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#exercises-7"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
<li class="chapter" data-level="8.10" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#references-5"><i class="fa fa-check"></i><b>8.10</b> References</a></li>
<li class="chapter" data-level="8.11" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#plot-code-7"><i class="fa fa-check"></i><b>8.11</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html"><i class="fa fa-check"></i><b>9</b> Quantitative predictors and their interactions with factors</a>
<ul>
<li class="chapter" data-level="9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#chapter-pre-cap-8"><i class="fa fa-check"></i><b>9.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>9.2</b> Data and research questions</a></li>
<li class="chapter" data-level="9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#modeling-variation-along-lines"><i class="fa fa-check"></i><b>9.3</b> Modeling variation along lines</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-3"><i class="fa fa-check"></i><b>9.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.3.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-centering"><i class="fa fa-check"></i><b>9.3.2</b> Centering quantitative predictors</a></li>
<li class="chapter" data-level="9.3.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-an-interpreting-the-model"><i class="fa fa-check"></i><b>9.3.3</b> Fitting an interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>9.4</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-4"><i class="fa fa-check"></i><b>9.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.4.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-7"><i class="fa fa-check"></i><b>9.4.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.4.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-shared-non-zero-slopes"><i class="fa fa-check"></i><b>9.4.3</b> Interpreting group effects in the presence of shared (non-zero) slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>9.5</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-5"><i class="fa fa-check"></i><b>9.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.5.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-3"><i class="fa fa-check"></i><b>9.5.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.5.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-varying-slopes"><i class="fa fa-check"></i><b>9.5.3</b> Interpreting group effects in the presence of varying slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-interim-discussion"><i class="fa fa-check"></i><b>9.6</b> Answering our research questions: Interim discussion</a></li>
<li class="chapter" data-level="9.7" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-updated"><i class="fa fa-check"></i><b>9.7</b> Data and research questions: Updated</a></li>
<li class="chapter" data-level="9.8" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-intercepts-and-slopes-for-each-level-of-a-grouping-factor-i.e.-random-slopes"><i class="fa fa-check"></i><b>9.8</b> Models with intercepts and slopes for each level of a grouping factor (i.e. ‘random slopes’)</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-6"><i class="fa fa-check"></i><b>9.8.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.8.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-8"><i class="fa fa-check"></i><b>9.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-multiple-predictors-for-each-level-of-a-grouping-factor"><i class="fa fa-check"></i><b>9.9</b> Models with multiple predictors for each level of a grouping factor</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-7"><i class="fa fa-check"></i><b>9.9.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-5"><i class="fa fa-check"></i><b>9.9.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#model-selection"><i class="fa fa-check"></i><b>9.9.3</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-updated"><i class="fa fa-check"></i><b>9.10</b> Answering our research questions: Updated</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#a-word-on-causality"><i class="fa fa-check"></i><b>9.10.1</b> A word on causality</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#exercises-8"><i class="fa fa-check"></i><b>9.11</b> Exercises</a></li>
<li class="chapter" data-level="9.12" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#references-6"><i class="fa fa-check"></i><b>9.12</b> References</a></li>
<li class="chapter" data-level="9.13" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#plot-code-8"><i class="fa fa-check"></i><b>9.13</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html"><i class="fa fa-check"></i><b>10</b> Logistic regression and signal detection theory models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#chapter-pre-cap-9"><i class="fa fa-check"></i><b>10.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous"><i class="fa fa-check"></i><b>10.2</b> Dichotomous variables and data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#generalizing-our-linear-models"><i class="fa fa-check"></i><b>10.3</b> Generalizing our linear models</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logits"><i class="fa fa-check"></i><b>10.4.1</b> Logits</a></li>
<li class="chapter" data-level="10.4.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit"><i class="fa fa-check"></i><b>10.4.2</b> The inverse logit link function</a></li>
<li class="chapter" data-level="10.4.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#building-intuitions-about-logits-and-the-inverse-logit-function"><i class="fa fa-check"></i><b>10.4.3</b> Building intuitions about logits and the inverse logit function</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-with-one-quantitative-predictor"><i class="fa fa-check"></i><b>10.5</b> Logistic regression with one quantitative predictor</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>10.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.5.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-8"><i class="fa fa-check"></i><b>10.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.5.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-0"><i class="fa fa-check"></i><b>10.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="10.5.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1"><i class="fa fa-check"></i><b>10.5.4</b> Interpreting the model</a></li>
<li class="chapter" data-level="10.5.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification"><i class="fa fa-check"></i><b>10.5.5</b> Using logistic models to understand classification</a></li>
<li class="chapter" data-level="10.5.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-question"><i class="fa fa-check"></i><b>10.5.6</b> Answering our research question</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#measuring-sensitivity-and-bias"><i class="fa fa-check"></i><b>10.6</b> Measuring sensitivity and bias</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-6"><i class="fa fa-check"></i><b>10.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.6.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-9"><i class="fa fa-check"></i><b>10.6.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.6.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#fitting-and-interpreting-the-model-9"><i class="fa fa-check"></i><b>10.6.3</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="10.6.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>10.6.4</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#exercises-9"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
<li class="chapter" data-level="10.8" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#references-7"><i class="fa fa-check"></i><b>10.8</b> References</a></li>
<li class="chapter" data-level="10.9" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#plot-code-9"><i class="fa fa-check"></i><b>10.9</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><i class="fa fa-check"></i><b>11</b> Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#chapter-pre-cap-10"><i class="fa fa-check"></i><b>11.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#models-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.2</b> Models with multiple quantitative predictors</a></li>
<li class="chapter" data-level="11.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#interactions-between-quantitative-predictors"><i class="fa fa-check"></i><b>11.3</b> Interactions between quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#centering-quantitative-predictors-when-including-interactions"><i class="fa fa-check"></i><b>11.3.1</b> Centering quantitative predictors when including interactions</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-7"><i class="fa fa-check"></i><b>11.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="11.3.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-description-1"><i class="fa fa-check"></i><b>11.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="11.3.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-the-model-2"><i class="fa fa-check"></i><b>11.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="11.3.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#advantages-of-bayesian-multilevel-models-for-large-models"><i class="fa fa-check"></i><b>11.3.5</b> Advantages of Bayesian multilevel models for large models</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-BANOVA"><i class="fa fa-check"></i><b>11.4</b> Bayesian Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#getting-the-standard-deviations-from-our-models-manually"><i class="fa fa-check"></i><b>11.4.1</b> Getting the standard deviations from our models ‘manually’</a></li>
<li class="chapter" data-level="11.4.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#using-the-banova-function"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>banova</code> function</a></li>
<li class="chapter" data-level="11.4.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-comparing-the-reduced-model"><i class="fa fa-check"></i><b>11.4.3</b> Fitting and comparing the reduced model</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#a-logistic-regression-model-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.5</b> A logistic regression model with multiple quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-8"><i class="fa fa-check"></i><b>11.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="11.5.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#description-of-the-model-10"><i class="fa fa-check"></i><b>11.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="11.5.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-the-model-and-applying-a-bayesian-anova"><i class="fa fa-check"></i><b>11.5.3</b> Fitting and the model and applying a Bayesian ANOVA</a></li>
<li class="chapter" data-level="11.5.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c12-2d-categorization"><i class="fa fa-check"></i><b>11.5.4</b> Categorization in two dimensions</a></li>
<li class="chapter" data-level="11.5.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#model-selection-and-misspecification"><i class="fa fa-check"></i><b>11.5.5</b> Model selection and misspecification</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#exercises-10"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#references-8"><i class="fa fa-check"></i><b>11.7</b> References</a></li>
<li class="chapter" data-level="11.8" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#plot-code-10"><i class="fa fa-check"></i><b>11.8</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial and Ordinal regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#chapter-pre-cap-11"><i class="fa fa-check"></i><b>12.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="12.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logits-and-the-softmax-function"><i class="fa fa-check"></i><b>12.2.1</b> Multinomial logits and the softmax function</a></li>
<li class="chapter" data-level="12.2.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#comparison-to-logistic-regression"><i class="fa fa-check"></i><b>12.2.2</b> Comparison to logistic regression</a></li>
<li class="chapter" data-level="12.2.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-9"><i class="fa fa-check"></i><b>12.2.3</b> Data and research questions</a></li>
<li class="chapter" data-level="12.2.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-our-model-3"><i class="fa fa-check"></i><b>12.2.4</b> Description of our model</a></li>
<li class="chapter" data-level="12.2.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-the-model-3"><i class="fa fa-check"></i><b>12.2.5</b> Fitting the model</a></li>
<li class="chapter" data-level="12.2.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>12.2.6</b> Interpreting the model</a></li>
<li class="chapter" data-level="12.2.7" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-multinomial-territorial-maps"><i class="fa fa-check"></i><b>12.2.7</b> Multinomial models and territorial maps</a></li>
<li class="chapter" data-level="12.2.8" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#refitting-the-model-without-speaker-random-effects"><i class="fa fa-check"></i><b>12.2.8</b> Refitting the model without speaker random effects</a></li>
<li class="chapter" data-level="12.2.9" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-2"><i class="fa fa-check"></i><b>12.2.9</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Ordinal (logistic) regression</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-cumulative-density"><i class="fa fa-check"></i><b>12.3.1</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="12.3.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-10"><i class="fa fa-check"></i><b>12.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="12.3.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-the-model-11"><i class="fa fa-check"></i><b>12.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="12.3.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-and-interpreting-the-model-10"><i class="fa fa-check"></i><b>12.3.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="12.3.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#listener-specific-discrimination-terms"><i class="fa fa-check"></i><b>12.3.5</b> Listener-specific discrimination terms</a></li>
<li class="chapter" data-level="12.3.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-3"><i class="fa fa-check"></i><b>12.3.6</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#exercises-11"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
<li class="chapter" data-level="12.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#references-9"><i class="fa fa-check"></i><b>12.5</b> References</a></li>
<li class="chapter" data-level="12.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#plot-code-11"><i class="fa fa-check"></i><b>12.6</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><i class="fa fa-check"></i><b>13</b> Writing up experiments: An investigation of the perception of apparent speaker characteristics from speech acoustics</a>
<ul>
<li class="chapter" data-level="13.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#fundamental-frequency-and-voice-pitch"><i class="fa fa-check"></i><b>13.1.1</b> Fundamental frequency and voice pitch</a></li>
<li class="chapter" data-level="13.1.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-fundamental-frequency-between-speakers"><i class="fa fa-check"></i><b>13.1.2</b> Variation in fundamental frequency between speakers</a></li>
<li class="chapter" data-level="13.1.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#voice-resonance-and-vocal-tract-length"><i class="fa fa-check"></i><b>13.1.3</b> Voice resonance and vocal-tract length</a></li>
<li class="chapter" data-level="13.1.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-estimating-vtl"><i class="fa fa-check"></i><b>13.1.4</b> Estimating vocal-tracts length from speech</a></li>
<li class="chapter" data-level="13.1.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-vocal-tract-length-between-speakers"><i class="fa fa-check"></i><b>13.1.5</b> Variation in vocal-tract length between speakers</a></li>
<li class="chapter" data-level="13.1.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-perception-of-chars"><i class="fa fa-check"></i><b>13.1.6</b> Perception of age, gender and size</a></li>
<li class="chapter" data-level="13.1.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#category-dependent-behavior"><i class="fa fa-check"></i><b>13.1.7</b> Category-dependent behavior</a></li>
<li class="chapter" data-level="13.1.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-current-experiment"><i class="fa fa-check"></i><b>13.1.8</b> The current experiment</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#methods"><i class="fa fa-check"></i><b>13.2</b> Methods</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#participants"><i class="fa fa-check"></i><b>13.2.1</b> Participants</a></li>
<li class="chapter" data-level="13.2.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-stimuli"><i class="fa fa-check"></i><b>13.2.2</b> Stimuli</a></li>
<li class="chapter" data-level="13.2.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#procedure"><i class="fa fa-check"></i><b>13.2.3</b> Procedure</a></li>
<li class="chapter" data-level="13.2.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#data-screening"><i class="fa fa-check"></i><b>13.2.4</b> Data screening</a></li>
<li class="chapter" data-level="13.2.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#loading-the-data-and-packages"><i class="fa fa-check"></i><b>13.2.5</b> Loading the data and packages</a></li>
<li class="chapter" data-level="13.2.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-height"><i class="fa fa-check"></i><b>13.2.6</b> Statistical Analysis: Apparent height</a></li>
<li class="chapter" data-level="13.2.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-gender"><i class="fa fa-check"></i><b>13.2.7</b> Statistical Analysis: Apparent gender</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-height-judgments"><i class="fa fa-check"></i><b>13.3</b> Results: Apparent height judgments</a></li>
<li class="chapter" data-level="13.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-height"><i class="fa fa-check"></i><b>13.4</b> Discussion: Apparent height</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#age-dependent-use-of-vtl-cues-on-apparent-height"><i class="fa fa-check"></i><b>13.4.1</b> Age-dependent use of VTL cues on apparent height</a></li>
<li class="chapter" data-level="13.4.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-effect-for-apparent-gender-on-apparent-height"><i class="fa fa-check"></i><b>13.4.2</b> The effect for apparent gender on apparent height</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-height-judgments"><i class="fa fa-check"></i><b>13.5</b> Conclusion: Apparent height judgments</a></li>
<li class="chapter" data-level="13.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.6</b> Results: Apparent gender judgments</a></li>
<li class="chapter" data-level="13.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.7</b> Discussion: Apparent gender judgments</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#effect-of-apparent-age-on-the-perception-of-femaleness"><i class="fa fa-check"></i><b>13.7.1</b> Effect of apparent age on the perception of femaleness</a></li>
<li class="chapter" data-level="13.7.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#between-listener-variation-in-gender-perception"><i class="fa fa-check"></i><b>13.7.2</b> Between-listener variation in gender perception</a></li>
<li class="chapter" data-level="13.7.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#beyond-gross-acoustic-cues-in-gender-perception"><i class="fa fa-check"></i><b>13.7.3</b> Beyond gross acoustic cues in gender perception</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-gender"><i class="fa fa-check"></i><b>13.8</b> Conclusion: Apparent gender</a></li>
<li class="chapter" data-level="13.9" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#next-steps"><i class="fa fa-check"></i><b>13.9</b> Next steps</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#research-design-variable-selection-etc."><i class="fa fa-check"></i><b>13.9.1</b> Research design, variable selection, etc.</a></li>
<li class="chapter" data-level="13.9.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#non-linear-models"><i class="fa fa-check"></i><b>13.9.2</b> Non-linear models</a></li>
<li class="chapter" data-level="13.9.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#other-data-distributions"><i class="fa fa-check"></i><b>13.9.3</b> Other data distributions</a></li>
<li class="chapter" data-level="13.9.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#multivariate-analyses"><i class="fa fa-check"></i><b>13.9.4</b> Multivariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#references-10"><i class="fa fa-check"></i><b>13.10</b> References</a></li>
<li class="chapter" data-level="13.11" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#plot-code-12"><i class="fa fa-check"></i><b>13.11</b> Plot Code</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/santiagobarreda/bmmrmd" target="blank">Book GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probabilities-likelihood-and-inference" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Probabilities, likelihood, and inference<a href="probabilities-likelihood-and-inference.html#probabilities-likelihood-and-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the previous chapter we introduced some data and talked about variables and experiments. In this chapter we’re going to talk about probabilities and explain how they can be used to make inferences about our data and research questions. Before beginning this chapter we should note that it’s normal if some, or many, of the topics to be discussed don’t make sense the first time you read this chapter. Things will make more sense once you start to actually build models and it becomes less hypothetical and more practical.</p>
<p>In addition, as noted in the preface, it’s a mistake to think that you can read a chapter in a statistics textbook once and move on having fully absorbed the content. If many of the topics in this chapter are new to you, you should probably: 1) Read this chapter, 2) wait a few days and read it again, and then 3) wait a few more days (or weeks) and read it again. It may also be useful to return to this chapter even once you are working through the following chapters. In our experience, you may find that you <em>see</em> information for the first time that was there all along, but that finally makes sense given your increased experience with the subject.</p>
<p>Finally, we want to note that what we present below is a teeny tiny slice of statistical inference based on the assumption of normally distributed data (or actually, errors). In chapter 5 we will introduce another error distribution (t) and in chapter 10 we will model binary dependent variables using logistic regression. In each case, those models require us to change aspects of the details outlined below. However, the same general principles still apply with respect to the relationships between data, likelihoods, and the estimation of credible parameter values.</p>
<div id="chapter-pre-cap-1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Chapter pre-cap<a href="probabilities-likelihood-and-inference.html#chapter-pre-cap-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter we introduce important statistical concepts such as probability, probability distributions, and likelihoods. The discussion in the chapter centers around the Gaussian/Normal distribution, and making inferences related to the population mean parameter. First, we introduce conditional, marginal, and joint probabilities, and the importance of assuming the independence of observations. Then, we present probability densities and the normal distribution, and discuss the parameters of this distribution in detail. Following this, we introduce likelihood functions, and discuss logarithms and the use of log-likelihoods. Finally, we outline the use of likelihood functions and statistical models to make inferences about credible values of parameters.</p>
</div>
<div id="c2-data" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Data and research questions<a href="probabilities-likelihood-and-inference.html#c2-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’re going to think about a hypothetical value “the average apparent height of adult males”. To put it another way, we might wonder “how tall do men ‘sound’?”. The apparent heights of men cannot be known <em>a priori</em>. In other words, you don’t know how tall a random man will ‘sound’ until you actually observe the judgment. For this reason, “the average apparent height of men” can be modeled as a random variable. We’re going to think about how we can use our experimental data to try to answer the following two questions:</p>
<p>(Q1) How tall does the average adult male ‘sound’?</p>
<p>(Q2) Can we set limits on credible average apparent heights based on the data we collected?</p>
<p>These two questions can be thought of as relating to the central location and the spread of the data, respectively. Answering the first question tells you what values your variable tends to take on, while answering the second question tells you how much variation you can expect around the most typical values. Scientific research is often focused on questions such as (Q1) regarding the central location, the average value, of some variable. For example someone might ask “how tall do adult male speakers sound?” and you can say, for example, “I have some data that suggests 174 cm is a reasonable estimate”. However, reliable inference requires answering question (Q2) as well, and determining what range of values are plausible for a certain variable.</p>
<p>Think of the (actual) average height of the people in a large city. You can go out and sample 100 people and find the mean (average) of your sample, arriving at a single estimate of the population mean. Now imagine that 50 people went out in the same city and each sampled 100 random people. There is no chance that every one of those 50 people would find identical means across all of their samples. Instead, there will be a distribution of sample means, in the same way there is a distribution of the original data used to calculate the means. Another way to look at this is that there is some degree of <strong>uncertainty</strong> involved when answering any research question. As a result of uncertainty, it can be difficult to rule out alternative possible answers to our research questions. For example, if 174 cm is a good estimate, what about 173.99 cm? What about 173 cm? 172 cm? Where do estimates of average height stop being ‘good’? Without being able to say what is <em>not</em> a good estimate, it’s not quite as useful to be able to say what <em>is</em>.</p>
<p>A related issue arises with respect to the interpretation of the magnitude of effects. Imagine that you read about a miracle diet that was guaranteed to make you lose one gram of weight a day. You know that is not very impressive. How? Because you understand that a difference of one gram is not large relative to the variation that exists in the weight of a human body on a daily basis. You could gain the weight back (and more) by drinking a teaspoon of water. Without knowing how much human weight tends to vary between and within people, it’s impossible to know whether a reduction of one gram constitutes a meaningful change in the mass of a human. In contrast, a diet that causes one gram of weight loss in the average hamster may actually be of interest to hamster owners, as this may be a large value relative to natural variation in hamster weight.</p>
<p>So, we see that imposing limits on credible ranges for our average values can be as important as finding the average values themselves. Further, in order to properly contextualize values and effects, we need to have some idea about the underlying variation in the measurements. Clearly, we need some principled way to ‘guess’ reasonable ranges based on our sample of observations, in addition to just talking about average values. In this chapter we will discuss how statistics provides us with a framework to answer (Q1) and (Q2) above using only our sample of values and a (statistical) model.</p>
<p>In order to discuss the apparent heights of adult males, we need to extract the subset of speakers judged to be adult males from our data. Below we load the book package (<code>bmmb</code>) and select the relevant rows from our experimental data. Recall that our experiment contained an acoustic manipulation such that speech resonances were changed to make speakers sound bigger (see section <a href="introduction-experiments-and-variables.html#c1-methods">1.3.2</a>). In the main text, we’re going to focus only on the unmodified productions, the ‘natural’ speech produced by the men in our sample. To do this, we are going to use the data in <code>exp_data</code>, which contains only the trials involving unmodified productions.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="probabilities-likelihood-and-inference.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load book package </span></span>
<span id="cb28-2"><a href="probabilities-likelihood-and-inference.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb28-3"><a href="probabilities-likelihood-and-inference.html#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="probabilities-likelihood-and-inference.html#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load experimental data</span></span>
<span id="cb28-5"><a href="probabilities-likelihood-and-inference.html#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb28-6"><a href="probabilities-likelihood-and-inference.html#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="probabilities-likelihood-and-inference.html#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Take only rows produced by men (`m`)</span></span>
<span id="cb28-8"><a href="probabilities-likelihood-and-inference.html#cb28-8" aria-hidden="true" tabindex="-1"></a>men <span class="ot">=</span> exp_data[exp_data<span class="sc">$</span>C_v <span class="sc">==</span> <span class="st">&#39;m&#39;</span>,]</span>
<span id="cb28-9"><a href="probabilities-likelihood-and-inference.html#cb28-9" aria-hidden="true" tabindex="-1"></a>mens_height <span class="ot">=</span> men<span class="sc">$</span>height</span></code></pre></div>
<p>We can have a look at some of the quantiles (see section <a href="introduction-experiments-and-variables.html#c1-quantitative">1.4.4</a>) to get an idea of what range of values this variable tends to have. We see that the minimum and maximum values are 139.7 and 192.3 cm, that 174.5 cm is the median, and that half of the observed height judgments for adult males fell between 169.2 and 179.1 cm.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="probabilities-likelihood-and-inference.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span> (mens_height)</span>
<span id="cb29-2"><a href="probabilities-likelihood-and-inference.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="do">##    0%   25%   50%   75%  100% </span></span>
<span id="cb29-3"><a href="probabilities-likelihood-and-inference.html#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 139.7 169.2 174.5 179.1 192.3</span></span></code></pre></div>
<p>Obviously, inspecting the distribution of our observed height judgments only gives us direct information about the judgments we actually observed. To make inferences about the probable characteristics of the height judgments we have <em>not</em> observed, or to talk about height judgments for adult males more generally, we rely on methods of statistical inference, as will be described below.</p>
</div>
<div id="c2-empirical-prob" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Empirical Probabilities<a href="probabilities-likelihood-and-inference.html#c2-empirical-prob" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>sample space</strong> of a variable is the set of all possible outcomes/values that a variable can take. Classic examples are a coin flip, which can take on the values ‘heads’ or ‘tails’, or the roll of a standard six-sided die, which can take on the values one through six. In other cases the sample space may have an infinite or practically infinite number of members. For example, since time is continuous there are an infinite number of durations an event may have, given adequate precision in measuring time. If we think of the human population, or the population of fish in the sea, these are theoretically finite but practically infinite. It would be extremely difficult to fully sample either of these populations, and impossible to do so before they changed substantially (e.g., before some members have died and others have been born).</p>
<p>The <strong>probability</strong> of an event/outcome is the number of times an outcome occurs relative to the number of all possible outcomes that can occur (i.e. all of the outcomes in the sample space). For example, there are six possible outcomes when you roll a die. As a result, the probability of any one outcome is 1/6 (0.17), and the probability of observing an even number is 3/6 (0.5). Actually, this is an extremely brief, and vastly oversimplified, presentation of probability, a topic that could occupy one’s entire career. There are actually many ways to conceive of probability mathematically and philosophically. For more detailed, rigorous expositions, see Ross (2019) for a good introductory text, Parzen (1960) for a more classical, and advanced, approach, and Jaynes (2003) for a distinctly Bayesian treatment of the subject.</p>
<p>By convention, the probability of each event is assigned a value between 0 and 1 and the total probability of all of the possible outcomes in the sample space is equal to one. As a result of this, you know that a probability of 0.5 means something is expected to occur half the time (i.e. on 50% of trials), and a probability of 0.25 indicates that something should happen 25% of the time, about one in every four trials.</p>
<p><strong>Empirical probabilities</strong> are the probabilities of different outcomes in a sample of data. For example, we can flip a coin 100 times and observe 65 heads. This means that the empirical probability of observing heads in our data is 0.65 (65% of trials). Empirical probabilities can also be referred to as relative frequencies or proportions. Later, we’re going to talk about <em>theoretical</em> probabilities based on mathematical models.</p>
<p>Suppose we want to know the probability of observing an apparent height judgment over 180 cm in our sample. To calculate the empirical probability of this occurring in our data we need to find 1) The number of times it occurred, and 2) the total number of observations for the variable. We can do this easily using the logical operators and variables discussed in chapter 1. Below, we find the total number of outcomes that satisfy our restrictions (being over 180 cm), and divide this by the total number of observations being considered.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="probabilities-likelihood-and-inference.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the evaluation in the parenthesis will return 1 if true, 0 if false</span></span>
<span id="cb30-2"><a href="probabilities-likelihood-and-inference.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations the fall above the threshold</span></span>
<span id="cb30-3"><a href="probabilities-likelihood-and-inference.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (mens_height <span class="sc">&gt;</span> <span class="dv">180</span>)  </span>
<span id="cb30-4"><a href="probabilities-likelihood-and-inference.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 136</span></span>
<span id="cb30-5"><a href="probabilities-likelihood-and-inference.html#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="probabilities-likelihood-and-inference.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># divided by total number of events</span></span>
<span id="cb30-7"><a href="probabilities-likelihood-and-inference.html#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (mens_height <span class="sc">&gt;</span> <span class="dv">180</span>) <span class="sc">/</span> <span class="fu">length</span> (mens_height)  </span>
<span id="cb30-8"><a href="probabilities-likelihood-and-inference.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.2015</span></span>
<span id="cb30-9"><a href="probabilities-likelihood-and-inference.html#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="probabilities-likelihood-and-inference.html#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># a shortcut to calculate probability, mean = sum/length</span></span>
<span id="cb30-11"><a href="probabilities-likelihood-and-inference.html#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (mens_height <span class="sc">&gt;</span> <span class="dv">180</span>)</span>
<span id="cb30-12"><a href="probabilities-likelihood-and-inference.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.2015</span></span></code></pre></div>
<p>The top value is the frequency of the occurrence. This is not so useful because this number can mean very different things given different sample sizes (e.g., 136/675, 136/675000). The middle and bottom values have been divided by the total number of observations. As a result, these now represent an estimate of the probability of occurrence in a way that is independent of the total number of observations.</p>
<div id="c2-conditional" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Conditional and marginal probabilities<a href="probabilities-likelihood-and-inference.html#c2-conditional" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-1">2.1</a> presents boxplots of the overall distribution of height judgments in our data (left), and of the distribution of these provided by each listener individually. We can see that height judgments range from about 140 to 200 cm, with most responses falling between 170 and 180 cm. Notice that our overall boxplot does not give us any information about the ranges used by different listeners, nor even the fact that the data was contributed by different listeners. This overall distribution of height responses is the <strong>marginal distribution</strong> of height judgments. The marginal distribution of a variable is the overall distribution, <em>across</em> all values of all other variables. The marginal boxplot on the left compresses all of the listener-specific boxplots on the right into one single box. It’s common to denote marginal probabilities using notation like this <span class="math inline">\(P(variable)\)</span>, meaning we might refer to the marginal probability of height responses like this <span class="math inline">\(P(height)\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:F2-1"></span>
<img src="_main_files/figure-html/F2-1-1.jpeg" alt="(left) Boxplot showing all height judgements for adult male speakers in our experiment. (right) Individual boxplots for each listener's responses" width="4800" />
<p class="caption">
Figure 2.1: (left) Boxplot showing all height judgements for adult male speakers in our experiment. (right) Individual boxplots for each listener’s responses
</p>
</div>
<p>A look at figure <a href="probabilities-likelihood-and-inference.html#fig:F2-1">2.1</a> reveals that the probability of observing a height response of over 180 cm can depend substantially on the listener that provided it (e.g. compare listener 7 vs listener 12). Recall that height is a quantitative variable and listener is a categorical predictor (a factor) with 15 levels, one for each listener (see section <a href="introduction-experiments-and-variables.html#c1-categorical">1.4.3</a>). We can talk about how height judgments vary across the levels of our listener factor (i.e. for different listeners) by considering the <strong>conditional probability</strong> of height <em>given</em> listener. A conditional probability is the probability of an outcome given that some other outcome has occurred. For example, rather than ask “what is the probability of observing an apparent height over 180 cm?”, we can ask “what is the probability of observing an apparent height over 180 cm <em>given</em> that we are observing data from listener 12?”. Conditional probabilities basically reduce the sample space by including only the subset of events that satisfy the given condition.</p>
<p>Conditional probabilities are often denoted like this <span class="math inline">\(P(outcome \,variable | conditioning \, variable)\)</span>, which in this case would look like <span class="math inline">\(P(height | L)\)</span>. For example, the first box in the right plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-1">2.1</a> is the distribution of <span class="math inline">\(P(height | L=1)\)</span>, and the second box is the distribution of <span class="math inline">\(P(height | L=2)\)</span>. Below we divide our vector of height judgments (<code>mens_height</code>) into those contributed by listener 7 and listener 12. We then find the probability of observing a height judgment over 180 cm conditional on listener, and see that these can differ quite a bit from each other.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="probabilities-likelihood-and-inference.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create subsets based on listener</span></span>
<span id="cb31-2"><a href="probabilities-likelihood-and-inference.html#cb31-2" aria-hidden="true" tabindex="-1"></a>L07 <span class="ot">=</span> mens_height[men<span class="sc">$</span>L<span class="sc">==</span><span class="dv">7</span>]</span>
<span id="cb31-3"><a href="probabilities-likelihood-and-inference.html#cb31-3" aria-hidden="true" tabindex="-1"></a>L12 <span class="ot">=</span> mens_height[men<span class="sc">$</span>L<span class="sc">==</span><span class="dv">12</span>]</span>
<span id="cb31-4"><a href="probabilities-likelihood-and-inference.html#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="probabilities-likelihood-and-inference.html#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># find the conditional probability of height&gt;180 for each listener</span></span>
<span id="cb31-6"><a href="probabilities-likelihood-and-inference.html#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (L07 <span class="sc">&gt;</span> <span class="dv">180</span>)</span>
<span id="cb31-7"><a href="probabilities-likelihood-and-inference.html#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.04444</span></span>
<span id="cb31-8"><a href="probabilities-likelihood-and-inference.html#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="probabilities-likelihood-and-inference.html#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (L12 <span class="sc">&gt;</span> <span class="dv">180</span>)</span>
<span id="cb31-10"><a href="probabilities-likelihood-and-inference.html#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.6444</span></span></code></pre></div>
<p>In the boxplots in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-1">2.1</a>, we see that apparent height distributions vary substantially as a function of the value of <code>L</code>, our listener variable. Contrast this with the boxplots seen in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-2">2.2</a>, which shows the distribution of stimulus durations conditional on listener. Since every individual recording had a fixed duration and all listeners heard the same sounds, we know that the distribution of stimulus durations is identical across all levels of the listener variable. As a result, we can see that all of the conditional distributions of duration given listener look just like each other, and just like the marginal probability. This tells us that duration and listener are <em>statistically independent</em>.</p>
<div class="figure"><span style="display:block;" id="fig:F2-2"></span>
<img src="_main_files/figure-html/F2-2-1.jpeg" alt="(left) Boxplot showing the duration of stimuli for adult male speakers in our experiment. (right) Individual boxplots for the stimuli presented to each listener." width="4800" />
<p class="caption">
Figure 2.2: (left) Boxplot showing the duration of stimuli for adult male speakers in our experiment. (right) Individual boxplots for the stimuli presented to each listener.
</p>
</div>
<p>When two variables are <strong>statistically independent</strong>, the distribution of one variable is not affected by the values of the other. As a result, the conditional distribution of one variable given the other will be the same as its marginal distribution, as seen in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-1">2.1</a>. This can be stated for the general case as <span class="math inline">\(P(variable | conditioning \, variable) = P(variable)\)</span>. In our example in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-1">2.1</a>, <span class="math inline">\(P(height | L=7)\)</span> is <em>not</em> equal to <span class="math inline">\(P(height | L=12)\)</span>, and neither of these is equal to the marginal distribution. As a result, we conclude that the variables apparent height and listener are not independent. Instead, they are <strong>statistically dependent</strong>, meaning that these variables <em>are</em> related to each other in some way, and that knowing the value of one may tell you something about probable values of the other.</p>
</div>
<div id="c2-joint" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Joint probabilities<a href="probabilities-likelihood-and-inference.html#c2-joint" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Joint probabilities</strong> reflect the probability of two or more things occurring together. We can refer to the joint probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> using the notation <span class="math inline">\(P(A \cap B)\)</span> or <span class="math inline">\(P(A \&amp; B)\)</span>. Here are some important things to know about joint probabilities:</p>
<ol style="list-style-type: decimal">
<li><p>The formula for calculating the joint probability of two outcomes <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is given by <span class="math inline">\(P(A \cap B)=P(A|B) \cdot P(B)\)</span>. This means that the probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is equal to the conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>, multiplied by the marginal probability of <span class="math inline">\(B\)</span>. In plain English this means that the joint probability of A and B is the probability of <span class="math inline">\(A\)</span> assuming <span class="math inline">\(B\)</span> is true, times the probability that <span class="math inline">\(B\)</span> is true.</p></li>
<li><p>Recall from section <a href="probabilities-likelihood-and-inference.html#c2-conditional">2.3.1</a> that when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent <span class="math inline">\(P(A|B)=P(B)\)</span>. In other words when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent the conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> equals the marginal (unconditional) probability of <span class="math inline">\(A\)</span>. As a result of this, when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent <span class="math inline">\(P(A|B) \cdot P(B)=P(A) \cdot P(B)\)</span>. Thus, when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent their joint probability can be found by simply multiplying their individual marginal probabilities.</p></li>
<li><p>The probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is equal to the probability of <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span>. As a result of this <span class="math inline">\(P(A \cap B)=P(B \cap A)\)</span>, and as a result of that <span class="math inline">\(P(A|B) \cdot P(B)=P(B|A) \cdot P(A)\)</span>. This relation is very important and will become useful later.</p></li>
</ol>
<p>We can demonstrate the above properties using the empirical probabilities in our data. Consider the joint probability of observing a response in our data that was contributed by listener seven (<code>L='12'</code>), and also being longer than 250 ms in duration. We can find this by joining two logical variables using the <code>&amp;</code> (and) symbol as shown below.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="probabilities-likelihood-and-inference.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TRUE if the listener is 12</span></span>
<span id="cb32-2"><a href="probabilities-likelihood-and-inference.html#cb32-2" aria-hidden="true" tabindex="-1"></a>L12 <span class="ot">=</span> (men<span class="sc">$</span>L<span class="sc">==</span><span class="st">&#39;12&#39;</span>)</span>
<span id="cb32-3"><a href="probabilities-likelihood-and-inference.html#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="probabilities-likelihood-and-inference.html#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># TRUE if the duration is greater than 250 ms</span></span>
<span id="cb32-5"><a href="probabilities-likelihood-and-inference.html#cb32-5" aria-hidden="true" tabindex="-1"></a>dur_250 <span class="ot">=</span> men<span class="sc">$</span>dur <span class="sc">&gt;</span> <span class="dv">250</span></span>
<span id="cb32-6"><a href="probabilities-likelihood-and-inference.html#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="probabilities-likelihood-and-inference.html#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The probability of A and B</span></span>
<span id="cb32-8"><a href="probabilities-likelihood-and-inference.html#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (L12 <span class="sc">&amp;</span> dur_250)</span>
<span id="cb32-9"><a href="probabilities-likelihood-and-inference.html#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.02815</span></span>
<span id="cb32-10"><a href="probabilities-likelihood-and-inference.html#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="probabilities-likelihood-and-inference.html#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (men<span class="sc">$</span>L<span class="sc">==</span><span class="st">&#39;12&#39;</span> <span class="sc">&amp;</span> men<span class="sc">$</span>dur <span class="sc">&gt;</span> <span class="dv">250</span>)</span>
<span id="cb32-12"><a href="probabilities-likelihood-and-inference.html#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.02815</span></span></code></pre></div>
<p>So, we see that the probability of observing this event is 0.028, indicating that we expect this in about 3% of trials. Below, we see that this same joint probability can be calculated based on <span class="math inline">\(P(A \cap B)=P(A|B)*P(B)\)</span>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="probabilities-likelihood-and-inference.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal probability of observing listener 12 (i.e. P(L=12))</span></span>
<span id="cb33-2"><a href="probabilities-likelihood-and-inference.html#cb33-2" aria-hidden="true" tabindex="-1"></a>p_L12 <span class="ot">=</span> <span class="fu">mean</span> (men<span class="sc">$</span>L<span class="sc">==</span><span class="st">&#39;12&#39;</span>)</span>
<span id="cb33-3"><a href="probabilities-likelihood-and-inference.html#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="probabilities-likelihood-and-inference.html#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset containing only listener 12 </span></span>
<span id="cb33-5"><a href="probabilities-likelihood-and-inference.html#cb33-5" aria-hidden="true" tabindex="-1"></a>L12 <span class="ot">=</span> men[men<span class="sc">$</span>L <span class="sc">==</span> <span class="st">&#39;12&#39;</span>,]</span>
<span id="cb33-6"><a href="probabilities-likelihood-and-inference.html#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="probabilities-likelihood-and-inference.html#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability of dur&gt;250 given listener 12 (i.e., P(dur&gt;250 | L=12))</span></span>
<span id="cb33-8"><a href="probabilities-likelihood-and-inference.html#cb33-8" aria-hidden="true" tabindex="-1"></a>p_dur_250_given_L12 <span class="ot">=</span> <span class="fu">mean</span> (L12<span class="sc">$</span>dur <span class="sc">&gt;</span> <span class="dv">250</span>)</span>
<span id="cb33-9"><a href="probabilities-likelihood-and-inference.html#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="probabilities-likelihood-and-inference.html#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint probability = P(dur&gt;250 | L=12) * P(L=12)</span></span>
<span id="cb33-11"><a href="probabilities-likelihood-and-inference.html#cb33-11" aria-hidden="true" tabindex="-1"></a>p_dur_250_given_L12 <span class="sc">*</span> p_L12</span>
<span id="cb33-12"><a href="probabilities-likelihood-and-inference.html#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.02815</span></span></code></pre></div>
<p>Because of the experimental design, we know that every listener heard the same vowel durations (see Section <a href="introduction-experiments-and-variables.html#c1-exp">1.3</a> for a review of the experimental design). This means that duration is independent of listener: Knowing who the listener is in no way changes the fact that every listener heard the same number of trials with durations longer than 250 ms. As a result of this independence, we could also have calculated the joint probability of the above events by simply multiplying their marginal probabilities, as seen below.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="probabilities-likelihood-and-inference.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint probability = P(dur&gt;250) * P(L=12)</span></span>
<span id="cb34-2"><a href="probabilities-likelihood-and-inference.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (dur_250) <span class="sc">*</span> <span class="fu">mean</span> (p_L12)</span>
<span id="cb34-3"><a href="probabilities-likelihood-and-inference.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.02815</span></span></code></pre></div>
<p>This short cut will not work for variables that are not independent. For example, let’s consider the probability of observing a height judgment of greater than 180 cm provided by listener 12. Below we calculate this the <em>wrong</em> way by multiplying the marginal probabilities, substantially underestimating the probability of the event. The problem is that this method of calculation does not take into account that listener 12 was among the most likely listeners to report heights greater than 180 cm.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="probabilities-likelihood-and-inference.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TRUE if the listener is 12</span></span>
<span id="cb35-2"><a href="probabilities-likelihood-and-inference.html#cb35-2" aria-hidden="true" tabindex="-1"></a>L12 <span class="ot">=</span> (men<span class="sc">$</span>L<span class="sc">==</span><span class="st">&#39;12&#39;</span>)</span>
<span id="cb35-3"><a href="probabilities-likelihood-and-inference.html#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="probabilities-likelihood-and-inference.html#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># TRUE if the height is over 180</span></span>
<span id="cb35-5"><a href="probabilities-likelihood-and-inference.html#cb35-5" aria-hidden="true" tabindex="-1"></a>over_180 <span class="ot">=</span> men<span class="sc">$</span>height <span class="sc">&gt;</span> <span class="dv">180</span></span>
<span id="cb35-6"><a href="probabilities-likelihood-and-inference.html#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="probabilities-likelihood-and-inference.html#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint probability of the observation</span></span>
<span id="cb35-8"><a href="probabilities-likelihood-and-inference.html#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(men<span class="sc">$</span>L<span class="sc">==</span><span class="st">&#39;12&#39;</span> <span class="sc">&amp;</span> men<span class="sc">$</span>height <span class="sc">&gt;</span> <span class="dv">180</span>)</span>
<span id="cb35-9"><a href="probabilities-likelihood-and-inference.html#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.04296</span></span>
<span id="cb35-10"><a href="probabilities-likelihood-and-inference.html#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="probabilities-likelihood-and-inference.html#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrong: multiplying marginal probabilities</span></span>
<span id="cb35-12"><a href="probabilities-likelihood-and-inference.html#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(L12) <span class="sc">*</span> <span class="fu">mean</span>(over_180)</span>
<span id="cb35-13"><a href="probabilities-likelihood-and-inference.html#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.01343</span></span></code></pre></div>
<p>Below we calculate the joint probability correctly by using the conditional probability.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="probabilities-likelihood-and-inference.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal probability of observing listener 12</span></span>
<span id="cb36-2"><a href="probabilities-likelihood-and-inference.html#cb36-2" aria-hidden="true" tabindex="-1"></a>p_L12 <span class="ot">=</span> <span class="fu">mean</span> (men<span class="sc">$</span>L<span class="sc">==</span><span class="st">&#39;12&#39;</span>)</span>
<span id="cb36-3"><a href="probabilities-likelihood-and-inference.html#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="probabilities-likelihood-and-inference.html#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset containing only listener 12 (i.e., given listener 12)</span></span>
<span id="cb36-5"><a href="probabilities-likelihood-and-inference.html#cb36-5" aria-hidden="true" tabindex="-1"></a>L12 <span class="ot">=</span> men[men<span class="sc">$</span>L <span class="sc">==</span> <span class="st">&#39;12&#39;</span>,]</span>
<span id="cb36-6"><a href="probabilities-likelihood-and-inference.html#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditional probability of a height &gt; 180 given listener 12</span></span>
<span id="cb36-7"><a href="probabilities-likelihood-and-inference.html#cb36-7" aria-hidden="true" tabindex="-1"></a>p_over_180_given_L12 <span class="ot">=</span> <span class="fu">mean</span> (L12<span class="sc">$</span>height <span class="sc">&gt;</span> <span class="dv">180</span>)</span>
<span id="cb36-8"><a href="probabilities-likelihood-and-inference.html#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="probabilities-likelihood-and-inference.html#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Correct joint probability</span></span>
<span id="cb36-10"><a href="probabilities-likelihood-and-inference.html#cb36-10" aria-hidden="true" tabindex="-1"></a>p_over_180_given_L12 <span class="sc">*</span> p_L12</span>
<span id="cb36-11"><a href="probabilities-likelihood-and-inference.html#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.04296</span></span></code></pre></div>
<p>This highlights a very important point: The calculation of joint probabilities is much (much) simpler when you can assume that your observations are independent. For example, consider the comparison of the joint probability of four events, one for independent events, and another for dependent events:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
\mathrm{Independent \, events:} \\
P(A \,\&amp;\, B \,\&amp;\, C \,\&amp;\, D) = P(A) \cdot P(B) \cdot P(C) \cdot P(D) \\ \\ \\
\mathrm{Dependent \, events:} \\
P(A \,\&amp;\, B \,\&amp;\, C \,\&amp;\, D) = P(A|B,C,D) \cdot P(B|C,D) \cdot P(C|D) \cdot P(D)
\end{split}
\end{equation}
\]</span></p>
<p>This means that to calculate the joint probability of dependent events, we may need to calculate many conditional probabilities. Although this may not matter when calculating the joint probability of a handful of observations, calculating the joint probability of hundreds or thousands of correlated variables can become difficult if not impossible in practice.</p>
</div>
</div>
<div id="c2-theoretical" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Probability distributions<a href="probabilities-likelihood-and-inference.html#c2-theoretical" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A probability distribution is a function that assigns probabilities to the different possible outcomes in a sample space. We will illustrate and discuss exactly what this means with reference to histograms. In the left plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-3">2.3</a> we see a histogram of counts. This histogram shows the number of times different ranges of height values were observed. The height values are indicated on the x axis, and the heights of the bars reflect the number of times that different ranges of heights were observed. As a result, the bars of the histogram tell you about the values of the variable that are more or less frequent. This sort of representation makes it difficult to compare distributions across samples of different sizes. For example, if the number of observations were tripled (and if the relative frequency of each height observation was preserved), the heights of the bars in the histogram would also triple.</p>
<div class="figure"><span style="display:block;" id="fig:F2-3"></span>
<img src="_main_files/figure-html/F2-3-1.jpeg" alt="(left) A histogram of adult male height judgments showing counts in each bin. (middle) The same data from the left plot, this time showing the density of the distribution. (right) The same data from the middle plot, this time with heights expressed in meters." width="4800" />
<p class="caption">
Figure 2.3: (left) A histogram of adult male height judgments showing counts in each bin. (middle) The same data from the left plot, this time showing the density of the distribution. (right) The same data from the middle plot, this time with heights expressed in meters.
</p>
</div>
<p>Histograms can also be used to show the <strong>probability densities</strong> associated with different values, i.e., the amount of probability ‘mass’ per unit of the variable of interest. In the middle in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-3">2.3</a> we see a histogram that shows the <strong>density</strong> of the distribution of height judgments, i.e. the thickness of the distribution at different locations along the number line. The values of the density, and the heights of the bars of the density histogram, are constrained by the fact that the area under the curve must equal one, since these reflect probabilities. In the case of our histograms this means that the total area of all the bars (rectangles) is constrained to equal one. As a result of this, when variables have wide ranges densities tend to be very small (as in the middle panel in Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-3">2.3</a>).</p>
<p>In the right plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-3">2.3</a> we present heights in meters rather than centimeters. This has the effect of substantially increasing the values of the density (one-hundred fold) but does not otherwise affect the shape of the distribution. The relationship between variable ranges and density values means that densities can’t really be interpreted in an absolute sense - they always reflect the amount of probability per unit. So, if you change the unit, you change the interpretation of the corresponding density. For example, a density value of 0.06 means very different things in the middle and right histograms in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-3">2.3</a>. As a result, density values should be interpreted relative to other density values only along the same x axis (i.e. given the same variable). A higher density in one location tells us that values in that vicinity are more probable than values in locations with lower densities, and differences in densities reflect differences in the relative probabilities of different values.</p>
<p>Probability distributions sometimes have shapes that can be represented using mathematical functions. These functions typically have a limited number of <strong>parameters</strong> that determine their exact shape. Think of parameters as the properties of a system that can be ‘set’ separately from each other. For example, a radio has three parameters: Tuner frequency, band (AM/FM), and volume. A toaster may have only one, a single knob determining the degree of toasting required. The more parameters something has, the more complicated it is (e.g., an airplane may have thousands).</p>
<p>Consider the code below, which defines a slope and intercept and draws a line based on these parameters. You can change the values of the intercept and the slope and draw many kinds of lines. However, there’s no way to change the characteristics of a line other than by changing its slope and intercept: A line is entirely defined by these parameters.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="probabilities-likelihood-and-inference.html#cb37-1" aria-hidden="true" tabindex="-1"></a>intercept <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb37-2"><a href="probabilities-likelihood-and-inference.html#cb37-2" aria-hidden="true" tabindex="-1"></a>slope <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb37-3"><a href="probabilities-likelihood-and-inference.html#cb37-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>,.<span class="dv">1</span>)</span>
<span id="cb37-4"><a href="probabilities-likelihood-and-inference.html#cb37-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> intercept <span class="sc">+</span> x<span class="sc">*</span>slope</span>
<span id="cb37-5"><a href="probabilities-likelihood-and-inference.html#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="probabilities-likelihood-and-inference.html#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,y, <span class="at">type=</span><span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p>Sometimes the same sorts of probability density shapes pop up over and over, and these shapes are often well-defined mathematically. <strong>Parametric probability distributions</strong> are those density shapes that can be represented using curves that vary in terms of a limited number of parameters. Just like with lines, the characteristics of a parametric probability distribution are entirely defined by the values of its parameters.</p>
</div>
<div id="c2-normal" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> The normal distribution<a href="probabilities-likelihood-and-inference.html#c2-normal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The distribution of many random variables (at least approximately) follows what’s known as the <strong>normal</strong> or <strong>Gaussian</strong> distribution. This means that if you take a sample of a random variable and arrange observations into bins, the resulting histogram will tend to have the familiar bell-shaped curve (seen in the histograms in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-3">2.3</a>). Normal distributions have two parameters, meaning they differ from each other in only two ways: A mean (<span class="math inline">\(\mu\)</span>) and a variance (<span class="math inline">\(\sigma^2\)</span>). The normal distribution has the following important characteristics:</p>
<ol style="list-style-type: decimal">
<li><p>The distribution is symmetrical - i.e., observations above and below the mean are equally likely.</p></li>
<li><p>The probability of observing a given value decreases as you get further from the mean (i.e., <em>average</em>) value.</p></li>
<li><p>It’s easy to work with, very well understood, and (approximately) normally distributed data arises in many domains.</p></li>
</ol>
<p>The mean (<span class="math inline">\(\mu\)</span>) of a normal distribution determines the location of the distribution along the number line. When the mean of a normal distribution changes, the whole shape of the distribution ‘slides’ along the number line. The mean is the 50% halfway point of the ‘mass’ of the distribution (i.e., with a normal distribution, the mean is identical to the median). If the distribution were a physical object, its mean would be its center of gravity and you would balance the distribution on your fingertip at this point. The mean of a normal distribution is also the <strong>expected value</strong> of that variable. For discrete variables, the <em>expected value</em> (<span class="math inline">\(\mathbb{E}\)</span>) is the sum of all possible values of <span class="math inline">\(y\)</span> multiplied by the probability of observing each value, <span class="math inline">\(P(y)\)</span>, as in <a href="probabilities-likelihood-and-inference.html#eq:2-3a">(2.1)</a>.</p>
<p><span class="math display" id="eq:2-3a">\[
\begin{equation}
\mathbb{E}(y_{[i]}) = \sum_{i=1}^{\infty} y_{[i]} P(y_{[i]})
\tag{2.1}
\end{equation}
\]</span></p>
<p>For example, for the roll of a single die the possible outcomes are the integers from 1 to 6 and each has a 1/6 chance of occurring. To calculate the expected value, we multiply each outcome by 1/6 and sum the resulting values (1/6, 2/6, 3/6, 4/6, 5/6, 6/6), resulting in 3.5. So, the expected value of a roll of a single die is 3.5. This is why the game of craps involves rolling two dice and centers on whether a player rolls a 7: This is the most probable outcome. Continuous variables have an analogous formula defining the expected value with an integral and a <strong>probability density function</strong> (<span class="math inline">\(f(y)\)</span>), as seen below. A probability density function is just a function that assigns some value, e.g. <span class="math inline">\(f(y)=x\)</span>, for the different values of a variable (e.g., <span class="math inline">\(y\)</span>).</p>
<p><span class="math display" id="eq:2-3b">\[
\begin{equation}
\mathbb{E}(y) = \int_{-\infty}^{\infty} yf(y) \; dx
\tag{2.2}
\end{equation}
\]</span></p>
<p>The <span class="math inline">\(y_{[i]} P(y_{[i]})\)</span> part in the discrete equation above is analogous to the <span class="math inline">\(y f(y)\)</span> in the continuous equation. In each case, the first element (<span class="math inline">\(y\)</span>) represents a possible value and the second element (<span class="math inline">\(P(y)\)</span>, or <span class="math inline">\(f(x)\)</span>) multiplies this by a value reflecting the probability that this value is observed. In the discrete case we use probabilities (<span class="math inline">\(P(y)\)</span>) and add our terms up (with <span class="math inline">\(\mathrm{\Sigma}\)</span>). For the continuous case we use the density (<span class="math inline">\(f(y)\)</span>) and integrate (i.e. find the area under the curve) across a given interval (with <span class="math inline">\(\int dx\)</span>).</p>
<p>The variance (<span class="math inline">\(\sigma^2\)</span>), or standard deviation (<span class="math inline">\(\sigma=\sqrt{\sigma^2}\)</span>), of a normal distribution determines its <em>spread</em> along the x axis. When the standard deviation changes, the distribution is stretched wide or made very narrow, but stays in the same place. Since every distribution has an area under the curve equal to one (i.e., they all have the same ‘volume’), distributions with smaller variances must necessarily be more dense.</p>
<p>In principle, a given probability distribution can be thought of as having a set of ‘true’, fixed parameters. For example, we might imagine that the average apparent height of adult male speakers is exactly 175.91254… cm. This is questionable at least in part because, as noted earlier, many real-life populations are practically infinite and constantly changing. In any case, even if ‘true’, fixed parameters exist, in most situations we can’t be certain of what the true parameters of a distribution are. Instead, we must be satisfied with estimating the values of parameters based on our samples.</p>
<p>Estimates of our population parameters based on our sample are called <strong>statistics</strong>. <strong>Statistical inference</strong> consists of using statistics (based on our sample) to make inferences about the characteristics of the overall population (i.e. the ‘true’ parameters). In the case of the normal distribution we’re interested in two statistics: The sample mean and the sample standard deviation.</p>
<div id="c2-sample-mean" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> The sample mean<a href="probabilities-likelihood-and-inference.html#c2-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The sample mean is our ‘best’ guess for the population mean, i.e., the expected value of the distribution. We’ll be more specific about what ‘best’ means later, but for now we can just say that if you don’t know the population mean parameter for a given normal distribution, and all you have is a sample of observations, the sample mean will provide you the best available estimate. The formula for the sample mean is given in equation <a href="probabilities-likelihood-and-inference.html#eq:2-1">(2.3)</a>. Initially, reading these mathematical formulas may seem daunting. However, learning to read these is just a skill that is developed with practice. In addition, you will begin to see the same ‘chunks’ or structures come up in formulas over and over, and reading these becomes much easier once you start to recognize the meaning of these repetitive structures intuitively.</p>
<p><span class="math display" id="eq:2-1">\[
\begin{equation}
\hat{\mu}_{y} = \sum_{i=1}^{n} y_{[i]}/n
\tag{2.3}
\end{equation}
\]</span></p>
<p>The sample mean is an estimator of the expected value. In <a href="probabilities-likelihood-and-inference.html#eq:2-1">(2.3)</a>, the division by <span class="math inline">\(n\)</span> reflects the fact that we treat each of our <span class="math inline">\(n\)</span> observations as equally likely, estimating each probability as <span class="math inline">\(1/n\)</span>. If we were to replace <span class="math inline">\(\frac{1}{n}\)</span> with <span class="math inline">\(P(y)\)</span> above, equation <a href="probabilities-likelihood-and-inference.html#eq:2-1">(2.3)</a> starts to look a lot more like <a href="probabilities-likelihood-and-inference.html#eq:2-3a">(2.1)</a>.</p>
<p>Equation <a href="probabilities-likelihood-and-inference.html#eq:2-1">(2.3)</a> says that the sample mean of <span class="math inline">\(y\)</span> (<span class="math inline">\(\hat{\mu}_{y}\)</span>) is equal to the sum of all of the elements of the vector <span class="math inline">\(y\)</span>, divided by <span class="math inline">\(n\)</span>, where <span class="math inline">\(n\)</span> is equal to the length of the <span class="math inline">\(y\)</span> vector (i.e., the number of observations). We use the little hat symbol (<span class="math inline">\(\hat{\mu}\)</span>) to indicate that this is an estimate of the mean, and to distinguish it from the population mean, which we cannot observed directly and goes hatless (<span class="math inline">\(\mu\)</span>). The summation (<span class="math inline">\(\sum\)</span>) symbol represents the repetitive adding of whatever is to the right of the symbol to some total. The summation begins at the number below the <span class="math inline">\(\sum\)</span> and performs one operation for every integer value of <span class="math inline">\(i\)</span> between the starting point and the end point (indicated below and above the <span class="math inline">\(\sum\)</span>). The counter variable, in this case <span class="math inline">\(i\)</span>, is also often used to index values of a vector (or other structure) that is being summed.</p>
<p>This behavior is similar to a <code>for</code> loop in R. Below we define the variable <code>n</code> (equal to the length of the vector) and initialize a variable to receive the summation (<code>mean_height</code>). The <code>for</code> loop then proceeds to increment a count variable (<code>i</code>) from one to n along the integers. For each iteration it adds the value of the vector, divided by n, to the summation variable <code>mean_height</code>. As we can see below, this results in a value identical to that returned by the <code>mean</code> function in R.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="probabilities-likelihood-and-inference.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize values</span></span>
<span id="cb38-2"><a href="probabilities-likelihood-and-inference.html#cb38-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span> (mens_height)</span>
<span id="cb38-3"><a href="probabilities-likelihood-and-inference.html#cb38-3" aria-hidden="true" tabindex="-1"></a>mean_height <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb38-4"><a href="probabilities-likelihood-and-inference.html#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="probabilities-likelihood-and-inference.html#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># summation equivalent to equation 2.3</span></span>
<span id="cb38-6"><a href="probabilities-likelihood-and-inference.html#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) mean_height <span class="ot">=</span> mean_height <span class="sc">+</span> mens_height[i]<span class="sc">/</span>n</span>
<span id="cb38-7"><a href="probabilities-likelihood-and-inference.html#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="probabilities-likelihood-and-inference.html#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># replicates values of the mean function</span></span>
<span id="cb38-9"><a href="probabilities-likelihood-and-inference.html#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (mens_height)</span>
<span id="cb38-10"><a href="probabilities-likelihood-and-inference.html#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 173.8</span></span>
<span id="cb38-11"><a href="probabilities-likelihood-and-inference.html#cb38-11" aria-hidden="true" tabindex="-1"></a>mean_height</span>
<span id="cb38-12"><a href="probabilities-likelihood-and-inference.html#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 173.8</span></span></code></pre></div>
<p>Here are some useful things to know about sample means, in no particular order:</p>
<ol style="list-style-type: decimal">
<li><p>The mean of a set of observations is affected by addition and multiplication. Adding <span class="math inline">\(a\)</span> to each member of a set of observations increases its mean by <span class="math inline">\(a\)</span>, and multiplying observations by <span class="math inline">\(b\)</span> results in a change in the mean by the same factor.</p></li>
<li><p>The mean of the sum of two sets of variables (of the same length) <span class="math inline">\(x\)</span> an <span class="math inline">\(y\)</span> is equal to <span class="math inline">\(\mu_{x}+\mu_{y}\)</span>. In other words, the average of the sum is just the sum of the averages.</p></li>
<li><p>The sum of the sample’s deviations from the sample mean equals zero (seen in <a href="probabilities-likelihood-and-inference.html#eq:2-2">(2.4)</a>). This means that the sum of the distances between positive and negative differences from the sample mean exactly balance out. To some extent this makes sense since the mean is the ‘center of gravity’ of a distribution. It’s worth noting that this does not apply to deviations from the <em>population</em> mean since the population mean is not estimated from the characteristics of a sample. So, there is no guarantee that some set of observations will ‘balance out’ around the population mean.</p></li>
</ol>
<p><span class="math display" id="eq:2-2">\[
\begin{equation}
0 = \sum_{i=1}^{n} y_{[i]} - \hat{\mu}_{y}
\tag{2.4}
\end{equation}
\]</span></p>
</div>
<div id="c2-sample-variance" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> The sample variance (or standard deviation)<a href="probabilities-likelihood-and-inference.html#c2-sample-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The formula to calculate the sample variance is seen in <a href="probabilities-likelihood-and-inference.html#eq:2-4">(2.5)</a>. Note that it is quite similar to the structure of <a href="probabilities-likelihood-and-inference.html#eq:2-1">(2.3)</a> and clearly involves the averaging of a value. In fact, if we were to replace <span class="math inline">\((y_{[i]} - \mu_{y})^2\)</span> with <span class="math inline">\(y_[i]\)</span>, the two equations would be identical. The value being averaged consists of a difference (<span class="math inline">\(y_{[i]} - \mu_{y}\)</span>) followed by a squaring operation. So, we see that what’s being averaged is squared deviations from the sample mean. This is what the variance is: The expected value of squared deviations around the mean of the variable.</p>
<p><span class="math display" id="eq:2-4">\[
\begin{equation}
\hat{\sigma}^2_{y} = \sum_{i=1}^n (y_{[i]} - \hat{\mu}_{y})^2 / n
\tag{2.5}
\end{equation}
\]</span></p>
<p>Below we see that we can use a <code>for</code> loop to recreate equation <a href="probabilities-likelihood-and-inference.html#eq:2-4">(2.5)</a>. However, we don’t manage to exactly recreate the output of the <code>var</code> (variance) function included in R.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="probabilities-likelihood-and-inference.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize values</span></span>
<span id="cb39-2"><a href="probabilities-likelihood-and-inference.html#cb39-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span> (mens_height)</span>
<span id="cb39-3"><a href="probabilities-likelihood-and-inference.html#cb39-3" aria-hidden="true" tabindex="-1"></a>variance_height <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb39-4"><a href="probabilities-likelihood-and-inference.html#cb39-4" aria-hidden="true" tabindex="-1"></a>mean_height <span class="ot">=</span> <span class="fu">mean</span> (mens_height)</span>
<span id="cb39-5"><a href="probabilities-likelihood-and-inference.html#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="probabilities-likelihood-and-inference.html#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co"># equivalent to equation 2.5 above</span></span>
<span id="cb39-7"><a href="probabilities-likelihood-and-inference.html#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) variance_height <span class="ot">=</span> </span>
<span id="cb39-8"><a href="probabilities-likelihood-and-inference.html#cb39-8" aria-hidden="true" tabindex="-1"></a>  variance_height <span class="sc">+</span> (mens_height[i]<span class="sc">-</span>mean_height)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n</span>
<span id="cb39-9"><a href="probabilities-likelihood-and-inference.html#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="probabilities-likelihood-and-inference.html#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co"># this time the values don&#39;t match</span></span>
<span id="cb39-11"><a href="probabilities-likelihood-and-inference.html#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span> (mens_height)</span>
<span id="cb39-12"><a href="probabilities-likelihood-and-inference.html#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 60.27</span></span>
<span id="cb39-13"><a href="probabilities-likelihood-and-inference.html#cb39-13" aria-hidden="true" tabindex="-1"></a>variance_height</span>
<span id="cb39-14"><a href="probabilities-likelihood-and-inference.html#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 60.18</span></span></code></pre></div>
<p>This is because R does not use <a href="probabilities-likelihood-and-inference.html#eq:2-4">(2.5)</a> to estimate variances but rather <a href="probabilities-likelihood-and-inference.html#eq:2-5">(2.6)</a>.<br />
<span class="math display" id="eq:2-5">\[
\begin{equation}
\hat{\sigma}^2_{y} = \sum_{i=1}^n (y_{[i]} - \hat{\mu}_{y})^2 / (n-1)
\tag{2.6}
\end{equation}
\]</span></p>
<p>Above we said that the sample mean is our ‘best’ estimate of the population mean given a sample. A more formal way to state this is that the sample mean is the value which minimizes the sample variance. In other words, if we choose <em>any</em> value of <span class="math inline">\(\mu_{y}\)</span> to calculate the sample variance other than the sample mean, the variance will <em>necessarily</em> be larger. However, we know that our sample mean is just an <em>estimate</em> of the population mean and will never be exactly equal to it. As a result of this, the true variance must be greater than the sample variance when calculated using the sample mean. We can put it like this: <span class="math inline">\(\sum_{i=1}^n (y_{[i]} - \hat{\mu}_{y})^2 \leq \sum_{i=1}^n (y_{[i]} - \mu_{y})^2\)</span>, the sum of squares around the sample mean will always be less than or equal to the sum of squares around the population mean. For reasons that we won’t get into (but which aren’t too complicated), this expected difference may be offset by dividing the squared deviations by <span class="math inline">\((n-1)\)</span> rather than <span class="math inline">\(n\)</span> as in <a href="probabilities-likelihood-and-inference.html#eq:2-5">(2.6)</a>. We can update our R code to reflect this change, and see that this now matches the calculation of the variance carried out by R.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="probabilities-likelihood-and-inference.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize variable</span></span>
<span id="cb40-2"><a href="probabilities-likelihood-and-inference.html#cb40-2" aria-hidden="true" tabindex="-1"></a>variance_height <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb40-3"><a href="probabilities-likelihood-and-inference.html#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="probabilities-likelihood-and-inference.html#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># equivalent to 2.6 above</span></span>
<span id="cb40-5"><a href="probabilities-likelihood-and-inference.html#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) variance_height <span class="ot">=</span> variance_height <span class="sc">+</span> (mens_height[i]<span class="sc">-</span>mean_height)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(n<span class="dv">-1</span>)</span>
<span id="cb40-6"><a href="probabilities-likelihood-and-inference.html#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="probabilities-likelihood-and-inference.html#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># this time the values do match</span></span>
<span id="cb40-8"><a href="probabilities-likelihood-and-inference.html#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span> (mens_height)</span>
<span id="cb40-9"><a href="probabilities-likelihood-and-inference.html#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 60.27</span></span>
<span id="cb40-10"><a href="probabilities-likelihood-and-inference.html#cb40-10" aria-hidden="true" tabindex="-1"></a>variance_height</span>
<span id="cb40-11"><a href="probabilities-likelihood-and-inference.html#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 60.27</span></span></code></pre></div>
<p>The sample standard deviation (<span class="math inline">\(\sigma\)</span>) is simply the square root of the sample variance, as in <a href="probabilities-likelihood-and-inference.html#eq:2-6">(2.7)</a>.</p>
<p><span class="math display" id="eq:2-6">\[
\begin{equation}
\hat{\sigma}_{y} = \sqrt{\hat{\sigma}^2_{y}} = \sqrt{\sum_{i=1}^n (y_{[i]} - \mu_{y})^2 / (n-1)}
\tag{2.7}
\end{equation}
\]</span></p>
<p>Here are some useful things to know about variances, in no particular order:</p>
<ol style="list-style-type: decimal">
<li><p>Variances are always positive, and can only be zero for variables that do not actually take on different values (i.e., constants).</p></li>
<li><p>The variance of a set of observations is not affected by addition. So, adding or subtracting some arbitrary value from a data set will not affect the variance of that data.</p></li>
<li><p>Multiplication <em>does</em> affect the variances of a set of values. Multiplying numbers by <span class="math inline">\(x\)</span> results in a change of the variances equal to <span class="math inline">\(x^2\)</span>. So, if we took our heights and multiplied them by 10 to express them in millimeters, we would expect the value of <span class="math inline">\(\sigma^2_{height}\)</span> to increase by a value of <span class="math inline">\(10^2=100\)</span>. Since standard deviations are the square roots of variances, this implies that multiplying data by <span class="math inline">\(x\)</span> results in an increase in the standard deviation of the data by a factor of <span class="math inline">\(x\)</span>.</p></li>
<li><p>The variance of the sum of variables depends on whether they are independent or not. When variables are independent, the sum of their variances is simply equal to <span class="math inline">\(\sigma^2_{x}+\sigma^2_{y}\)</span> for variables x and y. However, when variables are not independent, the variance can be much greater or smaller than this based on the nature of the relationship between the variables. As a result, just as with the calculation of joint probabilities, we need to take into account whether variables are independent or not when we consider the variance of their sum.</p></li>
</ol>
</div>
<div id="c2-normal-density" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> The normal density<a href="probabilities-likelihood-and-inference.html#c2-normal-density" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The parameters of a probability distribution are used to draw its shape, which can be used to make inferences about likely values. Think back to high school math and the function defining the shape of a parabola <span class="math inline">\(y=a(x-h)^2+k\)</span>. This function draws a shape based on the settings of its parameters <span class="math inline">\(a, h\)</span> and <span class="math inline">\(k\)</span>. The <span class="math inline">\(a\)</span> parameter determines the width of the parabola (and whether it points up or down), while the vertex of the parabola will have x and y axis coordinates of <span class="math inline">\(h\)</span> and <span class="math inline">\(k\)</span> respectively. In the same way, the formula defining the density of the normal distribution draws a shape given the settings of its <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> parameters.</p>
<p>The formula for the probability density function of the normal distribution is seen in <a href="probabilities-likelihood-and-inference.html#eq:2-7">(2.8)</a>. The function returns a density value for the probability distribution based on the value of <span class="math inline">\(x\)</span>, and the values of its parameters. The equation in <a href="probabilities-likelihood-and-inference.html#eq:2-7">(2.8)</a> features <strong>exponentiation</strong> (<span class="math inline">\(\exp(x)\)</span>), that is, raising the base <span class="math inline">\(e=2.718...\)</span> (Euler’s number) to some power as in <span class="math inline">\(e^x\)</span>. In <a href="probabilities-likelihood-and-inference.html#eq:2-7">(2.8)</a>, the value being exponentiated is <span class="math inline">\(-\frac{1}{2\sigma^2}(x-\mu)^{2}\)</span>.</p>
<p><span class="math display" id="eq:2-7">\[
\begin{equation}
f(x) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp( -\frac{1}{2\sigma^2}(x-\mu)^{2}\,)
\tag{2.8}
\end{equation}
\]</span></p>
<p>It’s much more common to denote exponentiation like <span class="math inline">\(e^x\)</span> rather than <span class="math inline">\(\exp(x)\)</span>. However, for now we will use the latter notation for two reasons. First, it makes the exponent bigger and more isolated, which can both make equations easier to read. Second, this makes it look just like the R function <code>exp(x)</code>, which makes it easier to remember what it is and what it does.</p>
</div>
<div id="c2-standard-normal" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> The standard normal distribution<a href="probabilities-likelihood-and-inference.html#c2-standard-normal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>standard normal distribution</strong> is a normal distribution with a mean of zero and a standard deviation of one. Variables drawn from a standard normal distribution are often represented by the symbol <span class="math inline">\(z\)</span> (sometimes called a <strong>z score</strong>). Any normally distributed variable can be turned into a standard normal variable by an operation known as <strong>standardization</strong>, which consists of <strong>centering</strong> and then <strong>scaling</strong> the variable as in <a href="probabilities-likelihood-and-inference.html#eq:2-8">(2.9)</a>. To center a variable we subtract the mean from the value of each observation, making the new mean equal to zero. By dividing our observations by the standard deviation, we scale these values so that the new standard deviation is equal to one (since anything divided by itself is equal to one).</p>
<p><span class="math display" id="eq:2-8">\[
\begin{equation}
z=(x-\mu) / \sigma
\tag{2.9}
\end{equation}
\]</span></p>
<p>Figure <a href="probabilities-likelihood-and-inference.html#eq:2-4">(2.5)</a> presents our height data again, but this time compares the data to its centered and standardized versions.</p>
<div class="figure"><span style="display:block;" id="fig:F2-4"></span>
<img src="_main_files/figure-html/F2-4-1.jpeg" alt="(left) A histogram of apparent height judgments for adult male speakers. (middle) The same data from the left plot, this time the data has been centered around the mean so that the new mean is zero. (right) The same data from the middle plot, this time the data has been scaled according to the standard deviation so that the standard deviation is now one (and the mean is still zero)." width="4800" />
<p class="caption">
Figure 2.4: (left) A histogram of apparent height judgments for adult male speakers. (middle) The same data from the left plot, this time the data has been centered around the mean so that the new mean is zero. (right) The same data from the middle plot, this time the data has been scaled according to the standard deviation so that the standard deviation is now one (and the mean is still zero).
</p>
</div>
<p>Equation <a href="probabilities-likelihood-and-inference.html#eq:2-9">(2.10)</a> re-arranges the terms in <a href="probabilities-likelihood-and-inference.html#eq:2-8">(2.9)</a> to isolate <span class="math inline">\(x\)</span> on the left-hand side. From <a href="probabilities-likelihood-and-inference.html#eq:2-9">(2.10)</a> we can see that any normally-distributed variable can be thought of as a standard normal that has been multiplied by a standard deviation and then had a mean added to this product.</p>
<p><span class="math display" id="eq:2-9">\[
\begin{equation}
x=z \cdot \sigma+\mu
\tag{2.10}
\end{equation}
\]</span></p>
<p>Normally-distributed data is often discussed in terms of ‘standard deviations from the mean’. This is because stating things in terms of standard deviations from the mean effectively standardizes a variable, making all variables seem standard normal. For example, if someone says “my test score was two standard deviations above the mean” what do you know about their score? You don’t know what the mean is, nor what the standard deviation is. All you know is that their score is two distance units (standard deviations) above the mean, so their test score can be thought of as a standard normal variable like <span class="math inline">\(z\)</span>. This is despite the obvious fact that the true average test score was not zero and the true average standard deviation was not one. This is an extremely useful property because it means that we can discuss the probability of any given event from any given normal distribution in consistent terms. For example, we can say that an observation four standard deviations from the mean is very unusual in <em>any</em> normal distribution. This means that if you have an observation equal to 140, the mean is 174, and the standard deviation is 8, you know that this observation is very improbable. That is because it is 4.25 (34/8) standard deviations from the mean of 174.</p>
</div>
</div>
<div id="c2-models-and-inference" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Models and inference<a href="probabilities-likelihood-and-inference.html#c2-models-and-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Models are simplified representations that help us understand things. For example, we may want to understand the movement of balls on a billiards table, perhaps to create a video game about playing pool. To do this we may assume the balls are spherical and that their mass is evenly distributed about their volume, among other things. Neither of these things are exactly true but assuming this helps us keep our model simple and manageable. It also helps us build our model in terms of things, like regular spheres, whose properties and behaviors are well understood and are easy to work with.</p>
<p>To build an <em>exact</em> model for our billiards game we would need to include friction from the felt on the table, the effect of wind resistance, the gravitational effect of the moon, and a large number of other factors. As a result, a perfect or exact model is not really possible for most things (and maybe for anything?). And yet, the simulation of realistic behavior of billiard balls is easy and can be done with great accuracy, suggesting that a simplified model can still be useful to understand the behavior of the more-complicated phenomenon it is meant to represent.</p>
<p>In general, it’s impossible to know what the ‘true’ data distribution is, so that <em>perfect</em> inference is not possible. As a result, scientists often use theoretical probability distributions to make inferences about real-life populations and observations. If our measurements more or less follow the ‘shape’ predicted by the theoretical normal distribution, we may be able to use the characteristics of an appropriate normal distribution to make inferences about our variables. Using a normal distribution to make inferences about your data is like using a mathematical model for spheres to understand the behavior of billiard balls. The billiard balls are <em>spherical enough</em> to allow us to make useful predictions based on the simplified model.</p>
<p>It’s useful to keep in mind that reality will never exactly conform to our model. This can result in unpredictable errors in our conclusions. In general, the things you don’t know you don’t know are the things that will cause the most problems. If you had known that your model was wrong, you would have fixed it! Further, using models to make inferences about the general properties of data assumes that the things you have not seen are more or less just like those you have. Under those conditions then the conclusions you draw may be reliable. It’s important to keep this limitation in mind, because you never know for sure that what you have not seen will conform to your model, and as a result the fit between a model and some set of observations can never definitively <em>prove</em> the truth of the relations encoded in the model (this is related to the problem of induction, discussed in section <a href="introduction-experiments-and-variables.html#c1-exp-and-inference">1.2.1</a>).</p>
<p>Parametric distributions can be used to establish <strong>theoretical probabilities</strong>, that is expectations about which events are and are not likely based on the general shape expected for the distribution. Basically, if we expect our distribution of values to have the shape of the normal distribution, we can use the shape of the normal distribution to make inferences about our distribution of values. When we used <em>empirical probabilities</em> above, our probabilities were estimated only with respect to the data we observed. However, when we refer to <em>theoretical</em> probability distributions we can also think about the behavior of values we did not observe, or the behavior of the distribution in general.</p>
<p>In order to calculate theoretical probabilities you first need to commit to a model of the data. You may be thinking, what model? It may seem too simple to be a model, but by assuming that our data can be understood as coming from a normal distribution with some given <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, we’ve already created a simple model for our data. This is analogous to committing to a spherical shape for our model of our billiard balls: Saying that you expect your data to be normally distributed commits you to a certain distribution ‘shape’ and to more and less probable parameter values for your variable.</p>
<p>In figure <a href="probabilities-likelihood-and-inference.html#fig:F2-5">2.5</a> we compare the histogram of apparent height judgments to the density of a normal distribution with a mean equal to the sample mean (<span class="math inline">\(\mu\)</span> = 173.8 cm) and a standard deviation equal to the sample standard deviation (<span class="math inline">\(\sigma\)</span> = 7.8 cm) of our <code>mens_height</code> vector. The density was drawn using the <code>dnorm</code> function, which draws a curve representing the shape of a theoretical normal distribution with a given mean and standard deviation. Clearly, there is a good alignment between our random sample of real-world data and the theoretical normal density. This suggests that we could potentially use the shape of the <em>theoretical</em> normal distribution to talk about the characteristics of our observed random sample of data. Although the distribution of our sample is unlikely to be perfectly normal, it is <em>normal enough</em> to make the comparison worthwhile.</p>
<div class="figure"><span style="display:block;" id="fig:F2-5"></span>
<img src="_main_files/figure-html/F2-5-1.jpeg" alt="The histogram shows the empirical distribution of height judgments for adult male speakers in our data. The shaded area shows the theoretical density of the equivalent normal distribution. The red area corresponds to the theoretical probability of observing a height under 162 cm, based on the shape of the normal density." width="4800" />
<p class="caption">
Figure 2.5: The histogram shows the empirical distribution of height judgments for adult male speakers in our data. The shaded area shows the theoretical density of the equivalent normal distribution. The red area corresponds to the theoretical probability of observing a height under 162 cm, based on the shape of the normal density.
</p>
</div>
<p>The average female over 20 in the United states is 162.1 cm, according to the information in <code>height_data</code>. A vertical line has been placed at this value in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-5">2.5</a>. We might wonder, what is a probability of observing a height judgment for an adult male that is shorter than this average adult female height? Asking this question is equivalent to asking: What is the area under the curve of the density above, to the <em>left</em> of the vertical line? Since the <em>total</em> area of the density is always equal to one, the area of the red portion below corresponds to a percentage/probability of observing values less than 162.1 cm. One way to answer this question is to calculate the empirical probability of observing an apparent height less than 162.1 cm for male speakers in our data. Another way to do this is by calculating the <em>theoretical probability</em> by finding the proportion of values expected to be less than 162.1 cm in the normal distribution that has approximately the same ‘shape’ as our data distribution (i.e. the one seen in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-5">2.5</a>).</p>
<p>Below, we use the function <code>pnorm</code> to find the proportion of values that are expected to be less than 162.1 cm. This function takes in a value, a mean, and a standard deviation. It then tells you the proportion of the distribution that is to the <em>left</em> of (i.e. less than) a given value. Below, we use parameters estimated from our sample to run the <code>pnorm</code> function, as these are our best guesses of the population parameters. The output of this function is equal to the area of the red section in the density in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-5">2.5</a>. As we can see, the theoretical and empirical probabilities are very similar to one another. If we subtract this area from one, we get the area under the curve to the <em>right</em> of the vertical line, the blue section of the density above.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="probabilities-likelihood-and-inference.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># empirical probability of height &lt; 162.1</span></span>
<span id="cb41-2"><a href="probabilities-likelihood-and-inference.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (mens_height <span class="sc">&lt;</span> <span class="fl">162.1</span>)</span>
<span id="cb41-3"><a href="probabilities-likelihood-and-inference.html#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.07704</span></span>
<span id="cb41-4"><a href="probabilities-likelihood-and-inference.html#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="probabilities-likelihood-and-inference.html#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  Red area of distribution, x &lt; 162.1</span></span>
<span id="cb41-6"><a href="probabilities-likelihood-and-inference.html#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span> (<span class="fl">162.1</span>, <span class="fu">mean</span> (mens_height), <span class="fu">sd</span>(mens_height))</span>
<span id="cb41-7"><a href="probabilities-likelihood-and-inference.html#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.0661</span></span>
<span id="cb41-8"><a href="probabilities-likelihood-and-inference.html#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="probabilities-likelihood-and-inference.html#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co">#  Blue area of distribution, x &gt; 162.1</span></span>
<span id="cb41-10"><a href="probabilities-likelihood-and-inference.html#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span> (<span class="fl">162.1</span>, <span class="fu">mean</span> (mens_height), <span class="fu">sd</span>(mens_height))</span>
<span id="cb41-11"><a href="probabilities-likelihood-and-inference.html#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.9339</span></span></code></pre></div>
<p>Imagine you had 1 pound of clay and you were asked to make a shape <em>exactly</em> like the normal density above. This shape should be perfectly flat and should have a constant depth (like a coin). If you had this shape made of clay and used a knife to remove the part to the left of 162.1 cm (the red subsection) and weighed it, it should weigh 6.6% of a pound (0.066 pounds). The ‘area under the curve’ of this clay sculpture would just correspond to the amount of clay in a certain area, and in this case we know that only 6.6% of the clay should be in that section of the shape. The area under the curve, the probability, is just the amount of the <em>stuff</em> in the density that falls below/above a certain point, or between two points. The <code>pnorm</code> function allows you to slice and ‘weigh’ the sections of the distribution to tell you how much of it is in any given interval.</p>
<p>This is what our theoretical probabilities tell us: <em>If</em> height judgments come from a normal distribution, <em>and</em> that distribution has a mean and standard deviation that is close to the sample estimates, <em>then</em> we expect (in the long run) that 6.6% of height judgments will be lower than 162.1 cm. What we are expressing here is effectively a conditional probability, we’re saying <em>if</em> the parameters have certain values, <em>and</em> the probability distribution has a certain shape, <em>then</em> we expect certain height judgments to be more or less probable. Of course, if you change any part of that, either the values of the parameters or the probability distribution, then your estimated theoretical probabilities are likely to change. This is an important thing to keep in mind because it means that inference based on theoretical probabilities can change when our assumptions change, and many of these assumptions cannot be ‘proven’ true or false.</p>
</div>
<div id="c2-likelihoods" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Probabilities of events and likelihoods of parameters<a href="probabilities-likelihood-and-inference.html#c2-likelihoods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’re going to switch from talking about <em>probabilities</em> to talking about <em>likelihoods</em>. When we talk about a probability, we are talking about the probability of observing some particular data/event/outcome, given some parameter(s). A <strong>likelihood</strong> inverts this, and places the focus on different <em>parameter</em> values, given some observed data. For example, you could say “how probable is it that a random man will sound shorter than 162.1 cm in height if the mean is 174 cm?”. When stated this way, we are discussing the probability of observing <em>data</em> (apparent height &lt; 162.1) from some fixed distribution. So, probability puts the <em>data</em> in question and takes the distribution for granted.</p>
<p>In contrast, you could ask “how likely is it that the average man sounds 162.1 cm tall, given some observations?”. Now, we are discussing the likelihood of a particular mean <em>parameter</em> (<span class="math inline">\(\mu\)</span> = 162.1) given some fixed data. So, likelihood puts the <em>parameters</em> in question and takes the data for granted. The likelihood of a parameter represents the joint probability (density) of observing all the data you observed, given specific parameter values. In other words, the likelihood relates to the probability of your first observation, <em>and</em> your second observation, <em>and</em> your third observation, and so on, for all observations, given the parameter value(s) of interest.</p>
<p>The <strong>likelihood function</strong> is a curve showing the relative likelihoods of different parameter values, given a fixed set of observations/data. The likelihood function tells you what parameter values are <em>credible</em> given your data. If a value is very unlikely, that means that it is not supported by your data. In other words, unlikely parameter estimates represent conclusions that your data is rejecting as not viable, and hence they are not credible. Here’s a simple example of how you use likelihoods informally in your everyday life. Suppose your friend tells you they can hit about 90% of their 3-point shots in basketball. You know this friend is prone to making spectacular claims, so you ask to watch them shoot. They heave up 100 shots, sinking 20/100, not too bad for an amateur, but not 90%. Do you believe that your friend sinks 90% of their shots overall? If you decide not to, it may be because their claim is extremely <em>unlikely</em>: A distribution which generates 90% successful shots is extremely unlikely to generate only 30 successes in 100 shots. In other words, given the data you have, a ‘real’ ability to score 90% of shots does not seem credible.</p>
<p>Every parameter for every probability distribution has a likelihood function, given some data. Here, we’re only going to discuss the likelihood of the normal mean parameter, <span class="math inline">\(\mu\)</span>, in detail. The likelihood of the sample mean reflects the joint probability of observing all of your data, given different values of the mean, for a given standard deviation. An example of how this is calculated is given in Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a>. The left plot shows the likelihood function for <span class="math inline">\(\mu\)</span> based on a random sample of ten height judgments from our data (indicated by the blue points at the bottom of the plot). We can see that the most <em>likely</em> values of <span class="math inline">\(\mu\)</span> are centered on the bulk of the observations, and that values become less likely as we deviate from them. The vertical dotted lines indicate three possible mean values that will be highlighted in this discussion.</p>
<div class="figure"><span style="display:block;" id="fig:F2-6"></span>
<img src="_main_files/figure-html/F2-6-1.jpeg" alt="(left) The red curve indicates the likelihood of the population mean given the blue points in the figure. The vertical lines indicate three different parameter values that will be considered. (right) The red curve indicates the probability density given an assumed mean of 174 cm (the sample mean). Vertical lines highlight the density over each point." width="4800" />
<p class="caption">
Figure 2.6: (left) The red curve indicates the likelihood of the population mean given the blue points in the figure. The vertical lines indicate three different parameter values that will be considered. (right) The red curve indicates the probability density given an assumed mean of 174 cm (the sample mean). Vertical lines highlight the density over each point.
</p>
</div>
<p>The likelihood of a parameter value (e.g., <span class="math inline">\(\mu\)</span> = 174 cm in the right plot of Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a>) is equal to the product of the density of each observation in the sample, given the value of the parameter. This sounds like a mouthful but is actually deceptively simple. For example, to calculate the likelihood that <span class="math inline">\(\mu=174\)</span>, we:</p>
<ol style="list-style-type: decimal">
<li><p>Assume that the data is generated by a normal distribution with <span class="math inline">\(\mu=174\)</span> and <span class="math inline">\(\sigma\)</span> equal to your sample standard deviation (7.8 cm).</p></li>
<li><p>Find the height of the curve of the probability density over each point (indicated by the vertical lines in the right plot in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a>). This reflects the relative probability of each observation given your parameter value.</p></li>
<li><p>The joint probability of all of the observations (the likelihood) is the product of all of these densities (heights). This assumes that all of your observations are statistically independent of each other (see section <a href="probabilities-likelihood-and-inference.html#c2-joint">2.3.2</a>).</p></li>
</ol>
<p>So, the value of the likelihood function in the left plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a> at 174 cm is equal to the product of the densities over the points in the right plot (i.e., the heights of the lines in the plot). Imagine we did this for a range of values along the <span class="math inline">\(x\)</span> axis, sliding our probability distribution along the x axis and recording the likelihood values at each step. If we do this and then plot the product of the densities for each corresponding <span class="math inline">\(x\)</span> value the result would be a curve identical to that of the left plot in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a>.</p>
<p>Let’s discuss what the likelihood in in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a> <em>means</em>. When you conduct an experiment, you <em>know</em> you have your data, but you don’t know much else. Suppose all your data was the blue points in the figure. If someone said to you “hey do you think the mean is 140 cm?” you might respond “that is unlikely”. It is unlikely because the observations you have are extremely <em>improbable</em> given a probability distribution centered at 140 cm. Since your data are incompatible with a mean of 140 cm, and you <em>know</em> you have your data, a mean of 140 cm does not seem credible given your probability model. This information is reflected by the very low value of the likelihood function at 140 cm. In contrast, based on the high value of the likelihood function, we can see that a mean of 174 cm <em>is</em> likely. This is because the data you have is very probable given a mean of 174 cm, making this parameter value consistent with our data.</p>
<p>The right plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a> shows the probability of points assuming that the population mean is equal to the sample mean (for our tiny sample). We can see in the left plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a> that this is the most likely value for the mean parameter. When we said earlier that the sample mean is the ‘best’ estimate of the population mean, what we really meant was that the sample mean is the <strong>maximum-likelihood estimate</strong> of the population mean. This means that the sample mean provides an estimate of <span class="math inline">\(\mu\)</span> that maximizes the value of the likelihood function given the data. This is related to the fact that, as mentioned in section <a href="probabilities-likelihood-and-inference.html#c2-sample-mean">2.5.1</a>, the sample mean minimizes the variance of the sample. As a result, if you want to know which mean estimate is most likely given your data, you simply need to calculate the sample mean as in equation <a href="probabilities-likelihood-and-inference.html#eq:2-1">(2.3)</a>.</p>
<p>In the left plot of figure <a href="probabilities-likelihood-and-inference.html#fig:F2-7">2.7</a> we see that a normal distribution with a <span class="math inline">\(\mu\)</span> of 170 cm is a reasonable fit to the data. However, several observations are very improbable and this relative lack of fit is reflected by the low value of the likelihood function at 170 in Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a>. In the right plot of Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-7">2.7</a> we see that a normal distribution with a mean of 160 is very unlikely to generate this data: Many points are extremely improbable and have densities close to zero. Correspondingly, the value of the likelihood corresponding to <span class="math inline">\(\mu=160\)</span> in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-6">2.6</a> reflects a very unlikely parameter value.</p>
<div class="figure"><span style="display:block;" id="fig:F2-7"></span>
<img src="_main_files/figure-html/F2-7-1.jpeg" alt="(left) The red curve indicates the probability density given an assumed mean of 170 cm (the sample mean). Vertical lines highlight the density over each point. (right) The same information as in the left plot, but given a mean of 160 cm." width="4800" />
<p class="caption">
Figure 2.7: (left) The red curve indicates the probability density given an assumed mean of 170 cm (the sample mean). Vertical lines highlight the density over each point. (right) The same information as in the left plot, but given a mean of 160 cm.
</p>
</div>
<p>We want to talk about why it makes sense to multiply densities to calculate joint probabilities. Above, we stated that probabilities relate to the area under the curve of the density function. A problem we have is that the area under the curve of a single point is always zero. This is because a single point is so thin (it’s width is zero) that the area under the curve under the point will equal zero regardless of its density. Suppose that we said “ok let’s agree to use a fixed width, <span class="math inline">\(a\)</span>, around our point to calculate our area under the curve”. We can make <span class="math inline">\(a\)</span> so teeny tiny that it is almost as if we were calculating just the area under our single point. We can use <span class="math inline">\(f(x)\)</span> to represent the height of a density for a given value of <span class="math inline">\(x\)</span>. This means that we could approximate the value of the area under the curve at <span class="math inline">\(x\)</span> by treating it like a trapezoid and finding <span class="math inline">\(f(x)*a\)</span>. If we wanted to calculate the joint probability of a number of observations, we could multiply a series of these areas, as seen in <a href="probabilities-likelihood-and-inference.html#eq:2-10">(2.11)</a>.</p>
<p><span class="math display" id="eq:2-10">\[
\begin{equation}
P(x_1\,\&amp;\,x_2\,\&amp;\, \dots \&amp; \, x_n)=[f(x_1) \cdot a] \cdot [f(x_2) \cdot a]  \cdot  \dots \cdot [f(x_n) \cdot a]
\tag{2.11}
\end{equation}
\]</span></p>
<p>Since each term in <a href="probabilities-likelihood-and-inference.html#eq:2-10">(2.11)</a> contains <span class="math inline">\(a\)</span>, this can be factored out as in <a href="probabilities-likelihood-and-inference.html#eq:2-11">(2.12)</a>.</p>
<p><span class="math display" id="eq:2-11">\[
\begin{equation}
P(x_1\,\&amp;\,x_2\,\&amp;\, \dots \&amp; \, x_n)=[f(x_1) \cdot f(x_2) \cdot \dots \cdot f(x_n)] \cdot a^n
\tag{2.12}
\end{equation}
\]</span></p>
<p>At this point, we see that the joint probability of a set of values (<span class="math inline">\(x_n\)</span>) can be thought of as the product of the densities over those values (<span class="math inline">\(f(x_n)\)</span>) times some arbitrary constant <span class="math inline">\(a^n\)</span>. If we agree to use the same constant for all our calculations, we can ignore it and use the product of the densities to evaluate the relative probabilities of different combinations of observations.</p>
<div id="c2-chars-of-likelihoods" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Characteristics of likelihoods<a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It’s fairly important to understand at least the gist of the previous section: Likelihoods tell you which parameter values are ‘likely’ (i.e. credible) given the data you have. It’s less important that you understand the information from here to section <a href="probabilities-likelihood-and-inference.html#c2-inference-and-likelihood">2.8</a>. We’re presenting it here to take the mystery out of it, and because in order to understand it you will probably need to read about it several times so you might as well get started now. If this is the first time you read about these topics, you should accept the fact that you may not <em>understand</em> much of this, however, given time and effort you will eventually <em>get used to it</em>.</p>
<p>We can think about the characteristics of the likelihood of <span class="math inline">\(\mu\)</span> to make some predictions about its expected shape given different numbers of observations and underlying data distributions. The likelihood is the joint probability of your data given some parameter values. We can calculate the joint probability of two observations <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> by multiplying their individual densities as in equation <a href="probabilities-likelihood-and-inference.html#eq:2-12">(2.13)</a>, assuming these observations are independent.</p>
<p><span class="math display" id="eq:2-12">\[
\begin{equation}
f(x_1,x_2) = [\frac{1}{\sigma\sqrt{2\pi}} \exp( -\frac{1}{2\sigma^2}(x_1-\mu)^{\!2}\,)]
\cdot
[\frac{1}{\sigma\sqrt{2\pi}} \exp( -\frac{1}{2\sigma^2}(x_2-\mu)^{\!2}\,)]
\tag{2.13}
\end{equation}
\]</span></p>
<p>Equation <a href="probabilities-likelihood-and-inference.html#eq:2-12">(2.13)</a> defines the likelihood of <span class="math inline">\(\mu\)</span> given the data <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and some <span class="math inline">\(\sigma\)</span>. We will update the left-hand side to reflect this, replacing a term representing the joint probability of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (<span class="math inline">\(f(x_1,x_2)\)</span>) with a term representing the likelihood of the mean given the data, <span class="math inline">\(\mathcal{L}_{(\mu|x)}\)</span>. Notice that nothing has changed except our perspective. In equation <a href="probabilities-likelihood-and-inference.html#eq:2-12">(2.13)</a> we treat the parameter <span class="math inline">\(\mu\)</span> as fixed and use it to calculate the joint probability of some data. In equation <a href="probabilities-likelihood-and-inference.html#eq:2-13">(2.14)</a> we treat the data <span class="math inline">\(x\)</span> as fixed and use it to calculate the likelihood of values of <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display" id="eq:2-13">\[
\begin{equation}
\mathcal{L}_{(\mu|x)} = [\frac{1}{\sigma\sqrt{2\pi}} \exp( -\frac{1}{2\sigma^2}(x_1-\mu)^{\!2}\,)] \cdot
                        [\frac{1}{\sigma\sqrt{2\pi}} \exp( -\frac{1}{2\sigma^2}(x_2-\mu)^{\!2}\,)]
\tag{2.14}
\end{equation}
\]</span></p>
</div>
<div id="c2-logarithms" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> A brief aside on logarithms<a href="probabilities-likelihood-and-inference.html#c2-logarithms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>logarithm</strong> (<code>log</code>) is the inverse function to exponentiation, it basically <em>erases</em> or <em>undoes</em> exponentiation. We can apply a logarithmic transformation to both sides of the density in equation <a href="probabilities-likelihood-and-inference.html#eq:2-7">(2.8)</a>, resulting in the <strong>log density</strong> of the normal distribution seen in equation <a href="probabilities-likelihood-and-inference.html#eq:2-14">(2.15)</a>.</p>
<p><span class="math display" id="eq:2-14">\[
\begin{equation}
\log(f(x)) =  \log(\frac{1}{\sigma\sqrt{2\pi}})-\frac{1}{2\sigma^2}\left(x-\mu\right)^2
\tag{2.15}
\end{equation}
\]</span></p>
<p>Understanding logarithms and log densities is important because, in practice, much of statistics is done on the logarithms of probabilities, likelihoods, and densities. In addition, it is extremely common to log-transform probability density functions or other equations before manipulating them, making a good understanding of the properties of logarithms extremely useful. As a result, as we will see below, using logarithms can help us simplify and understand at least some probability distributions.</p>
<p>Before explaining equation <a href="probabilities-likelihood-and-inference.html#eq:2-14">(2.15)</a>, we will discuss some basic properties of logarithms that are useful to understand probabilities and probability distributions, as these often involve exponentiation and logarithms. The first line in <a href="probabilities-likelihood-and-inference.html#eq:2-15">(2.16)</a> shows the basic behavior of logarithms. The next four lines pertain to the values expected, or undefined, for different <span class="math inline">\(x\)</span>. The next two lines highlight the fact that exponentiation of numbers is equivalent to the multiplication of their logarithms. The final two lines highlight the fact that multiplication of two numbers is equivalent to the addition of the logarithms of the numbers.</p>
<p><span class="math display" id="eq:2-15">\[
\begin{equation}
\begin{split}
\log(e^x)=x \\ \\
\log(1) = 0 \\
\textrm{if} \: x &lt; 1, \: \log(x) &lt; 0 \\
\textrm{if} \: x &gt; 1, \: \log(x) &gt; 0 \\
\textrm{if} \: x &lt; 0, \: \log(x) = \textrm{undefined} \\ \\
\log(x^y)=\log(x) \cdot y \\
\log(\sqrt[y]{x})=\log(x)/y \\ \\
\log(x)+\log(y)= log(x \cdot y) \\
\log(x)-\log(y)= log(x/y) \\
\end{split}
\tag{2.16}
\end{equation}
\]</span></p>
<p>Armed with knowledge of the behavior of logarithms, we can see that compared to equation <a href="probabilities-likelihood-and-inference.html#eq:2-7">(2.8)</a>, equation <a href="probabilities-likelihood-and-inference.html#eq:2-14">(2.15)</a> reflects the following changes associated with taking the logarithm of both sides of the equation. First, we remove the <span class="math inline">\(\exp\)</span> function around the rightmost term <span class="math inline">\(-\frac{1}{2\sigma^2}\left(x-\mu\right)^2\)</span>, and added the <span class="math inline">\(\log\)</span> function around all terms that were <em>not</em> previously exponentiated. Then, multiplication of the two terms in the right hand side of the equation turns into addition, or subtraction in this case because we are adding a negative term.</p>
<p>The properties of logarithms presented above can be used to make the simplifications presented in <a href="probabilities-likelihood-and-inference.html#eq:2-16">(2.17)</a>. When presented in its final form, this function is that of a parabola in vertex form, <span class="math inline">\(y=a(x-h)^2+k\)</span>, where <span class="math inline">\(a=-1/2\sigma^2\)</span>, <span class="math inline">\(h=\mu\)</span>, and <span class="math inline">\(h=-\log({\sigma\sqrt{2\pi}})\)</span>. So, we can see that the density of the normal distribution is just an exponentiated parabola that is scaled by <span class="math inline">\(1 / \sigma\sqrt{2\pi}\)</span> so that the area under the curve is equal to one.</p>
<p><span class="math display" id="eq:2-16">\[
\begin{equation}
\begin{split}
\log(f(x)) =  \log(\frac{1}{\sigma\sqrt{2\pi}})-\frac{1}{2\sigma^2}\left(x-\mu\right)^2 \\
\log(f(x)) =  \log(1)-\log(\sigma\sqrt{2\pi})-\frac{1}{2\sigma^2}\left(x-\mu\right)^2 \\
\log(f(x)) =  0-\log(\sigma\sqrt{2\pi})-\frac{1}{2\sigma^2}\left(x-\mu\right)^2 \\
\log(f(x)) =  -\frac{1}{2\sigma^2}\left(x-\mu\right)^2 - \log({\sigma\sqrt{2\pi}})
\end{split}
\tag{2.17}
\end{equation}
\]</span></p>
<p>The parabola defined in <a href="probabilities-likelihood-and-inference.html#eq:2-16">(2.17)</a> has its vertex at <span class="math inline">\((\mu, -\log({\sigma\sqrt{2\pi}})\)</span>, and opens downwards since the <span class="math inline">\(a\)</span> term is negative. When a parabola is in vertex form, the relationship between parabola width and the value of <span class="math inline">\(a\)</span> is inverted, its width decreases as the value of <span class="math inline">\(a\)</span> increases. However, because <span class="math inline">\(a=-\frac{1}{2\sigma^2}\)</span>, the relationship is doubly inverted. As a result, in the case of normal distributions the parabola width increases as <span class="math inline">\(\sigma\)</span> grows larger, leading to wider parabolas and wider probability densities.</p>
<p>Equation <a href="probabilities-likelihood-and-inference.html#eq:2-16">(2.17)</a> shows how the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> parameters work to make observations further from the mean less probable. First, we know that negative logarithmic values will fall between 0 and 1, with more negative values being closer to zero (i.e. less probable). As observations (<span class="math inline">\(x\)</span>) are further from the mean, the value of <span class="math inline">\((x-\mu)^2\)</span> will be greater so that values further from the mean will be generally less probable. However, whether a deviation is considered “big” or “small” is relative, and so this distance is scaled with respect to the average expected squared deviation from the mean (i.e. the variance <span class="math inline">\(\sigma^2\)</span>). Variation of 1 cm in body length means different things for an earthworm as opposed to an anaconda. As a result, large values of <span class="math inline">\((x-\mu)^2\)</span> can be offset by large values of <span class="math inline">\(\sigma^2\)</span> when determining the probability of an outcome.</p>
</div>
<div id="c2-chars-of-likelihoods-2" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Characteristics of likelihoods, continued<a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will pick up where we left off, considering the likelihood of <span class="math inline">\(\mu\)</span> given some data. This was presented in equation <a href="probabilities-likelihood-and-inference.html#eq:2-13">(2.14)</a> and is presented again as <a href="probabilities-likelihood-and-inference.html#eq:2-17">(2.18)</a>.</p>
<p><span class="math display" id="eq:2-17">\[
\begin{equation}
\mathcal{L}_{(\mu|x)} = [\frac{1}{\sigma\sqrt{2\pi}} \exp( -\frac{1}{2\sigma^2}(x_1-\mu)^{\!2}\,)] \cdot
                        [\frac{1}{\sigma\sqrt{2\pi}} \exp( -\frac{1}{2\sigma^2}(x_2-\mu)^{\!2}\,)]
\tag{2.18}
\end{equation}
\]</span></p>
<p>We can log-transform both sides of equation <a href="probabilities-likelihood-and-inference.html#eq:2-17">(2.18)</a> so that each bracketed element in the right hand side of <a href="probabilities-likelihood-and-inference.html#eq:2-17">(2.18)</a> is equal to the final version of equation <a href="probabilities-likelihood-and-inference.html#eq:2-16">(2.17)</a>. This is presented in equation <a href="probabilities-likelihood-and-inference.html#eq:2-18">(2.19)</a>.</p>
<p><span class="math display" id="eq:2-18">\[
\begin{equation}
\mathcal{L}\mathcal{L}_{(\mu|x)} =
[-\frac{1}{2\sigma^2}(x_1-\mu)^2 - \log({\sigma\sqrt{2\pi}})] +
[-\frac{1}{2\sigma^2}(x_2-\mu)^2 - \log({\sigma\sqrt{2\pi}})]
\tag{2.19}
\end{equation}
\]</span></p>
<p>Because they make working with likelihoods much simpler, statisticians often use the logarithms of likelihood functions, referred to as <strong>log-likelihoods</strong>. Log-likelihoods are denoted using the symbol <span class="math inline">\(\mathcal{L}\mathcal{L}_{(\mu|x)}\)</span>. When you see this, <span class="math inline">\(\mathcal{L}\mathcal{L}_{(\mu|x)}\)</span>, just think “the logarithm of the likelihood of the mean given the data <span class="math inline">\(x\)</span>”. In equation <a href="probabilities-likelihood-and-inference.html#eq:2-18">(2.19)</a> we see that the log-likelihood of these two observations is the sum of two parabolas. Since the sum of parabolas will (almost always) also be a parabola, we know that the log-likelihood of the mean given these, and any number of other observations, will also be a parabola. As a result, we see that the (non-log) likelihood of the mean is an exponentiated parabola and has the same general shape as the normal distribution.</p>
<p>Equation <a href="probabilities-likelihood-and-inference.html#eq:2-18">(2.19)</a> is specifically for two variables, however, Because each term is identical except for <span class="math inline">\(x_n-\mu\)</span>, the log-likelihood function can be greatly simplified as in equation <a href="probabilities-likelihood-and-inference.html#eq:2-19">(2.20)</a>. In <a href="probabilities-likelihood-and-inference.html#eq:2-19">(2.20)</a> we can see that the log-likelihood of <span class="math inline">\(\mu\)</span> given <span class="math inline">\(n\)</span> observations of <span class="math inline">\(x\)</span> is equal to the sum of the individual squared deviations from the mean, divided by <span class="math inline">\(-1 / 2\sigma^2\)</span>, with the value <span class="math inline">\(n \cdot (\log({\sigma\sqrt{2\pi}}))\)</span> subtracted from it. So, we see that the log-likelihood is a parabola that has its vertex at <span class="math inline">\((\hat{\mu}, - n \cdot (\log({\sigma\sqrt{2\pi}}))\)</span>, meaning that this parabola has its maximum value when <span class="math inline">\(\mu\)</span> is equal to the sample mean. This again reflects the fact that the sample mean is the maximum-likelihood estimate for the population mean.</p>
<p><span class="math display" id="eq:2-19">\[
\begin{equation}
\mathcal{L}\mathcal{L}_{(\mu|x)} = -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2 - n \cdot (\log({\sigma\sqrt{2\pi}}))
\tag{2.20}
\end{equation}
\]</span></p>
<p>There are two mechanisms by which the likelihood function may get narrower or wider. The first was discussed in section <a href="probabilities-likelihood-and-inference.html#c2-normal-density">2.5.3</a>, where a smaller value of <span class="math inline">\(\sigma\)</span> results in a narrower parabola and a narrower likelihood. Independently of the underlying variation in the data, likelihood functions can also become narrower when the sample size is increased. To see why this is the case, consider what happens to the relative value of our <span class="math inline">\(a\)</span> parameter (<span class="math inline">\(-1 / 2\sigma^2\)</span>) as the sample size grows. Imagine that we are calculating the sum of squared deviations about our sample mean. We know that the average squared variation we expect from our mean is equal to the variance. So, let’s replace <span class="math inline">\((x_i-\mu)^2\)</span> in <a href="probabilities-likelihood-and-inference.html#eq:2-19">(2.20)</a> with <span class="math inline">\(\sigma^2\)</span> in <a href="probabilities-likelihood-and-inference.html#eq:2-20">(2.21)</a>. Now, instead of adding <em>n</em> squared deviations around the mean, <span class="math inline">\(\sum_{i=1}^{n}(x_i-\mu)^2\)</span>, we are multiplying our expected squared deviation by <em>n</em>, <span class="math inline">\(n \cdot \sigma^2\)</span>.</p>
<p><span class="math display" id="eq:2-20">\[
\begin{equation}
\mathcal{L}\mathcal{L}_{(\mu|x)} = -\frac{1}{2\sigma^2} \cdot (n \cdot \sigma^2) - n \cdot (\log({\sigma\sqrt{2\pi}}))
\tag{2.21}
\end{equation}
\]</span></p>
<p>We can also move the <span class="math inline">\(\sigma^2\)</span> over from under the <span class="math inline">\(-1/2\)</span> to make the following point simpler, as in <a href="probabilities-likelihood-and-inference.html#eq:2-21">(2.22)</a>.</p>
<p><span class="math display" id="eq:2-21">\[
\begin{equation}
\mathcal{L}\mathcal{L}_{(\mu|x)} = -\frac{1}{2} \cdot \frac{n \cdot \sigma^2}{\sigma^2} - n \cdot (\log({\sigma\sqrt{2\pi}}))
\tag{2.22}
\end{equation}
\]</span></p>
<p>Multiplying the numerator of a fraction is exactly equivalent to dividing the denominator of the fraction by the same amount, e.g. <span class="math inline">\((x \cdot y)/z = x/(z/y)\)</span>. Thus, we see that calculating the likelihood using <span class="math inline">\(n\)</span> data points is expected to have about the same effect on parabola width as dividing the data variance by <span class="math inline">\(n\)</span>, as shown in <a href="probabilities-likelihood-and-inference.html#eq:2-22">(2.23)</a>. As a result, increasing numbers of observations reduces the uncertainty in parameter estimates by making the likelihood narrower and narrower for any given underlying <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display" id="eq:2-22">\[
\begin{equation}
\mathcal{L}\mathcal{L}_{(\mu|x)} = -\frac{1}{2} \cdot \frac{\sigma^2}{(\sigma^2/n)} - n \cdot (\log({\sigma\sqrt{2\pi}}))
\tag{2.23}
\end{equation}
\]</span></p>
</div>
</div>
<div id="c2-inference-and-likelihood" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Answering our research questions<a href="probabilities-likelihood-and-inference.html#c2-inference-and-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In section <a href="probabilities-likelihood-and-inference.html#c2-models-and-inference">2.6</a> we discussed using the normal distribution to make inferences about the probable values of a random variable. When variables are normally distributed we can use the theoretical normal distribution and functions such as <code>pnorm</code> to answer questions about values we expect, and don’t expect, to see. In the same way, we can use likelihood functions to understand probable, and improbable, values of parameters given our data.</p>
<p>For example, suppose that you measured the heights of 100 women in a small town (pop. 1500) and found the average height was 160 cm, with a standard deviation of 6 cm. You might accept that the <em>actual</em> population average is 161 cm, but may find it difficult to accept that it was actually 180 cm. This is because a true mean of 180 cm is <em>unlikely</em> given your observed data: The observations you have are <em>improbable</em> given a true mean of 180 cm. The logic is quite simple: A true mean of 180 cm is unlikely to result in a random selection of so many women around 160 cm. You <em>know</em> you observed the short women, therefore, you have no reason to believe that the true mean is 180 cm.</p>
<p>Below we calculate the likelihood of different mean parameters given our data. We do this by finding the log-density of each observation and then adding the points together. Log-densities are used because the likelihood is often a number so small that computers have a hard time representing them otherwise.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="probabilities-likelihood-and-inference.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make candidates for mean parameter</span></span>
<span id="cb42-2"><a href="probabilities-likelihood-and-inference.html#cb42-2" aria-hidden="true" tabindex="-1"></a>mus <span class="ot">=</span> <span class="fu">seq</span> (<span class="fl">172.5</span>,<span class="dv">175</span>, .<span class="dv">01</span>)</span>
<span id="cb42-3"><a href="probabilities-likelihood-and-inference.html#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="probabilities-likelihood-and-inference.html#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># easy way to make zero vector of same length as above</span></span>
<span id="cb42-5"><a href="probabilities-likelihood-and-inference.html#cb42-5" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="ot">=</span> mus<span class="sc">*</span><span class="dv">0</span></span>
<span id="cb42-6"><a href="probabilities-likelihood-and-inference.html#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="probabilities-likelihood-and-inference.html#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># add the log-density of all observations. Notice only the </span></span>
<span id="cb42-8"><a href="probabilities-likelihood-and-inference.html#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># mean changes across iterations of the for loop.</span></span>
<span id="cb42-9"><a href="probabilities-likelihood-and-inference.html#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(mus)) log_likelihood[i] <span class="ot">=</span> </span>
<span id="cb42-10"><a href="probabilities-likelihood-and-inference.html#cb42-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span> (<span class="fu">dnorm</span> (mens_height, mus[i], <span class="fu">sd</span>(mens_height), <span class="at">log =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<p>For example, the highest point of the probability densities in figure <a href="probabilities-likelihood-and-inference.html#fig:F2-7">2.7</a> is about 0.07. Let’s pretend it’s 0.1 for the sake of simplicity. Recall that to find the likelihood we need to multiply the densities above each of the points. This means that the likelihood of two observations at the mean is <span class="math inline">\(0.1 \cdot 0.1=0.001\)</span>, and the likelihood of observing <span class="math inline">\(n\)</span> observations at the mean is equal to <span class="math inline">\(0.1^n\)</span>. Since we have 675 observations in our <code>mens_height</code> vector, the likelihood of observing every one of those at the mean, the most probable outcome, would be equal to <span class="math inline">\(0.1^{675}\)</span>, or a decimal point followed by 674 zeros and then a one. By relying on the logarithms of densities instead, we can accurately represent very small numbers more comfortably. This is because adding together the logarithms of two numbers is equivalent to multiplying those numbers, and multiplying a logarithm by another number is equivalent to raising it to that power (see <a href="probabilities-likelihood-and-inference.html#eq:2-6">(2.7)</a>). For example, the number <span class="math inline">\(0.1^{675}\)</span> can also be expressed like <span class="math inline">\(\log(0.1) \cdot 675\)</span>, which equals -1554.245 (i.e. <span class="math inline">\(0.1^{675}=e^{-1554.245}\)</span>).This is obviously a much easier number to deal with than one with 675 decimal places.</p>
<p>In figure <a href="probabilities-likelihood-and-inference.html#fig:F2-8">2.8</a> we plot the log likelihood calculated above, and the ‘scaled likelihood’. This is just the likelihood that has been scaled so that its peak is equal to one, and the peak of the log-likelihood equals zero. This allows us to actually plot and consider the likelihood function, though the values of the density no longer reflect the actual values of the likelihood. However, the <em>relational</em> characteristics are maintained by this scaling. So, a scaled likelihood value of 0.2 is still five times less likely than a value of one.</p>
<div class="figure"><span style="display:block;" id="fig:F2-8"></span>
<img src="_main_files/figure-html/F2-8-1.jpeg" alt="(left) The curve represents the log-likelihood for different values of our mean given our apparent height judgments and assuming a normal distribution. (right) The likelihood implied by the log-likelihood on the left. The curve has been scaled to have a peak of 1 so that we can plot it. For example, exp (-2300) is a value so small that it is difficult to represent in a figure axis." width="4800" />
<p class="caption">
Figure 2.8: (left) The curve represents the log-likelihood for different values of our mean given our apparent height judgments and assuming a normal distribution. (right) The likelihood implied by the log-likelihood on the left. The curve has been scaled to have a peak of 1 so that we can plot it. For example, exp (-2300) is a value so small that it is difficult to represent in a figure axis.
</p>
</div>
<p>At this point we can provide answers to the questions posed in section <a href="probabilities-likelihood-and-inference.html#c2-data">2.2</a>. The questions were:</p>
<p>(Q1) How tall does the average adult male sound?</p>
<p>(Q2) Can we set limits on credible average apparent heights based on the data we collected?</p>
<p>Below, we see that the <strong>maximum likelihood estimate</strong>, the value of the parameter which maximizes the likelihood, for the mean corresponds to the sample mean:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="probabilities-likelihood-and-inference.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find index number of highest values in log-likelihood</span></span>
<span id="cb43-2"><a href="probabilities-likelihood-and-inference.html#cb43-2" aria-hidden="true" tabindex="-1"></a>maximum <span class="ot">=</span> <span class="fu">which.max</span>(scaled_log_likelihood)</span>
<span id="cb43-3"><a href="probabilities-likelihood-and-inference.html#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="probabilities-likelihood-and-inference.html#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print and compare to sample mean</span></span>
<span id="cb43-5"><a href="probabilities-likelihood-and-inference.html#cb43-5" aria-hidden="true" tabindex="-1"></a>mus[maximum]</span>
<span id="cb43-6"><a href="probabilities-likelihood-and-inference.html#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 173.8</span></span>
<span id="cb43-7"><a href="probabilities-likelihood-and-inference.html#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (mens_height)</span>
<span id="cb43-8"><a href="probabilities-likelihood-and-inference.html#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 173.8</span></span></code></pre></div>
<p>So, we may conclude that the average male speaker is <em>most likely</em> to sound about 174 cm tall. We can also conclude informally based on Figure <a href="probabilities-likelihood-and-inference.html#fig:F2-8">2.8</a> that the most likely mean values fall between (approximately) 173 and 174.5 cm. This means that although the sample mean was 174 cm, it is reasonable that the true population mean might actually be 173.5 cm. This is because mean parameters in this range are also reasonably <em>likely</em> given our data. Basically, maybe our sample mean is wrong and 173.5 cm is the true population <span class="math inline">\(\mu\)</span>. This outcome is compatible with our data. However, a value of 172 cm is very <em>unlikely</em> given our data. Since we think that 172 cm is not a plausible mean parameter given our sample, we can rule it out as a credible value for the <span class="math inline">\(\mu\)</span> parameter of our variable. Using this approach, we can use the information in likelihood functions to rule out implausible values of <span class="math inline">\(\mu\)</span> based on the characteristics of our data.</p>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Exercises<a href="probabilities-likelihood-and-inference.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Use the techniques outlined in this chapter to start thinking about credible estimates, and bounds, of the mean apparent height for other groups of speakers. This can be done for the original or modified voice resonance conditions.</p>
</div>
<div id="references-1" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> References<a href="probabilities-likelihood-and-inference.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ross, S. M. (2019). A first course in probability. Boston: Pearson.</p>
<p>Parzen, E. (1960). Modern probability theory and its applications. Wiley.</p>
<p>Jaynes, E. T. (2003). Probability theory: The logic of science. Cambridge university press.</p>
</div>
<div id="plot-code-1" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Plot Code<a href="probabilities-likelihood-and-inference.html#plot-code-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="probabilities-likelihood-and-inference.html#cb44-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-2"><a href="probabilities-likelihood-and-inference.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-3"><a href="probabilities-likelihood-and-inference.html#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.1</span></span>
<span id="cb44-4"><a href="probabilities-likelihood-and-inference.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-5"><a href="probabilities-likelihood-and-inference.html#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="probabilities-likelihood-and-inference.html#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,.<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>), <span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb44-7"><a href="probabilities-likelihood-and-inference.html#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="fu">layout</span> (<span class="at">mat =</span> <span class="fu">t</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)), <span class="at">widths =</span> <span class="fu">c</span>(.<span class="dv">15</span>,.<span class="dv">85</span>))</span>
<span id="cb44-8"><a href="probabilities-likelihood-and-inference.html#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span> (men<span class="sc">$</span>height, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">col=</span>lavender, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">135</span>,<span class="dv">200</span>),</span>
<span id="cb44-9"><a href="probabilities-likelihood-and-inference.html#cb44-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">width=</span><span class="dv">2</span>)</span>
<span id="cb44-10"><a href="probabilities-likelihood-and-inference.html#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>,<span class="at">text=</span><span class="st">&quot;Apparent height (cm)&quot;</span>,<span class="at">line=</span><span class="dv">3</span>)</span>
<span id="cb44-11"><a href="probabilities-likelihood-and-inference.html#cb44-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-12"><a href="probabilities-likelihood-and-inference.html#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span> (height <span class="sc">~</span> L, <span class="at">data =</span> men, <span class="at">ylab =</span> <span class="st">&quot;Apparent height (cm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Listener&quot;</span>,</span>
<span id="cb44-13"><a href="probabilities-likelihood-and-inference.html#cb44-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> bmmb<span class="sc">::</span>cols,<span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">135</span>,<span class="dv">200</span>))</span>
<span id="cb44-14"><a href="probabilities-likelihood-and-inference.html#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb44-15"><a href="probabilities-likelihood-and-inference.html#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span> (height <span class="sc">~</span> L, <span class="at">data =</span> men, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,<span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb44-16"><a href="probabilities-likelihood-and-inference.html#cb44-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> bmmb<span class="sc">::</span>cols, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb44-17"><a href="probabilities-likelihood-and-inference.html#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="probabilities-likelihood-and-inference.html#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-19"><a href="probabilities-likelihood-and-inference.html#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.2</span></span>
<span id="cb44-20"><a href="probabilities-likelihood-and-inference.html#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-21"><a href="probabilities-likelihood-and-inference.html#cb44-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-22"><a href="probabilities-likelihood-and-inference.html#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,.<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>), <span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb44-23"><a href="probabilities-likelihood-and-inference.html#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="fu">layout</span> (<span class="at">mat =</span> <span class="fu">t</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)), <span class="at">widths =</span> <span class="fu">c</span>(.<span class="dv">15</span>,.<span class="dv">85</span>))</span>
<span id="cb44-24"><a href="probabilities-likelihood-and-inference.html#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="probabilities-likelihood-and-inference.html#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span> (men<span class="sc">$</span>dur, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">col=</span>lavender, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">120</span>,<span class="dv">370</span>),</span>
<span id="cb44-26"><a href="probabilities-likelihood-and-inference.html#cb44-26" aria-hidden="true" tabindex="-1"></a>         <span class="at">width=</span><span class="dv">2</span>)</span>
<span id="cb44-27"><a href="probabilities-likelihood-and-inference.html#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>,<span class="at">text=</span><span class="st">&quot;Duration (ms)&quot;</span>,<span class="at">line=</span><span class="dv">3</span>)</span>
<span id="cb44-28"><a href="probabilities-likelihood-and-inference.html#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span> (dur <span class="sc">~</span> L, <span class="at">data =</span> men, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Listener&quot;</span>,</span>
<span id="cb44-29"><a href="probabilities-likelihood-and-inference.html#cb44-29" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(yellow, deepgreen,coral,skyblue,darkorange,lavender),<span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">120</span>,<span class="dv">370</span>))</span>
<span id="cb44-30"><a href="probabilities-likelihood-and-inference.html#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb44-31"><a href="probabilities-likelihood-and-inference.html#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span> (dur <span class="sc">~</span> L, <span class="at">data =</span> men, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,<span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb44-32"><a href="probabilities-likelihood-and-inference.html#cb44-32" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(yellow, deepgreen,coral,skyblue,darkorange,lavender), <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb44-33"><a href="probabilities-likelihood-and-inference.html#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="probabilities-likelihood-and-inference.html#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-35"><a href="probabilities-likelihood-and-inference.html#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.3</span></span>
<span id="cb44-36"><a href="probabilities-likelihood-and-inference.html#cb44-36" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-37"><a href="probabilities-likelihood-and-inference.html#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="probabilities-likelihood-and-inference.html#cb44-38" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb44-39"><a href="probabilities-likelihood-and-inference.html#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="probabilities-likelihood-and-inference.html#cb44-40" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (mens_height, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">col =</span> lavender, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">135</span>,<span class="dv">200</span>),</span>
<span id="cb44-41"><a href="probabilities-likelihood-and-inference.html#cb44-41" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">205</span>), <span class="at">xlab=</span><span class="st">&quot;Apparent height (cm)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.3</span>)</span>
<span id="cb44-42"><a href="probabilities-likelihood-and-inference.html#cb44-42" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (mens_height, <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">col =</span> deepgreen, </span>
<span id="cb44-43"><a href="probabilities-likelihood-and-inference.html#cb44-43" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">135</span>,<span class="dv">200</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.06</span>), <span class="at">xlab=</span><span class="st">&quot;Apparent height (cm)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.3</span>)</span>
<span id="cb44-44"><a href="probabilities-likelihood-and-inference.html#cb44-44" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (mens_height<span class="sc">/</span><span class="dv">100</span>, <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">col =</span> skyblue, </span>
<span id="cb44-45"><a href="probabilities-likelihood-and-inference.html#cb44-45" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">1.35</span>,<span class="fl">2.00</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">06.0</span>), <span class="at">xlab=</span><span class="st">&quot;Apparent height (m)&quot;</span>,</span>
<span id="cb44-46"><a href="probabilities-likelihood-and-inference.html#cb44-46" aria-hidden="true" tabindex="-1"></a>      <span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.3</span>)</span>
<span id="cb44-47"><a href="probabilities-likelihood-and-inference.html#cb44-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-48"><a href="probabilities-likelihood-and-inference.html#cb44-48" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-49"><a href="probabilities-likelihood-and-inference.html#cb44-49" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.4</span></span>
<span id="cb44-50"><a href="probabilities-likelihood-and-inference.html#cb44-50" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-51"><a href="probabilities-likelihood-and-inference.html#cb44-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-52"><a href="probabilities-likelihood-and-inference.html#cb44-52" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb44-53"><a href="probabilities-likelihood-and-inference.html#cb44-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-54"><a href="probabilities-likelihood-and-inference.html#cb44-54" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (mens_height, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">col =</span> lavender, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">130</span>,<span class="dv">205</span>),<span class="at">freq =</span> <span class="cn">FALSE</span>,</span>
<span id="cb44-55"><a href="probabilities-likelihood-and-inference.html#cb44-55" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">06</span>), <span class="at">xlab=</span><span class="st">&quot;Apparent height (cm)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">breaks=</span><span class="dv">10</span>)</span>
<span id="cb44-56"><a href="probabilities-likelihood-and-inference.html#cb44-56" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (mens_height<span class="sc">-</span><span class="fu">mean</span>(mens_height), <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">col =</span> deepgreen, </span>
<span id="cb44-57"><a href="probabilities-likelihood-and-inference.html#cb44-57" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">40</span>,<span class="dv">30</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.057</span>), <span class="at">xlab=</span><span class="st">&quot;Centered apparent height (cm)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">breaks=</span><span class="dv">10</span>)</span>
<span id="cb44-58"><a href="probabilities-likelihood-and-inference.html#cb44-58" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (<span class="fu">scale</span>(mens_height), <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">col =</span> skyblue, </span>
<span id="cb44-59"><a href="probabilities-likelihood-and-inference.html#cb44-59" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">5.2</span>,<span class="dv">4</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.45</span>), <span class="at">xlab=</span><span class="st">&quot;Scaled apparent height (cm)&quot;</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>,<span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">breaks=</span><span class="dv">10</span>)</span>
<span id="cb44-60"><a href="probabilities-likelihood-and-inference.html#cb44-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-61"><a href="probabilities-likelihood-and-inference.html#cb44-61" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-62"><a href="probabilities-likelihood-and-inference.html#cb44-62" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.5</span></span>
<span id="cb44-63"><a href="probabilities-likelihood-and-inference.html#cb44-63" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-64"><a href="probabilities-likelihood-and-inference.html#cb44-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-65"><a href="probabilities-likelihood-and-inference.html#cb44-65" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb44-66"><a href="probabilities-likelihood-and-inference.html#cb44-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-67"><a href="probabilities-likelihood-and-inference.html#cb44-67" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (mens_height, <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">breaks =</span> <span class="dv">20</span>, <span class="at">col =</span> <span class="st">&#39;grey&#39;</span>,</span>
<span id="cb44-68"><a href="probabilities-likelihood-and-inference.html#cb44-68" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">140</span>,<span class="dv">200</span>), <span class="at">xlab =</span> <span class="st">&quot;Apparent height (cm)&quot;</span>)</span>
<span id="cb44-69"><a href="probabilities-likelihood-and-inference.html#cb44-69" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v =</span> <span class="fl">162.1</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb44-70"><a href="probabilities-likelihood-and-inference.html#cb44-70" aria-hidden="true" tabindex="-1"></a><span class="co"># plots the normal density (red line) using stats calculated form our sample. </span></span>
<span id="cb44-71"><a href="probabilities-likelihood-and-inference.html#cb44-71" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> (<span class="fu">dnorm</span> (x, <span class="fu">mean</span>(mens_height), <span class="fu">sd</span>(mens_height)),<span class="at">from=</span><span class="dv">100</span>, <span class="at">to=</span><span class="dv">300</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb44-72"><a href="probabilities-likelihood-and-inference.html#cb44-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-73"><a href="probabilities-likelihood-and-inference.html#cb44-73" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">140</span>,<span class="fu">seq</span>(<span class="dv">140</span>,<span class="fl">162.1</span>,<span class="at">length.out =</span> <span class="dv">100</span>),<span class="fl">162.1</span>)</span>
<span id="cb44-74"><a href="probabilities-likelihood-and-inference.html#cb44-74" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">dnorm</span>(<span class="fu">seq</span>(<span class="dv">140</span>,<span class="fl">162.1</span>,<span class="at">length.out =</span> <span class="dv">100</span>), <span class="fu">mean</span> (mens_height), <span class="fu">sd</span> (mens_height)),<span class="dv">0</span>)</span>
<span id="cb44-75"><a href="probabilities-likelihood-and-inference.html#cb44-75" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(x, y, <span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>,<span class="fl">0.3</span>))</span>
<span id="cb44-76"><a href="probabilities-likelihood-and-inference.html#cb44-76" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v =</span> <span class="fl">63.8</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>); <span class="fu">abline</span> (<span class="at">v =</span> <span class="dv">70</span>, <span class="at">lwd =</span> <span class="dv">2</span>,<span class="at">col=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb44-77"><a href="probabilities-likelihood-and-inference.html#cb44-77" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">162.1</span>,<span class="fu">seq</span>(<span class="fl">162.1</span>,<span class="dv">200</span>,<span class="at">length.out =</span> <span class="dv">100</span>),<span class="dv">200</span>)</span>
<span id="cb44-78"><a href="probabilities-likelihood-and-inference.html#cb44-78" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">dnorm</span>(<span class="fu">seq</span>(<span class="fl">162.1</span>,<span class="dv">200</span>,<span class="at">length.out =</span> <span class="dv">100</span>), <span class="fu">mean</span> (mens_height), <span class="fu">sd</span> (mens_height)),<span class="dv">0</span>)</span>
<span id="cb44-79"><a href="probabilities-likelihood-and-inference.html#cb44-79" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(x, y, <span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>,<span class="fl">0.3</span>))</span>
<span id="cb44-80"><a href="probabilities-likelihood-and-inference.html#cb44-80" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v =</span> <span class="fl">63.8</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>); <span class="fu">abline</span> (<span class="at">v =</span> <span class="dv">70</span>, <span class="at">lwd =</span> <span class="dv">2</span>,<span class="at">col=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb44-81"><a href="probabilities-likelihood-and-inference.html#cb44-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-82"><a href="probabilities-likelihood-and-inference.html#cb44-82" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-83"><a href="probabilities-likelihood-and-inference.html#cb44-83" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.6</span></span>
<span id="cb44-84"><a href="probabilities-likelihood-and-inference.html#cb44-84" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-85"><a href="probabilities-likelihood-and-inference.html#cb44-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-86"><a href="probabilities-likelihood-and-inference.html#cb44-86" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span> (<span class="dv">4</span>)</span>
<span id="cb44-87"><a href="probabilities-likelihood-and-inference.html#cb44-87" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb44-88"><a href="probabilities-likelihood-and-inference.html#cb44-88" aria-hidden="true" tabindex="-1"></a>height_sample <span class="ot">=</span> <span class="fu">sample</span> (mens_height,n)   <span class="do">## tiny sub sample for example</span></span>
<span id="cb44-89"><a href="probabilities-likelihood-and-inference.html#cb44-89" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb44-90"><a href="probabilities-likelihood-and-inference.html#cb44-90" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">2</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">col=</span><span class="dv">4</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">140</span>,<span class="dv">195</span>), </span>
<span id="cb44-91"><a href="probabilities-likelihood-and-inference.html#cb44-91" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&#39;Density&#39;</span>, <span class="at">main =</span> <span class="st">&#39;Likelihood of mean&#39;</span>,<span class="at">xlab=</span><span class="st">&#39;Apparent height (cm)&#39;</span>,<span class="at">cex.main=</span>.<span class="dv">8</span>, <span class="at">cex.main=</span><span class="dv">1</span>)</span>
<span id="cb44-92"><a href="probabilities-likelihood-and-inference.html#cb44-92" aria-hidden="true" tabindex="-1"></a><span class="do">## here the likelihood sd is divided by the sample size</span></span>
<span id="cb44-93"><a href="probabilities-likelihood-and-inference.html#cb44-93" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> (<span class="fu">dnorm</span> (x, <span class="fu">mean</span>(height_sample), <span class="fu">sd</span>(height_sample) <span class="sc">/</span> <span class="fu">sqrt</span> (n)), <span class="at">from =</span> <span class="fu">c</span>(<span class="dv">140</span>,<span class="dv">195</span>), </span>
<span id="cb44-94"><a href="probabilities-likelihood-and-inference.html#cb44-94" aria-hidden="true" tabindex="-1"></a>       <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-95"><a href="probabilities-likelihood-and-inference.html#cb44-95" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v =</span> <span class="fu">c</span>(<span class="fu">mean</span> (height_sample), <span class="dv">160</span>, <span class="dv">170</span>), <span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb44-96"><a href="probabilities-likelihood-and-inference.html#cb44-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-97"><a href="probabilities-likelihood-and-inference.html#cb44-97" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">06</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">130</span>,<span class="dv">200</span>),<span class="at">cex.main=</span>.<span class="dv">8</span>, </span>
<span id="cb44-98"><a href="probabilities-likelihood-and-inference.html#cb44-98" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&#39;Density&#39;</span>, <span class="at">main =</span> <span class="st">&quot;Distribution if mean = 174 cm&quot;</span>,<span class="at">xlab=</span><span class="st">&#39;Apparent height (cm)&#39;</span>, <span class="at">cex.main=</span><span class="dv">1</span>)</span>
<span id="cb44-99"><a href="probabilities-likelihood-and-inference.html#cb44-99" aria-hidden="true" tabindex="-1"></a><span class="do">## now it is centered at mean = 175 cm</span></span>
<span id="cb44-100"><a href="probabilities-likelihood-and-inference.html#cb44-100" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> (<span class="fu">dnorm</span> (x, <span class="fu">mean</span>(height_sample), <span class="fu">sd</span>(height_sample)), <span class="at">from =</span> <span class="fu">c</span>(<span class="dv">130</span>,<span class="dv">200</span>), </span>
<span id="cb44-101"><a href="probabilities-likelihood-and-inference.html#cb44-101" aria-hidden="true" tabindex="-1"></a>       <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-102"><a href="probabilities-likelihood-and-inference.html#cb44-102" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n),height_sample,</span>
<span id="cb44-103"><a href="probabilities-likelihood-and-inference.html#cb44-103" aria-hidden="true" tabindex="-1"></a>          <span class="fu">dnorm</span> (height_sample, <span class="fu">mean</span>(height_sample), <span class="fu">sd</span>(height_sample) ))</span>
<span id="cb44-104"><a href="probabilities-likelihood-and-inference.html#cb44-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-105"><a href="probabilities-likelihood-and-inference.html#cb44-105" aria-hidden="true" tabindex="-1"></a><span class="co">#abline (v = c(mean (height_sample)), lwd=1,lty=3)</span></span>
<span id="cb44-106"><a href="probabilities-likelihood-and-inference.html#cb44-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-107"><a href="probabilities-likelihood-and-inference.html#cb44-107" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-108"><a href="probabilities-likelihood-and-inference.html#cb44-108" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.7</span></span>
<span id="cb44-109"><a href="probabilities-likelihood-and-inference.html#cb44-109" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-110"><a href="probabilities-likelihood-and-inference.html#cb44-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-111"><a href="probabilities-likelihood-and-inference.html#cb44-111" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb44-112"><a href="probabilities-likelihood-and-inference.html#cb44-112" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">06</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">140</span>,<span class="dv">195</span>),<span class="at">cex.main=</span>.<span class="dv">8</span>, </span>
<span id="cb44-113"><a href="probabilities-likelihood-and-inference.html#cb44-113" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&#39;Density&#39;</span>, <span class="at">main =</span> <span class="st">&quot;Distribution if mean = 170 cm&quot;</span>,<span class="at">xlab=</span><span class="st">&#39;Apparent height (cm)&#39;</span>, <span class="at">cex.main=</span><span class="dv">1</span>)</span>
<span id="cb44-114"><a href="probabilities-likelihood-and-inference.html#cb44-114" aria-hidden="true" tabindex="-1"></a><span class="do">## now it is centered at mean = 175 cm</span></span>
<span id="cb44-115"><a href="probabilities-likelihood-and-inference.html#cb44-115" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> (<span class="fu">dnorm</span> (x, <span class="dv">170</span>, <span class="fu">sd</span>(height_sample)), <span class="at">from =</span> <span class="fu">c</span>(<span class="dv">130</span>,<span class="dv">200</span>), </span>
<span id="cb44-116"><a href="probabilities-likelihood-and-inference.html#cb44-116" aria-hidden="true" tabindex="-1"></a>       <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-117"><a href="probabilities-likelihood-and-inference.html#cb44-117" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n),height_sample,</span>
<span id="cb44-118"><a href="probabilities-likelihood-and-inference.html#cb44-118" aria-hidden="true" tabindex="-1"></a>          <span class="fu">dnorm</span> (height_sample, <span class="dv">170</span>, <span class="fu">sd</span>(height_sample) ))</span>
<span id="cb44-119"><a href="probabilities-likelihood-and-inference.html#cb44-119" aria-hidden="true" tabindex="-1"></a><span class="co">#abline (v = 170, lwd=2,lty=3)</span></span>
<span id="cb44-120"><a href="probabilities-likelihood-and-inference.html#cb44-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-121"><a href="probabilities-likelihood-and-inference.html#cb44-121" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">06</span>), <span class="at">pch=</span><span class="dv">16</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">140</span>,<span class="dv">195</span>),<span class="at">cex.main=</span>.<span class="dv">8</span>, </span>
<span id="cb44-122"><a href="probabilities-likelihood-and-inference.html#cb44-122" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&#39;Density&#39;</span>, <span class="at">main =</span> <span class="st">&quot;Distribution if mean = 160 cm&quot;</span>,<span class="at">xlab=</span><span class="st">&#39;Apparent height (cm)&#39;</span>, <span class="at">cex.main=</span><span class="dv">1</span>)</span>
<span id="cb44-123"><a href="probabilities-likelihood-and-inference.html#cb44-123" aria-hidden="true" tabindex="-1"></a><span class="do">## now it is centered at mean = 175 cm</span></span>
<span id="cb44-124"><a href="probabilities-likelihood-and-inference.html#cb44-124" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> (<span class="fu">dnorm</span> (x, <span class="dv">160</span>, <span class="fu">sd</span>(height_sample)), <span class="at">from =</span> <span class="fu">c</span>(<span class="dv">130</span>,<span class="dv">200</span>), </span>
<span id="cb44-125"><a href="probabilities-likelihood-and-inference.html#cb44-125" aria-hidden="true" tabindex="-1"></a>       <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-126"><a href="probabilities-likelihood-and-inference.html#cb44-126" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span> (height_sample,<span class="fu">rep</span>(<span class="dv">0</span>,n),height_sample,</span>
<span id="cb44-127"><a href="probabilities-likelihood-and-inference.html#cb44-127" aria-hidden="true" tabindex="-1"></a>          <span class="fu">dnorm</span> (height_sample, <span class="dv">160</span>, <span class="fu">sd</span>(height_sample) ))</span>
<span id="cb44-128"><a href="probabilities-likelihood-and-inference.html#cb44-128" aria-hidden="true" tabindex="-1"></a><span class="co">#abline (v = 160, lwd=2,lty=3)</span></span>
<span id="cb44-129"><a href="probabilities-likelihood-and-inference.html#cb44-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-130"><a href="probabilities-likelihood-and-inference.html#cb44-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-131"><a href="probabilities-likelihood-and-inference.html#cb44-131" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-132"><a href="probabilities-likelihood-and-inference.html#cb44-132" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 2.8</span></span>
<span id="cb44-133"><a href="probabilities-likelihood-and-inference.html#cb44-133" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb44-134"><a href="probabilities-likelihood-and-inference.html#cb44-134" aria-hidden="true" tabindex="-1"></a>mus <span class="ot">=</span> <span class="fu">seq</span> (<span class="fl">172.5</span>,<span class="dv">175</span>, .<span class="dv">01</span>)</span>
<span id="cb44-135"><a href="probabilities-likelihood-and-inference.html#cb44-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-136"><a href="probabilities-likelihood-and-inference.html#cb44-136" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="ot">=</span> mus<span class="sc">*</span><span class="dv">0</span></span>
<span id="cb44-137"><a href="probabilities-likelihood-and-inference.html#cb44-137" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(mus)) log_likelihood[i] <span class="ot">=</span> <span class="fu">sum</span> (<span class="fu">dnorm</span> (mens_height, mus[i], <span class="fu">sd</span>(mens_height), <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb44-138"><a href="probabilities-likelihood-and-inference.html#cb44-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-139"><a href="probabilities-likelihood-and-inference.html#cb44-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-140"><a href="probabilities-likelihood-and-inference.html#cb44-140" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb44-141"><a href="probabilities-likelihood-and-inference.html#cb44-141" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (mus, log_likelihood, <span class="at">col =</span> skyblue, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">ylab =</span> <span class="st">&quot;Log-Likelihood&quot;</span>,</span>
<span id="cb44-142"><a href="probabilities-likelihood-and-inference.html#cb44-142" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;Apparent height (cm)&quot;</span>)</span>
<span id="cb44-143"><a href="probabilities-likelihood-and-inference.html#cb44-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-144"><a href="probabilities-likelihood-and-inference.html#cb44-144" aria-hidden="true" tabindex="-1"></a>scaled_log_likelihood <span class="ot">=</span> log_likelihood<span class="sc">-</span><span class="fu">max</span>(log_likelihood)</span>
<span id="cb44-145"><a href="probabilities-likelihood-and-inference.html#cb44-145" aria-hidden="true" tabindex="-1"></a>scaled_log_likelihood <span class="ot">=</span> <span class="fu">exp</span> (scaled_log_likelihood)</span>
<span id="cb44-146"><a href="probabilities-likelihood-and-inference.html#cb44-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-147"><a href="probabilities-likelihood-and-inference.html#cb44-147" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (mus, scaled_log_likelihood, <span class="at">col =</span> skyblue, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">ylab =</span> <span class="st">&quot;Scaled Likelihood&quot;</span>,</span>
<span id="cb44-148"><a href="probabilities-likelihood-and-inference.html#cb44-148" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;Apparent height (cm)&quot;</span>)</span></code></pre></div>

<div style="page-break-after: always;"></div>
</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="introduction-experiments-and-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="fitting-bayesian-regression-models-with-brms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
