<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Multinomial and Ordinal regression | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</title>
  <meta name="description" content="Bayesian Models for Repeated Measures" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Multinomial and Ordinal regression | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Bayesian Models for Repeated Measures" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Multinomial and Ordinal regression | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  
  <meta name="twitter:description" content="Bayesian Models for Repeated Measures" />
  

<meta name="author" content="Santiago Bareda and Noah Silbert" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"/>
<link rel="next" href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Models for Linguists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bayesian-multilevel-models-and-repeated-measures-data"><i class="fa fa-check"></i>Bayesian Multilevel models and repeated measures data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-missing-from-this-book"><i class="fa fa-check"></i>What’s missing from this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#statistics-as-procedural-knowledge"><i class="fa fa-check"></i>Statistics as Procedural knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practice-vs-brain-power"><i class="fa fa-check"></i>Practice vs brain power</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#supplemental-resources"><i class="fa fa-check"></i>Supplemental Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#our-target-audience"><i class="fa fa-check"></i>Our target audience</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-self-starter"><i class="fa fa-check"></i>The self-starter</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-convert"><i class="fa fa-check"></i>The convert</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-instructor"><i class="fa fa-check"></i>The instructor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-you-need-installed-to-use-this-book"><i class="fa fa-check"></i>What you need installed to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-go-bayesian"><i class="fa fa-check"></i>Why go Bayesian?</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-brms"><i class="fa fa-check"></i>Why brms?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#it-takes-a-village-of-books"><i class="fa fa-check"></i>It takes a village (of books)</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="c1.html"><a href="c1.html"><i class="fa fa-check"></i><b>1</b> Introduction: Experiments and Variables</a>
<ul>
<li class="chapter" data-level="1.1" data-path="c1.html"><a href="c1.html#chapter-pre-cap"><i class="fa fa-check"></i><b>1.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="1.2" data-path="c1.html"><a href="c1.html#c1-exp-and-effects"><i class="fa fa-check"></i><b>1.2</b> Experiments and effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="c1.html"><a href="c1.html#c1-exp-and-inference"><i class="fa fa-check"></i><b>1.2.1</b> Experiments and inference</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="c1.html"><a href="c1.html#c1-exp"><i class="fa fa-check"></i><b>1.3</b> Our experiment</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="c1.html"><a href="c1.html#c1-exp-intro"><i class="fa fa-check"></i><b>1.3.1</b> Our experiment: Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="c1.html"><a href="c1.html#c1-methods"><i class="fa fa-check"></i><b>1.3.2</b> Our experimental methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="c1.html"><a href="c1.html#c1-research-questions"><i class="fa fa-check"></i><b>1.3.3</b> Our research questions</a></li>
<li class="chapter" data-level="1.3.4" data-path="c1.html"><a href="c1.html#c1-exp-data"><i class="fa fa-check"></i><b>1.3.4</b> Our experimental data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="c1.html"><a href="c1.html#c1-variables"><i class="fa fa-check"></i><b>1.4</b> Variables</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="c1.html"><a href="c1.html#c1-pops-and-samps"><i class="fa fa-check"></i><b>1.4.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.4.2" data-path="c1.html"><a href="c1.html#c1-dep-and-indep"><i class="fa fa-check"></i><b>1.4.2</b> Dependent and Independent Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="c1.html"><a href="c1.html#c1-categorical"><i class="fa fa-check"></i><b>1.4.3</b> Categorical variables and ‘factors’</a></li>
<li class="chapter" data-level="1.4.4" data-path="c1.html"><a href="c1.html#c1-quantitative"><i class="fa fa-check"></i><b>1.4.4</b> Quantitative variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="c1.html"><a href="c1.html#c1-logical"><i class="fa fa-check"></i><b>1.4.5</b> Logical variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="c1.html"><a href="c1.html#c1-inspecting"><i class="fa fa-check"></i><b>1.5</b> Inspecting our data</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="c1.html"><a href="c1.html#c1-inspecting-categorical"><i class="fa fa-check"></i><b>1.5.1</b> Inspecting categorical variables</a></li>
<li class="chapter" data-level="1.5.2" data-path="c1.html"><a href="c1.html#c1-inspecting-quantitative"><i class="fa fa-check"></i><b>1.5.2</b> Inspecting quantitative variables</a></li>
<li class="chapter" data-level="1.5.3" data-path="c1.html"><a href="c1.html#c1-inspecting-together"><i class="fa fa-check"></i><b>1.5.3</b> Exploring continuous and categorical variables together</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="c1.html"><a href="c1.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
<li class="chapter" data-level="1.7" data-path="c1.html"><a href="c1.html#references"><i class="fa fa-check"></i><b>1.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c2.html"><a href="c2.html"><i class="fa fa-check"></i><b>2</b> Probabilities, likelihood, and inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="c2.html"><a href="c2.html#chapter-pre-cap-1"><i class="fa fa-check"></i><b>2.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="2.2" data-path="c2.html"><a href="c2.html#c2-data"><i class="fa fa-check"></i><b>2.2</b> Data and research questions</a></li>
<li class="chapter" data-level="2.3" data-path="c2.html"><a href="c2.html#c2-empirical-prob"><i class="fa fa-check"></i><b>2.3</b> Empirical Probabilities</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="c2.html"><a href="c2.html#c2-conditional"><i class="fa fa-check"></i><b>2.3.1</b> Conditional and marginal probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="c2.html"><a href="c2.html#c2-joint"><i class="fa fa-check"></i><b>2.3.2</b> Joint probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="c2.html"><a href="c2.html#c2-theoretical"><i class="fa fa-check"></i><b>2.4</b> Probability distributions</a></li>
<li class="chapter" data-level="2.5" data-path="c2.html"><a href="c2.html#c2-normal"><i class="fa fa-check"></i><b>2.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="c2.html"><a href="c2.html#c2-sample-mean"><i class="fa fa-check"></i><b>2.5.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.5.2" data-path="c2.html"><a href="c2.html#c2-sample-variance"><i class="fa fa-check"></i><b>2.5.2</b> The sample variance (or standard deviation)</a></li>
<li class="chapter" data-level="2.5.3" data-path="c2.html"><a href="c2.html#c2-normal-density"><i class="fa fa-check"></i><b>2.5.3</b> The normal density</a></li>
<li class="chapter" data-level="2.5.4" data-path="c2.html"><a href="c2.html#c2-standard-normal"><i class="fa fa-check"></i><b>2.5.4</b> The standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="c2.html"><a href="c2.html#c2-models-and-inference"><i class="fa fa-check"></i><b>2.6</b> Models and inference</a></li>
<li class="chapter" data-level="2.7" data-path="c2.html"><a href="c2.html#c2-likelihoods"><i class="fa fa-check"></i><b>2.7</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="c2.html"><a href="c2.html#c2-chars-of-likelihoods"><i class="fa fa-check"></i><b>2.7.1</b> Characteristics of likelihoods</a></li>
<li class="chapter" data-level="2.7.2" data-path="c2.html"><a href="c2.html#c2-logarithms"><i class="fa fa-check"></i><b>2.7.2</b> A brief aside on logarithms</a></li>
<li class="chapter" data-level="2.7.3" data-path="c2.html"><a href="c2.html#c2-chars-of-likelihoods-2"><i class="fa fa-check"></i><b>2.7.3</b> Characteristics of likelihoods, continued</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="c2.html"><a href="c2.html#c2-inference-and-likelihood"><i class="fa fa-check"></i><b>2.8</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.9" data-path="c2.html"><a href="c2.html#exercises-1"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
<li class="chapter" data-level="2.10" data-path="c2.html"><a href="c2.html#references-1"><i class="fa fa-check"></i><b>2.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c3.html"><a href="c3.html"><i class="fa fa-check"></i><b>3</b> Fitting Bayesian regression models with <em>brms</em></a>
<ul>
<li class="chapter" data-level="3.1" data-path="c3.html"><a href="c3.html#chapter-pre-cap-2"><i class="fa fa-check"></i><b>3.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="3.2" data-path="c3.html"><a href="c3.html#c3-what-is-reg"><i class="fa fa-check"></i><b>3.2</b> What are regression models?</a></li>
<li class="chapter" data-level="3.3" data-path="c3.html"><a href="c3.html#c3-whats-bayes"><i class="fa fa-check"></i><b>3.3</b> What’s ‘Bayesian’ about these models?</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="c3.html"><a href="c3.html#c3-priors"><i class="fa fa-check"></i><b>3.3.1</b> Prior probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="c3.html"><a href="c3.html#c3-posterior"><i class="fa fa-check"></i><b>3.3.2</b> Posterior distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="c3.html"><a href="c3.html#c3-characteristics-posteriors"><i class="fa fa-check"></i><b>3.3.3</b> Posterior distributions and shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="c3.html"><a href="c3.html#c3-sampling"><i class="fa fa-check"></i><b>3.4</b> Sampling from the posterior using <em>Stan</em> and <em>brms</em></a></li>
<li class="chapter" data-level="3.5" data-path="c3.html"><a href="c3.html#c3-estimating"><i class="fa fa-check"></i><b>3.5</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="c3.html"><a href="c3.html#c3-data-qs-1"><i class="fa fa-check"></i><b>3.5.1</b> Data and Research Questions</a></li>
<li class="chapter" data-level="3.5.2" data-path="c3.html"><a href="c3.html#c3-description-1"><i class="fa fa-check"></i><b>3.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.5.3" data-path="c3.html"><a href="c3.html#c3-errors-and-residuals"><i class="fa fa-check"></i><b>3.5.3</b> Errors and residuals</a></li>
<li class="chapter" data-level="3.5.4" data-path="c3.html"><a href="c3.html#c3-model-formula"><i class="fa fa-check"></i><b>3.5.4</b> The model formula</a></li>
<li class="chapter" data-level="3.5.5" data-path="c3.html"><a href="c3.html#c3-calling-brm"><i class="fa fa-check"></i><b>3.5.5</b> Fitting the model: Calling the <em>brm</em> function</a></li>
<li class="chapter" data-level="3.5.6" data-path="c3.html"><a href="c3.html#c3-interpreting-print"><i class="fa fa-check"></i><b>3.5.6</b> Interpreting the model: The print statement</a></li>
<li class="chapter" data-level="3.5.7" data-path="c3.html"><a href="c3.html#c3-seeing-samples"><i class="fa fa-check"></i><b>3.5.7</b> Seeing the samples</a></li>
<li class="chapter" data-level="3.5.8" data-path="c3.html"><a href="c3.html#c3-getting-residuals"><i class="fa fa-check"></i><b>3.5.8</b> Getting the residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="c3.html"><a href="c3.html#c3-checking-convergence"><i class="fa fa-check"></i><b>3.6</b> Checking model convergence</a></li>
<li class="chapter" data-level="3.7" data-path="c3.html"><a href="c3.html#c3-specifying-priors"><i class="fa fa-check"></i><b>3.7</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="3.8" data-path="c3.html"><a href="c3.html#c3-log-posterior"><i class="fa fa-check"></i><b>3.8</b> The log prior and log posterior densities</a></li>
<li class="chapter" data-level="3.9" data-path="c3.html"><a href="c3.html#c3-answering-qs"><i class="fa fa-check"></i><b>3.9</b> Answering our research questions</a></li>
<li class="chapter" data-level="3.10" data-path="c3.html"><a href="c3.html#c3-frequentist"><i class="fa fa-check"></i><b>3.10</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="c3.html"><a href="c3.html#c3-vs-ttest"><i class="fa fa-check"></i><b>3.10.1</b> One-sample t-test vs. intercept-only Bayesian models</a></li>
<li class="chapter" data-level="3.10.2" data-path="c3.html"><a href="c3.html#c3-vs-ols"><i class="fa fa-check"></i><b>3.10.2</b> Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="c3.html"><a href="c3.html#exercises-2"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c4.html"><a href="c4.html"><i class="fa fa-check"></i><b>4</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="c4.html"><a href="c4.html#chapter-pre-cap-3"><i class="fa fa-check"></i><b>4.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="4.2" data-path="c4.html"><a href="c4.html#c4-multilevel"><i class="fa fa-check"></i><b>4.2</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="c4.html"><a href="c4.html#c4-levels"><i class="fa fa-check"></i><b>4.2.1</b> Multilevel models and ‘levels’ of variation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="c4.html"><a href="c4.html#c4-many-levels"><i class="fa fa-check"></i><b>4.3</b> Representing predictors with many levels</a></li>
<li class="chapter" data-level="4.4" data-path="c4.html"><a href="c4.html#c4-strategies"><i class="fa fa-check"></i><b>4.4</b> Strategies for estimating factors with many levels</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="c4.html"><a href="c4.html#c4-complete-pooling"><i class="fa fa-check"></i><b>4.4.1</b> Complete pooling</a></li>
<li class="chapter" data-level="4.4.2" data-path="c4.html"><a href="c4.html#c4-no-pooling"><i class="fa fa-check"></i><b>4.4.2</b> No pooling</a></li>
<li class="chapter" data-level="4.4.3" data-path="c4.html"><a href="c4.html#c4-partial-pooling"><i class="fa fa-check"></i><b>4.4.3</b> (Adaptive) Partial pooling</a></li>
<li class="chapter" data-level="4.4.4" data-path="c4.html"><a href="c4.html#hyperpriors"><i class="fa fa-check"></i><b>4.4.4</b> Hyperpriors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="c4.html"><a href="c4.html#c4-estimating1"><i class="fa fa-check"></i><b>4.5</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="c4.html"><a href="c4.html#c4-data-and-qs-1"><i class="fa fa-check"></i><b>4.5.1</b> Data and Research questions</a></li>
<li class="chapter" data-level="4.5.2" data-path="c4.html"><a href="c4.html#description-of-the-model"><i class="fa fa-check"></i><b>4.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="4.5.3" data-path="c4.html"><a href="c4.html#c4-fitting-1"><i class="fa fa-check"></i><b>4.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="4.5.4" data-path="c4.html"><a href="c4.html#interpreting-the-model"><i class="fa fa-check"></i><b>4.5.4</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="c4.html"><a href="c4.html#c4-random-effects"><i class="fa fa-check"></i><b>4.6</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="c4.html"><a href="c4.html#c4-inspecting-random-effects"><i class="fa fa-check"></i><b>4.6.1</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="c4.html"><a href="c4.html#c4-simulating"><i class="fa fa-check"></i><b>4.7</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="4.8" data-path="c4.html"><a href="c4.html#c4-second-random-effect"><i class="fa fa-check"></i><b>4.8</b> Adding a second random effect</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="c4.html"><a href="c4.html#c4-updating-model"><i class="fa fa-check"></i><b>4.8.1</b> Updating the model description</a></li>
<li class="chapter" data-level="4.8.2" data-path="c4.html"><a href="c4.html#fitting-and-interpreting-the-model"><i class="fa fa-check"></i><b>4.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="c4.html"><a href="c4.html#c4-investigating-shrinkage"><i class="fa fa-check"></i><b>4.9</b> Investigating ‘shrinkage’</a></li>
<li class="chapter" data-level="4.10" data-path="c4.html"><a href="c4.html#c4-answering-question"><i class="fa fa-check"></i><b>4.10</b> Answering our research questions</a></li>
<li class="chapter" data-level="4.11" data-path="c4.html"><a href="c4.html#c4-frequentist"><i class="fa fa-check"></i><b>4.11</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="c4.html"><a href="c4.html#c4-vs-lmer"><i class="fa fa-check"></i><b>4.11.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="c4.html"><a href="c4.html#exercises-3"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
<li class="chapter" data-level="4.13" data-path="c4.html"><a href="c4.html#references-2"><i class="fa fa-check"></i><b>4.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="c5.html"><a href="c5.html"><i class="fa fa-check"></i><b>5</b> Comparing two groups of observations: Factors and contrasts</a>
<ul>
<li class="chapter" data-level="5.1" data-path="c5.html"><a href="c5.html#chapter-pre-cap-4"><i class="fa fa-check"></i><b>5.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="5.2" data-path="c5.html"><a href="c5.html#comparing-two-groups"><i class="fa fa-check"></i><b>5.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="5.3" data-path="c5.html"><a href="c5.html#distribution-of-repeated-measures-across-factor-levels"><i class="fa fa-check"></i><b>5.3</b> Distribution of repeated measures across factor levels</a></li>
<li class="chapter" data-level="5.4" data-path="c5.html"><a href="c5.html#c5-data-and-qs"><i class="fa fa-check"></i><b>5.4</b> Data and research questions</a></li>
<li class="chapter" data-level="5.5" data-path="c5.html"><a href="c5.html#c5-two-means"><i class="fa fa-check"></i><b>5.5</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="c5.html"><a href="c5.html#fitting-the-model"><i class="fa fa-check"></i><b>5.5.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="c5.html"><a href="c5.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>5.5.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="c5.html"><a href="c5.html#c5-contrasts"><i class="fa fa-check"></i><b>5.6</b> Contrasts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="c5.html"><a href="c5.html#c5-treatment-coding"><i class="fa fa-check"></i><b>5.6.1</b> Treatment coding</a></li>
<li class="chapter" data-level="5.6.2" data-path="c5.html"><a href="c5.html#c5-sum-coding"><i class="fa fa-check"></i><b>5.6.2</b> Sum coding</a></li>
<li class="chapter" data-level="5.6.3" data-path="c5.html"><a href="c5.html#c5-comparison-sum-treatment"><i class="fa fa-check"></i><b>5.6.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="c5.html"><a href="c5.html#c5-refittin-sum"><i class="fa fa-check"></i><b>5.7</b> Sum coding and the decomposition of variation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="c5.html"><a href="c5.html#c5-description-1"><i class="fa fa-check"></i><b>5.7.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.7.2" data-path="c5.html"><a href="c5.html#fitting-the-model-1"><i class="fa fa-check"></i><b>5.7.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.7.3" data-path="c5.html"><a href="c5.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>5.7.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="c5.html"><a href="c5.html#c5-working-with-posteriors"><i class="fa fa-check"></i><b>5.8</b> Inspecting and manipulating the posterior samples</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="c5.html"><a href="c5.html#c5-using-hypothesis"><i class="fa fa-check"></i><b>5.8.1</b> Using the <em>hypothesis</em> function</a></li>
<li class="chapter" data-level="5.8.2" data-path="c5.html"><a href="c5.html#c5-manipulating-random-effects"><i class="fa fa-check"></i><b>5.8.2</b> Working with the random effects</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="c5.html"><a href="c5.html#c5-robustness"><i class="fa fa-check"></i><b>5.9</b> Making our models more robust: The (non-standardized) t distribution</a></li>
<li class="chapter" data-level="5.10" data-path="c5.html"><a href="c5.html#re-fitting-with-t-distributed-errors."><i class="fa fa-check"></i><b>5.10</b> Re-fitting with t-distributed errors</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="c5.html"><a href="c5.html#description-of-the-model-1"><i class="fa fa-check"></i><b>5.10.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.10.2" data-path="c5.html"><a href="c5.html#fitting-and-interpreting-the-model-1"><i class="fa fa-check"></i><b>5.10.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="c5.html"><a href="c5.html#c5-simulating"><i class="fa fa-check"></i><b>5.11</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="5.12" data-path="c5.html"><a href="c5.html#c5-answering-qs"><i class="fa fa-check"></i><b>5.12</b> Answering our research questions</a></li>
<li class="chapter" data-level="5.13" data-path="c5.html"><a href="c5.html#c5-frequentist"><i class="fa fa-check"></i><b>5.13</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="c5.html"><a href="c5.html#bayesian-multilevel-models-vs.-lmer"><i class="fa fa-check"></i><b>5.13.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="c5.html"><a href="c5.html#exercises-4"><i class="fa fa-check"></i><b>5.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="c6.html"><a href="c6.html"><i class="fa fa-check"></i><b>6</b> Variation in parameters (‘random effects’) and model comparison</a>
<ul>
<li class="chapter" data-level="6.1" data-path="c6.html"><a href="c6.html#chapter-pre-cap-5"><i class="fa fa-check"></i><b>6.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="6.2" data-path="c6.html"><a href="c6.html#c6-data-and-qs"><i class="fa fa-check"></i><b>6.2</b> Data and research questions</a></li>
<li class="chapter" data-level="6.3" data-path="c6.html"><a href="c6.html#c6-variation-sources"><i class="fa fa-check"></i><b>6.3</b> Variation in parameters across sources of data</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="c6.html"><a href="c6.html#description-of-our-model"><i class="fa fa-check"></i><b>6.3.1</b> Description of our model</a></li>
<li class="chapter" data-level="6.3.2" data-path="c6.html"><a href="c6.html#c6-correlations"><i class="fa fa-check"></i><b>6.3.2</b> Correlations between random parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="c6.html"><a href="c6.html#c6-random-and-mvn"><i class="fa fa-check"></i><b>6.3.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.4" data-path="c6.html"><a href="c6.html#c6-mvn-priors"><i class="fa fa-check"></i><b>6.3.4</b> Specifying priors for a multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.5" data-path="c6.html"><a href="c6.html#updating-our-model-description"><i class="fa fa-check"></i><b>6.3.5</b> Updating our model description</a></li>
<li class="chapter" data-level="6.3.6" data-path="c6.html"><a href="c6.html#c6-fitting"><i class="fa fa-check"></i><b>6.3.6</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="c6.html"><a href="c6.html#c6-model-comparison"><i class="fa fa-check"></i><b>6.4</b> Model Comparison</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="c6.html"><a href="c6.html#c6-in-and-out-prediction"><i class="fa fa-check"></i><b>6.4.1</b> In-sample and out-of-sample prediction</a></li>
<li class="chapter" data-level="6.4.2" data-path="c6.html"><a href="c6.html#c6-out-sample-adjust"><i class="fa fa-check"></i><b>6.4.2</b> Out-of-sample prediction: Adjusting predictive accuracy</a></li>
<li class="chapter" data-level="6.4.3" data-path="c6.html"><a href="c6.html#c6-out-sample-crossval"><i class="fa fa-check"></i><b>6.4.3</b> Out-of-sample prediction: Cross validation</a></li>
<li class="chapter" data-level="6.4.4" data-path="c6.html"><a href="c6.html#selecting-a-model"><i class="fa fa-check"></i><b>6.4.4</b> Selecting a model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="c6.html"><a href="c6.html#c6-answering"><i class="fa fa-check"></i><b>6.5</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.6" data-path="c6.html"><a href="c6.html#c6-frequentist"><i class="fa fa-check"></i><b>6.6</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="c6.html"><a href="c6.html#c6-vs-lmer"><i class="fa fa-check"></i><b>6.6.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="c6.html"><a href="c6.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="c6.html"><a href="c6.html#references-3"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><i class="fa fa-check"></i><b>7</b> Comparing many groups, interactions, and posterior predictive checks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#chapter-pre-cap-6"><i class="fa fa-check"></i><b>7.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="7.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions"><i class="fa fa-check"></i><b>7.2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.2.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-description-1"><i class="fa fa-check"></i><b>7.2.2</b> Description of our model</a></li>
<li class="chapter" data-level="7.2.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-2"><i class="fa fa-check"></i><b>7.2.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-multiple-factors-simultaneously"><i class="fa fa-check"></i><b>7.3</b> Investigating multiple factors simultaneously</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>7.3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.3.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-the-model-2"><i class="fa fa-check"></i><b>7.3.2</b> Description of the model</a></li>
<li class="chapter" data-level="7.3.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-3"><i class="fa fa-check"></i><b>7.3.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-posterior-prediction"><i class="fa fa-check"></i><b>7.4</b> Posterior prediction: Using our models to predict new data</a></li>
<li class="chapter" data-level="7.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-interactions-and-plots"><i class="fa fa-check"></i><b>7.5</b> Interactions and interaction plots</a></li>
<li class="chapter" data-level="7.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>7.6</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>7.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.6.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#model-formulas"><i class="fa fa-check"></i><b>7.6.2</b> Model formulas</a></li>
<li class="chapter" data-level="7.6.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-our-model-1"><i class="fa fa-check"></i><b>7.6.3</b> Description of our model</a></li>
<li class="chapter" data-level="7.6.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-4"><i class="fa fa-check"></i><b>7.6.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="7.6.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-calc-means"><i class="fa fa-check"></i><b>7.6.5</b> Caulculating group means in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#calculating-simple-effects-in-the-presence-of-interactions"><i class="fa fa-check"></i><b>7.6.6</b> Calculating simple effects in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#assessing-model-fit-bayesian-r2"><i class="fa fa-check"></i><b>7.6.7</b> Assessing model fit: Bayesian <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-answering"><i class="fa fa-check"></i><b>7.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="7.8" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.8</b> Factors with more than two levels</a></li>
<li class="chapter" data-level="7.9" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-frequentist"><i class="fa fa-check"></i><b>7.9</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#bayesian-multilevel-models-vs.-lmer-1"><i class="fa fa-check"></i><b>7.9.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#exercises-6"><i class="fa fa-check"></i><b>7.10</b> Exercises</a></li>
<li class="chapter" data-level="7.11" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#references-4"><i class="fa fa-check"></i><b>7.11</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html"><i class="fa fa-check"></i><b>8</b> Varying variances, more about priors, and prior predictive checks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#chapter-pre-cap-7"><i class="fa fa-check"></i><b>8.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>8.2</b> Data and Research questions</a></li>
<li class="chapter" data-level="8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-about-priors"><i class="fa fa-check"></i><b>8.3</b> More about priors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-prior-prediction"><i class="fa fa-check"></i><b>8.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.3.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-specific-priors"><i class="fa fa-check"></i><b>8.3.2</b> More specific priors</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#heteroskedasticity-and-distributional-or-mixture-models"><i class="fa fa-check"></i><b>8.4</b> Heteroskedasticity and distributional (or mixture) models</a></li>
<li class="chapter" data-level="8.5" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-simple-model-error-varies-according-to-a-single-fixed-effect"><i class="fa fa-check"></i><b>8.5</b> A ‘simple’ model: Error varies according to a single fixed effect</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#description-of-our-model-2"><i class="fa fa-check"></i><b>8.5.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.5.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#prior-predictive-checks"><i class="fa fa-check"></i><b>8.5.2</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.5.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-5"><i class="fa fa-check"></i><b>8.5.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-complex-model-error-varies-according-to-fixed-and-random-effects"><i class="fa fa-check"></i><b>8.6</b> A ‘complex’ model: Error varies according to fixed and random effects</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2"><i class="fa fa-check"></i><b>8.6.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.6.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-6"><i class="fa fa-check"></i><b>8.6.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#answering-our-research-questions"><i class="fa fa-check"></i><b>8.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="8.8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability"><i class="fa fa-check"></i><b>8.8</b> Building identifiable and supportable models</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#collinearity"><i class="fa fa-check"></i><b>8.8.1</b> Collinearity</a></li>
<li class="chapter" data-level="8.8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#predictable-values-of-categorical-predictors"><i class="fa fa-check"></i><b>8.8.2</b> Predictable values of categorical predictors</a></li>
<li class="chapter" data-level="8.8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#saturated-and-nearly-saturated-models"><i class="fa fa-check"></i><b>8.8.3</b> Saturated, and ‘nearly-saturated’, models</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#exercises-7"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
<li class="chapter" data-level="8.10" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#references-5"><i class="fa fa-check"></i><b>8.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html"><i class="fa fa-check"></i><b>9</b> Quantitative predictors and their interactions with factors</a>
<ul>
<li class="chapter" data-level="9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#chapter-pre-cap-8"><i class="fa fa-check"></i><b>9.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>9.2</b> Data and research questions</a></li>
<li class="chapter" data-level="9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#modeling-variation-along-lines"><i class="fa fa-check"></i><b>9.3</b> Modeling variation along lines</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-3"><i class="fa fa-check"></i><b>9.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.3.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-centering"><i class="fa fa-check"></i><b>9.3.2</b> Centering quantitative predictors</a></li>
<li class="chapter" data-level="9.3.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-an-interpreting-the-model"><i class="fa fa-check"></i><b>9.3.3</b> Fitting an interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>9.4</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-4"><i class="fa fa-check"></i><b>9.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.4.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-7"><i class="fa fa-check"></i><b>9.4.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.4.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-shared-non-zero-slopes"><i class="fa fa-check"></i><b>9.4.3</b> Interpreting group effects in the presence of shared (non-zero) slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>9.5</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-5"><i class="fa fa-check"></i><b>9.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.5.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-3"><i class="fa fa-check"></i><b>9.5.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.5.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-varying-slopes"><i class="fa fa-check"></i><b>9.5.3</b> Interpreting group effects in the presence of varying slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-interim-discussion"><i class="fa fa-check"></i><b>9.6</b> Answering our research questions: Interim discussion</a></li>
<li class="chapter" data-level="9.7" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-updated"><i class="fa fa-check"></i><b>9.7</b> Data and research questions: Updated</a></li>
<li class="chapter" data-level="9.8" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-intercepts-and-slopes-for-each-level-of-a-grouping-factor-i.e.-random-slopes"><i class="fa fa-check"></i><b>9.8</b> Models with intercepts and slopes for each level of a grouping factor (i.e. ‘random slopes’)</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-6"><i class="fa fa-check"></i><b>9.8.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.8.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-8"><i class="fa fa-check"></i><b>9.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-multiple-predictors-for-each-level-of-a-grouping-factor"><i class="fa fa-check"></i><b>9.9</b> Models with multiple predictors for each level of a grouping factor</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-7"><i class="fa fa-check"></i><b>9.9.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-5"><i class="fa fa-check"></i><b>9.9.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#model-selection"><i class="fa fa-check"></i><b>9.9.3</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-updated"><i class="fa fa-check"></i><b>9.10</b> Answering our research questions: Updated</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#a-word-on-causality"><i class="fa fa-check"></i><b>9.10.1</b> A word on causality</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#exercises-8"><i class="fa fa-check"></i><b>9.11</b> Exercises</a></li>
<li class="chapter" data-level="9.12" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#references-6"><i class="fa fa-check"></i><b>9.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html"><i class="fa fa-check"></i><b>10</b> Logistic regression and signal detection theory models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#chapter-pre-cap-9"><i class="fa fa-check"></i><b>10.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous"><i class="fa fa-check"></i><b>10.2</b> Dichotomous variables and data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#generalizing-our-linear-models"><i class="fa fa-check"></i><b>10.3</b> Generalizing our linear models</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logits"><i class="fa fa-check"></i><b>10.4.1</b> Logits</a></li>
<li class="chapter" data-level="10.4.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit"><i class="fa fa-check"></i><b>10.4.2</b> The inverse logit link function</a></li>
<li class="chapter" data-level="10.4.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#building-intuitions-about-logits-and-the-inverse-logit-function"><i class="fa fa-check"></i><b>10.4.3</b> Building intuitions about logits and the inverse logit function</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-with-one-quantitative-predictor"><i class="fa fa-check"></i><b>10.5</b> Logistic regression with one quantitative predictor</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>10.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.5.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-8"><i class="fa fa-check"></i><b>10.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.5.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-0"><i class="fa fa-check"></i><b>10.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="10.5.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1"><i class="fa fa-check"></i><b>10.5.4</b> Interpreting the model</a></li>
<li class="chapter" data-level="10.5.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification"><i class="fa fa-check"></i><b>10.5.5</b> Using logistic models to understand classification</a></li>
<li class="chapter" data-level="10.5.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-question"><i class="fa fa-check"></i><b>10.5.6</b> Answering our research question</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#measuring-sensitivity-and-bias"><i class="fa fa-check"></i><b>10.6</b> Measuring sensitivity and bias</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-6"><i class="fa fa-check"></i><b>10.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.6.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-9"><i class="fa fa-check"></i><b>10.6.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.6.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#fitting-and-interpreting-the-model-9"><i class="fa fa-check"></i><b>10.6.3</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="10.6.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>10.6.4</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#exercises-9"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
<li class="chapter" data-level="10.8" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#references-7"><i class="fa fa-check"></i><b>10.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><i class="fa fa-check"></i><b>11</b> Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#chapter-pre-cap-10"><i class="fa fa-check"></i><b>11.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#models-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.2</b> Models with multiple quantitative predictors</a></li>
<li class="chapter" data-level="11.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#interactions-between-quantitative-predictors"><i class="fa fa-check"></i><b>11.3</b> Interactions between quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#centering-quantitative-predictors-when-including-interactions"><i class="fa fa-check"></i><b>11.3.1</b> Centering quantitative predictors when including interactions</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-7"><i class="fa fa-check"></i><b>11.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="11.3.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-description-1"><i class="fa fa-check"></i><b>11.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="11.3.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-the-model-2"><i class="fa fa-check"></i><b>11.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="11.3.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#advantages-of-bayesian-multilevel-models-for-large-models"><i class="fa fa-check"></i><b>11.3.5</b> Advantages of Bayesian multilevel models for large models</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-BANOVA"><i class="fa fa-check"></i><b>11.4</b> Bayesian Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#getting-the-standard-deviations-from-our-models-manually"><i class="fa fa-check"></i><b>11.4.1</b> Getting the standard deviations from our models ‘manually’</a></li>
<li class="chapter" data-level="11.4.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#using-the-banova-function"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>banova</code> function</a></li>
<li class="chapter" data-level="11.4.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-comparing-the-reduced-model"><i class="fa fa-check"></i><b>11.4.3</b> Fitting and comparing the reduced model</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#a-logistic-regression-model-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.5</b> A logistic regression model with multiple quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-8"><i class="fa fa-check"></i><b>11.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="11.5.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#description-of-the-model-10"><i class="fa fa-check"></i><b>11.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="11.5.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-the-model-and-applying-a-bayesian-anova"><i class="fa fa-check"></i><b>11.5.3</b> Fitting and the model and applying a Bayesian ANOVA</a></li>
<li class="chapter" data-level="11.5.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c12-2d-categorization"><i class="fa fa-check"></i><b>11.5.4</b> Categorization in two dimensions</a></li>
<li class="chapter" data-level="11.5.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#model-selection-and-misspecification"><i class="fa fa-check"></i><b>11.5.5</b> Model selection and misspecification</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#exercises-10"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#references-8"><i class="fa fa-check"></i><b>11.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial and Ordinal regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#chapter-pre-cap-11"><i class="fa fa-check"></i><b>12.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="12.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logits-and-the-softmax-function"><i class="fa fa-check"></i><b>12.2.1</b> Multinomial logits and the softmax function</a></li>
<li class="chapter" data-level="12.2.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#comparison-to-logistic-regression"><i class="fa fa-check"></i><b>12.2.2</b> Comparison to logistic regression</a></li>
<li class="chapter" data-level="12.2.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-9"><i class="fa fa-check"></i><b>12.2.3</b> Data and research questions</a></li>
<li class="chapter" data-level="12.2.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-our-model-3"><i class="fa fa-check"></i><b>12.2.4</b> Description of our model</a></li>
<li class="chapter" data-level="12.2.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-the-model-3"><i class="fa fa-check"></i><b>12.2.5</b> Fitting the model</a></li>
<li class="chapter" data-level="12.2.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>12.2.6</b> Interpreting the model</a></li>
<li class="chapter" data-level="12.2.7" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-multinomial-territorial-maps"><i class="fa fa-check"></i><b>12.2.7</b> Multinomial models and territorial maps</a></li>
<li class="chapter" data-level="12.2.8" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#refitting-the-model-without-speaker-random-effects"><i class="fa fa-check"></i><b>12.2.8</b> Refitting the model without speaker random effects</a></li>
<li class="chapter" data-level="12.2.9" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-2"><i class="fa fa-check"></i><b>12.2.9</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Ordinal (logistic) regression</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-cumulative-density"><i class="fa fa-check"></i><b>12.3.1</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="12.3.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-10"><i class="fa fa-check"></i><b>12.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="12.3.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-the-model-11"><i class="fa fa-check"></i><b>12.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="12.3.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-and-interpreting-the-model-10"><i class="fa fa-check"></i><b>12.3.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="12.3.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#listener-specific-discrimination-terms"><i class="fa fa-check"></i><b>12.3.5</b> Listener-specific discrimination terms</a></li>
<li class="chapter" data-level="12.3.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-3"><i class="fa fa-check"></i><b>12.3.6</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#exercises-11"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
<li class="chapter" data-level="12.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#references-9"><i class="fa fa-check"></i><b>12.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><i class="fa fa-check"></i><b>13</b> Writing up experiments: An investigation of the perception of apparent speaker characteristics from speech acoustics</a>
<ul>
<li class="chapter" data-level="13.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#fundamental-frequency-and-voice-pitch"><i class="fa fa-check"></i><b>13.1.1</b> Fundamental frequency and voice pitch</a></li>
<li class="chapter" data-level="13.1.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-fundamental-frequency-between-speakers"><i class="fa fa-check"></i><b>13.1.2</b> Variation in fundamental frequency between speakers</a></li>
<li class="chapter" data-level="13.1.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#voice-resonance-and-vocal-tract-length"><i class="fa fa-check"></i><b>13.1.3</b> Voice resonance and vocal-tract length</a></li>
<li class="chapter" data-level="13.1.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-estimating-vtl"><i class="fa fa-check"></i><b>13.1.4</b> Estimating vocal-tracts length from speech</a></li>
<li class="chapter" data-level="13.1.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-vocal-tract-length-between-speakers"><i class="fa fa-check"></i><b>13.1.5</b> Variation in vocal-tract length between speakers</a></li>
<li class="chapter" data-level="13.1.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-perception-of-chars"><i class="fa fa-check"></i><b>13.1.6</b> Perception of age, gender and size</a></li>
<li class="chapter" data-level="13.1.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#category-dependent-behavior"><i class="fa fa-check"></i><b>13.1.7</b> Category-dependent behavior</a></li>
<li class="chapter" data-level="13.1.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-current-experiment"><i class="fa fa-check"></i><b>13.1.8</b> The current experiment</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#methods"><i class="fa fa-check"></i><b>13.2</b> Methods</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#participants"><i class="fa fa-check"></i><b>13.2.1</b> Participants</a></li>
<li class="chapter" data-level="13.2.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-stimuli"><i class="fa fa-check"></i><b>13.2.2</b> Stimuli</a></li>
<li class="chapter" data-level="13.2.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#procedure"><i class="fa fa-check"></i><b>13.2.3</b> Procedure</a></li>
<li class="chapter" data-level="13.2.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#data-screening"><i class="fa fa-check"></i><b>13.2.4</b> Data screening</a></li>
<li class="chapter" data-level="13.2.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#loading-the-data-and-packages"><i class="fa fa-check"></i><b>13.2.5</b> Loading the data and packages</a></li>
<li class="chapter" data-level="13.2.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-height"><i class="fa fa-check"></i><b>13.2.6</b> Statistical Analysis: Apparent height</a></li>
<li class="chapter" data-level="13.2.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-gender"><i class="fa fa-check"></i><b>13.2.7</b> Statistical Analysis: Apparent gender</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-height-judgments"><i class="fa fa-check"></i><b>13.3</b> Results: Apparent height judgments</a></li>
<li class="chapter" data-level="13.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-height"><i class="fa fa-check"></i><b>13.4</b> Discussion: Apparent height</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#age-dependent-use-of-vtl-cues-on-apparent-height"><i class="fa fa-check"></i><b>13.4.1</b> Age-dependent use of VTL cues on apparent height</a></li>
<li class="chapter" data-level="13.4.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-effect-for-apparent-gender-on-apparent-height"><i class="fa fa-check"></i><b>13.4.2</b> The effect for apparent gender on apparent height</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-height-judgments"><i class="fa fa-check"></i><b>13.5</b> Conclusion: Apparent height judgments</a></li>
<li class="chapter" data-level="13.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.6</b> Results: Apparent gender judgments</a></li>
<li class="chapter" data-level="13.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.7</b> Discussion: Apparent gender judgments</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#effect-of-apparent-age-on-the-perception-of-femaleness"><i class="fa fa-check"></i><b>13.7.1</b> Effect of apparent age on the perception of femaleness</a></li>
<li class="chapter" data-level="13.7.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#between-listener-variation-in-gender-perception"><i class="fa fa-check"></i><b>13.7.2</b> Between-listener variation in gender perception</a></li>
<li class="chapter" data-level="13.7.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#beyond-gross-acoustic-cues-in-gender-perception"><i class="fa fa-check"></i><b>13.7.3</b> Beyond gross acoustic cues in gender perception</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-gender"><i class="fa fa-check"></i><b>13.8</b> Conclusion: Apparent gender</a></li>
<li class="chapter" data-level="13.9" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#next-steps"><i class="fa fa-check"></i><b>13.9</b> Next steps</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#research-design-variable-selection-etc."><i class="fa fa-check"></i><b>13.9.1</b> Research design, variable selection, etc.</a></li>
<li class="chapter" data-level="13.9.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#non-linear-models"><i class="fa fa-check"></i><b>13.9.2</b> Non-linear models</a></li>
<li class="chapter" data-level="13.9.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#other-data-distributions"><i class="fa fa-check"></i><b>13.9.3</b> Other data distributions</a></li>
<li class="chapter" data-level="13.9.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#multivariate-analyses"><i class="fa fa-check"></i><b>13.9.4</b> Multivariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#references-10"><i class="fa fa-check"></i><b>13.10</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="http://www.santiagobarreda.com" target="blank">Written by Santiago Barreda</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-and-ordinal-regression" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Multinomial and Ordinal regression<a href="multinomial-and-ordinal-regression.html#multinomial-and-ordinal-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In chapter 10 we introduced models that can predict dichotomous (2-category) categorical variables using logistic regression. Here, we extend the concepts introduced in that chapter to the modeling of dependent variables with any number of categories. First, we discuss multinomial regression models that can be used to predict categorical variables without an inherent ordering. For example, we could use a multinomial model to predict the perception of vowel sounds from speech acoustics, to predict the lexical category of a word (verb, noun, adjective, …), or to predict a speaker’s native language. After this, we introduce ordinal models appropriate for the prediction of categorical variables with some inherent ordering (e.g., first, second, and third).</p>
<div id="chapter-pre-cap-11" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Chapter pre-cap<a href="multinomial-and-ordinal-regression.html#chapter-pre-cap-11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we introduce models for the prediction of categorical dependent variables with &gt;2 categories. First, we discuss multinomial regression and its relationship to logistic regression. A multinomial logistic regression model with several predictors and random effects is fit and interpreted. We then explain the use of multinomial models to classify data, model participant responses, and make territorial maps. After that, we introduce ordinal logistic regression models, models to predict ordered categorical data. We present ordinal regression as a latent variable model, and discuss the use of cumulative distributions and thresholds by ordinal models. An ordinal logistic regression model with several predictors and random effects is fit, interpreted, and discussed. Finally, we present an example of an ordinal regression model with participant-specific latent variable distributions.</p>
</div>
<div id="multinomial-logistic-regression" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Multinomial logistic regression<a href="multinomial-and-ordinal-regression.html#multinomial-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Multinomial logistic regression</strong> allows you to model data generated by a <strong>multinomial distribution</strong> with unknown parameters. Multinomial data arises when you have a variable that can take on any number of discrete values (often, but not always, a small number). These values do not <em>need</em> to have a necessary order (but they can), and the ‘differences’ between them don’t mean anything. For example, in our experiment listeners were asked to identify speakers as either a boy, a girl, a man, or a woman. There was no choice but to respond one of these values, there is nothing ‘between’ them, and it doesn’t make sense to talk about, e.g., boy plus girl or woman minus man. Further, although these categories could potentially be ordered in several ways, no ordering is inherent to the categories.</p>
<p>The multinomial distribution is a generalization of the binomial distribution (see section <a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous">10.2</a>) to any number of discrete categories. This distribution will allow us to model the categorization of speakers into boys, girls, men, and women simultaneously rather than modeling this as two binary categorizations (male vs. female and adult vs. child). The multinomial distribution has three parameters:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(n\)</span>: The number of trials, a positive integer.</li>
<li><span class="math inline">\(J\)</span>: The number of possible outcome categories, a positive integer.</li>
<li><span class="math inline">\(p_1, \dots, p_J\)</span>: A vector of probabilities of observing each of the <span class="math inline">\(J\)</span> outcomes where <span class="math inline">\(\sum p_j = 1\)</span>.</li>
</ol>
<p>The multinomial distribution (equation <a href="multinomial-and-ordinal-regression.html#eq:12-1">(12.1)</a>) takes the above parameters and generates a vector of counts (<span class="math inline">\(y_1, \dots, y_J\)</span>) representing the number of observations for each category. For every observation of a multinomial variable, the sum of the outcome probabilities (<span class="math inline">\(p_1, \dots, p_J\)</span>) must equal 1 and the sum of the outcomes (<span class="math inline">\(y_1, \dots, y_J\)</span>) must equal n, the total number of trials.</p>
<p><span class="math display" id="eq:12-1">\[
\begin{equation}
y_1, \dots, y_J \sim \mathrm{multinomial}(p_1, \dots, p_J, n)
\tag{12.1}
\end{equation}
\]</span></p>
<p>We can begin by thinking about the case where n=1. In this case our <span class="math inline">\(y\)</span> vector will equal 1 for one category and 0 for all other categories. Actually, data like this could be represented using a <strong>categorical distribution</strong>, which is just like the multinomial distribution except the n is <em>always</em> equal to 1. The multinomial distribution is to the categorical distribution as the binomial distribution is to the Bernoulli distribution (see chapter 10). We will focus on the multinomial distribution here, however, we will provide an example of how to fit our model using a categorical distribution a bit later in the chapter.</p>
<p>Below we draw 10 instances of a four-category multinomial variable with probabilities of 0.4, 0.1, 0.2, and 0.3 for the first, second, third and fourth categories respectively. In each case, the variable we sample has n=1.</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="multinomial-and-ordinal-regression.html#cb441-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rmultinom</span> (<span class="dv">10</span>, <span class="dv">1</span>, <span class="fu">c</span>(.<span class="dv">4</span>,.<span class="dv">1</span>,.<span class="dv">2</span>,.<span class="dv">3</span>))</span>
<span id="cb441-2"><a href="multinomial-and-ordinal-regression.html#cb441-2" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]</span></span>
<span id="cb441-3"><a href="multinomial-and-ordinal-regression.html#cb441-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]    1    0    0    1    0    0    0    0    0     0</span></span>
<span id="cb441-4"><a href="multinomial-and-ordinal-regression.html#cb441-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]    0    0    1    0    0    1    0    0    0     0</span></span>
<span id="cb441-5"><a href="multinomial-and-ordinal-regression.html#cb441-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]    0    1    0    0    1    0    0    0    1     0</span></span>
<span id="cb441-6"><a href="multinomial-and-ordinal-regression.html#cb441-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]    0    0    0    0    0    0    1    1    0     1</span></span></code></pre></div>
<p>Each individual observation (column) consists of only a single one and three zeros. However, in the code below we can see that if we average across a large number of such observations (across rows), our estimates of <span class="math inline">\(p_1, \dots, p_J\)</span> closely match the real parameters.</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="multinomial-and-ordinal-regression.html#cb442-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rowMeans</span> (<span class="fu">rmultinom</span> (<span class="dv">10000</span>, <span class="dv">1</span>, <span class="fu">c</span>(.<span class="dv">4</span>,.<span class="dv">1</span>,.<span class="dv">2</span>,.<span class="dv">3</span>)))</span>
<span id="cb442-2"><a href="multinomial-and-ordinal-regression.html#cb442-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.3985 0.1020 0.1987 0.3008</span></span></code></pre></div>
<p>Below we sample ten more multinomial variables, this time each with n=100. We can see that we get a distribution of values across the four outcomes for each variable that resembles the values of <span class="math inline">\(p\)</span> we defined for each outcome. Note that when we average across these observations, we get a good estimate of the <span class="math inline">\(p\)</span> parameter associated with each outcome.</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="multinomial-and-ordinal-regression.html#cb443-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span> (<span class="dv">1</span>)</span>
<span id="cb443-2"><a href="multinomial-and-ordinal-regression.html#cb443-2" aria-hidden="true" tabindex="-1"></a>multinomial_variable <span class="ot">=</span> <span class="fu">rmultinom</span> (<span class="dv">10</span>, <span class="dv">100</span>, <span class="fu">c</span>(.<span class="dv">4</span>,.<span class="dv">1</span>,.<span class="dv">2</span>,.<span class="dv">3</span>))</span>
<span id="cb443-3"><a href="multinomial-and-ordinal-regression.html#cb443-3" aria-hidden="true" tabindex="-1"></a>multinomial_variable</span>
<span id="cb443-4"><a href="multinomial-and-ordinal-regression.html#cb443-4" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]</span></span>
<span id="cb443-5"><a href="multinomial-and-ordinal-regression.html#cb443-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   42   37   41   41   41   37   36   47   31    37</span></span>
<span id="cb443-6"><a href="multinomial-and-ordinal-regression.html#cb443-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   10   15    7   14    8   14   13   10   14    12</span></span>
<span id="cb443-7"><a href="multinomial-and-ordinal-regression.html#cb443-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   24   21   18   15   19   18   22   16   22    22</span></span>
<span id="cb443-8"><a href="multinomial-and-ordinal-regression.html#cb443-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   24   27   34   30   32   31   29   27   33    29</span></span>
<span id="cb443-9"><a href="multinomial-and-ordinal-regression.html#cb443-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb443-10"><a href="multinomial-and-ordinal-regression.html#cb443-10" aria-hidden="true" tabindex="-1"></a><span class="co"># the row means closely approximate p</span></span>
<span id="cb443-11"><a href="multinomial-and-ordinal-regression.html#cb443-11" aria-hidden="true" tabindex="-1"></a><span class="fu">rowMeans</span>(multinomial_variable)<span class="sc">/</span><span class="dv">100</span></span>
<span id="cb443-12"><a href="multinomial-and-ordinal-regression.html#cb443-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.390 0.117 0.197 0.296</span></span></code></pre></div>
<p>Multinomial regression is just what it sounds like: A regression model for the prediction of multinomial outcome variables. There are many ways to think about multinomial regression. We’re going to present one that’s consistent with the way we presented logistic regression in chapter 10 and the way we’ve been presenting regression models in general in this book. For a more complete treatment of the topic please see Agresti (2003), Agresti (2018), and Kruschke (2014). Before beginning the explanation we can ‘spoil’ the ending: The number of trials (<span class="math inline">\(n\)</span>) and the number of possible outcomes (<span class="math inline">\(J\)</span>) are aspects of your data and experimental design, and are known to you before you fit any model. Thus, multinomial logistic regression effectively consists of estimating <span class="math inline">\(p_j\)</span>, or values related to it, for each category as a function of your independent variables. In other words, multinomial models let you predict how probable it is that you will see each of the outcome categories, given your independent variables and model structure.</p>
<div id="multinomial-logits-and-the-softmax-function" class="section level3 hasAnchor" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Multinomial logits and the softmax function<a href="multinomial-and-ordinal-regression.html#multinomial-logits-and-the-softmax-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Multinomial regression is a surprisingly ‘simple’ extension of the concepts underlying logistic regression. When we discussed logistic regression in chapter 10, we introduced the inverse logit function (the link function for logistic regression). In section <a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit">10.4.2</a>, we discussed the fact that this function converts log odds to probabilities by arbitrarily setting the ‘count’ of failures to 1 and modeling only the expected ‘count’ of outcomes (i.e. <span class="math inline">\(e^z\)</span>). We see in equation <a href="multinomial-and-ordinal-regression.html#eq:12-2">(12.2)</a> that the probability that <span class="math inline">\(y=1\)</span> (i.e. a ‘success’) is equal to the ratio of the expected number of successes over the sum of the total number of outcomes.</p>
<p><span class="math display" id="eq:12-2">\[
\begin{equation}
\begin{split}
P(y=1) = \frac{\mathrm{success}}{\mathrm{failure}+\mathrm{success}} = \frac{e^{z}}{1+e^{z}}
\end{split}
\tag{12.2}
\end{equation}
\]</span></p>
<p>The equation above works when we have exactly two categories. However, it can be modified so that it can be applied to three or more categories. First, we assume that rather than exactly two possible outcomes, there are now <span class="math inline">\(J\)</span> outcomes, where <span class="math inline">\(J\)</span> is some integer larger than one. If we stick to our interpretation of the value of <span class="math inline">\(e^{z}\)</span> as an expected count for that category, then <span class="math inline">\(e^{z_j}\)</span> is the expected count for category <span class="math inline">\(j\)</span>. To find the probability of observing category <span class="math inline">\(j\)</span> we find the ratio of the ‘count’ of <span class="math inline">\(j\)</span> over the sum of all possible outcomes (i.e., the sum of counts for all categories). This is just an extension of the sum of successes and failures when we had only two categories as in equation <a href="multinomial-and-ordinal-regression.html#eq:12-2">(12.2)</a>.</p>
<p><span class="math display" id="eq:12-3">\[
\begin{equation}
\begin{split}
P(Y=j) = \frac{e^{z_j}}{\sum_{j=1}^{J} e^{z_j}}
\end{split}
\tag{12.3}
\end{equation}
\]</span></p>
<p>The function above is called the <strong>softmax</strong> function, and it is basically a generalization of the inverse logit function to more than two categories. When we did dichotomous logistic regression, we modeled only the ‘count’ of one variable, the one for ‘success’. The ‘count’ for the category set to ‘failure’ was not modeled and was set to 1 (by setting <span class="math inline">\(z=0\)</span>, and <span class="math inline">\(e^{0}=1\)</span>) for all cases. When we model multinomial data we follow the same convention and set one category as the ‘reference’. To do this, we modify the equation above to pull the first category out of the summation and set its count to 1 for all situations, as in equation <a href="multinomial-and-ordinal-regression.html#eq:12-4">(12.4)</a>. Notice that the summation in the denominator of the fraction now begins at two. In chapter 10 we said that you can think of the inverse logit function as relating expected successes relative to one failure. You can think of the softmax function as relating the expected number of outcomes of one category to the sum of the expected number of outcomes of all categories, where one of the counts (the reference category) has been set to one.</p>
<p><span class="math display" id="eq:12-4">\[
\begin{equation}
\begin{split}
P(Y=j) = \frac{e^{z_j}}{1 + \sum_{j=2}^{J} e^{z_j}}
\end{split}
\tag{12.4}
\end{equation}
\]</span></p>
<p>The equation above results in a set of probabilities which can serve as the multinomial parameters, <span class="math inline">\(p_1, \dots , p_j\)</span>, for categories <span class="math inline">\(1, \dots, J\)</span>. These probabilities are modeled by estimating <span class="math inline">\(z_j\)</span> as the linear combination of our predictor variables based on some unknown parameters (<span class="math inline">\(\beta_j\)</span>). Each outcome has a different prediction equation for <span class="math inline">\(z_j\)</span> so that a multinomial regression has <span class="math inline">\(J\)</span> prediction equations, one for each outcome. In addition, each prediction equation combines the same <span class="math inline">\(x_k\)</span> predictors, albeit in a category-specific way (based on the <span class="math inline">\(\beta_j\)</span> parameters for that category). This can be seen in the equation below representing the prediction of <span class="math inline">\(z_j\)</span> for category <span class="math inline">\(j\)</span> based on the coefficients (but not predictors) specific to that outcome.</p>
<p><span class="math display" id="eq:12-5">\[
\begin{equation}
z_j = \beta_j + \beta_{j1} \cdot x_1 + \beta_{j2} \cdot x_2 + \dots + \beta_{jk} \cdot x_k
\tag{12.5}
\end{equation}
\]</span></p>
<p>The prediction equation for the ‘reference’ category (<code>brm</code> uses the alphabetically-first category) is fixed to have a value of 0 for <span class="math inline">\(z_j\)</span>. One way to accomplish this is to not estimate parameters for the reference category and instead fix these to zero, as seen in <a href="multinomial-and-ordinal-regression.html#eq:12-6">(12.6)</a></p>
<p><span class="math display" id="eq:12-6">\[
\begin{equation}
\begin{split}
z_1 = \alpha_1 + \beta_{11} \cdot x_1 + \beta_{12} \cdot x_2 + \dots + \beta_{1k} \cdot x_k \\
z_1 = 0 + 0 \cdot x_1 + 0 \cdot x_2 + \dots + 0 \cdot x_k \\
z_1 = 0
\end{split}
\tag{12.6}
\end{equation}
\]</span></p>
<p>When we carried out logistic regression, the variable <span class="math inline">\(z\)</span> was referred to as a logit. Sometimes the <span class="math inline">\(z_j\)</span> values involved in multinomial regression are called <strong>multinomial logits</strong> to highlight their similarity to the dichotomous logit. More often, these values are referred to as the <strong>score</strong> for each category. The ‘meaning’ of the score is somewhat vaguely defined, in part because there are many ways to think about it. One way to think of the score is that it is a <em>latent variable</em> associated with each category that relates to the probability of observing that category given the values of the independent variables.</p>
<p><strong>Latent variables</strong> are variables that are inferred mathematically using some statistical model, but are not observed directly. For example, in our logistic regression model in chapter 10 we predicted the perception of femaleness. We did this by predicting variation in the logit of the probability of observing a female response as a function of speaker vocal-tract length. The logit of this probability can be thought of as a latent variable representing something like ‘femininity/femaleness’ in the mind of the listener. Based on the ‘feeling’ of voice femininity that the listener has, they produce the surface variable that we do measure: A binary classification of a speaker as female/male. In this view the realization of the response variable (a female classification) is the result of a secret, <em>latent</em>, variable that is not directly observed (or observable) by us, but which we assume underlies the process.</p>
<p>In our four-category multinomial model we can imagine four latent variables, the score for each category. We can <em>think</em> of these as the ‘feeling’ the listener has regarding the ‘boyness’, ‘girlness’, ‘manness’, and ‘womanness’ of the voice. What is the ‘boyness’ of a voice? The difficult-to-quantify internal knowledge a listener has that the speaker is a boy, as opposed to some other sort of speaker. These latent variables are not directly measured in our experiment but we think they underlie the classification of speakers into categories. Thus, by observing the categorization of speakers, we can try to make inferences about these underlying variables, and the way that they relate to our predictors.</p>
</div>
<div id="comparison-to-logistic-regression" class="section level3 hasAnchor" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> Comparison to logistic regression<a href="multinomial-and-ordinal-regression.html#comparison-to-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>At this point we’ve laid out the basics of multinomial regression, however, our presentation has been very quick and fairly abstract. To make our example more concrete we’re going to talk about a small toy example using a single predictor and three categories, and show that a multinomial model with a single quantitative predictor is a modest extension of a logistic regression model with a single quantitative predictor.</p>
<p>Let’s assume that we’re interested in the prediction of a variable <span class="math inline">\(Y\)</span> with two outcome categories, 1 and 2. We arbitrarily set 2 to ‘success’ and make it equal to 1 in the vector representing the dependent variable. Supposed we are interested in predicting the probability of observing a success, i.e. <span class="math inline">\(Y=2\)</span>, using a single quantitative predictor <span class="math inline">\(x\)</span>. We model the logit of the probability of observing a success using a line with a given intercept and slope for the quantitative predictor. We set the value of failures to 0 logits for all cases, which is equivalent to representing the failure category with a horizontal line whose intercept is zero. So, our logistic model can be thought of as consisting of the two prediction equations in <a href="multinomial-and-ordinal-regression.html#eq:12-00">(12.7)</a>, presented in the top left plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-1">12.1</a>.</p>
<p><span class="math display" id="eq:12-00">\[
\begin{equation}
\begin{split}
z_1= 0 + x \cdot 0
\\
z_2= -3 + x \cdot -3
\end{split}
\tag{12.7}
\end{equation}
\]</span></p>
<div class="figure"><span style="display:block;" id="fig:F12-1"></span>
<img src="_main_files/figure-html/F12-1-1.jpeg" alt="(top left) Lines indicate the logit of the probability of observing a success (blue line), or a failure (black line). (top right) Lines indicate the probabilities associated with observing the outcomes in the top left plot. (bottom left) Lines represent scores for the reference category (black line), the second category (blue line), and the third category (red line). (bottom right) Lines indicate the probabilities associated with observing the categories in the bottom left plot." width="4800" />
<p class="caption">
Figure 12.1: (top left) Lines indicate the logit of the probability of observing a success (blue line), or a failure (black line). (top right) Lines indicate the probabilities associated with observing the outcomes in the top left plot. (bottom left) Lines represent scores for the reference category (black line), the second category (blue line), and the third category (red line). (bottom right) Lines indicate the probabilities associated with observing the categories in the bottom left plot.
</p>
</div>
<p>Rather than use the inverse logit function to convert logits to probabilities, we will use the softmax function (introduced above), since we know that these are equivalent when there are two categories. Below, we see that when we set the ‘count’ for failures to 1, i.e. set <span class="math inline">\(z_1=0\)</span> so that <span class="math inline">\(e^{z_1}=1\)</span>, the softmax function really does ‘become’ the inverse logit function when calculating the probability of a success (<span class="math inline">\(P(Y=2)\)</span>). We also use this approach to calculate the probability of a failure (<span class="math inline">\(P(Y=1)\)</span>). Of course, this is not usually done because <span class="math inline">\(P(Y=2) = 1-P(Y=2)\)</span>, however, we do this explicitly here for illustrative purposes.</p>
<p><span class="math display" id="eq:12-001">\[
\begin{equation}
\begin{split}
P(Y=2) = \frac{e^{z_2}}{e^{z_1} + e^{z_2}} = \frac{e^{z_2}}{1 + e^{z_2}} \\ \\
P(Y=1) = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2}}
\end{split}
\tag{12.8}
\end{equation}
\]</span></p>
<p>You can use the code below to recreate the lines in the top right row of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-1">12.1</a>, and to find the probabilities in the top right plot. The probability of observing a success (category 2, blue line) looks just like the sigmoid curves we saw when doing a logistic regression in chapter 10. In figure <a href="multinomial-and-ordinal-regression.html#fig:F12-1">12.1</a> we also plot the curve associated with failures. Normally this is not plotted for logistic regression since it is just the curve for successes mirrored about the line at p=0.5.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="multinomial-and-ordinal-regression.html#cb444-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predictor from -3 to 3</span></span>
<span id="cb444-2"><a href="multinomial-and-ordinal-regression.html#cb444-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="fl">0.1</span>)</span>
<span id="cb444-3"><a href="multinomial-and-ordinal-regression.html#cb444-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb444-4"><a href="multinomial-and-ordinal-regression.html#cb444-4" aria-hidden="true" tabindex="-1"></a><span class="co"># lines for failures</span></span>
<span id="cb444-5"><a href="multinomial-and-ordinal-regression.html#cb444-5" aria-hidden="true" tabindex="-1"></a>z_1 <span class="ot">=</span> <span class="dv">0</span> <span class="sc">+</span> x<span class="sc">*</span><span class="dv">0</span></span>
<span id="cb444-6"><a href="multinomial-and-ordinal-regression.html#cb444-6" aria-hidden="true" tabindex="-1"></a><span class="co"># line for successes</span></span>
<span id="cb444-7"><a href="multinomial-and-ordinal-regression.html#cb444-7" aria-hidden="true" tabindex="-1"></a>z_2 <span class="ot">=</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">+</span> x<span class="sc">*-</span><span class="dv">3</span></span>
<span id="cb444-8"><a href="multinomial-and-ordinal-regression.html#cb444-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb444-9"><a href="multinomial-and-ordinal-regression.html#cb444-9" aria-hidden="true" tabindex="-1"></a>scores <span class="ot">=</span> <span class="fu">cbind</span> (z1,z2)</span>
<span id="cb444-10"><a href="multinomial-and-ordinal-regression.html#cb444-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb444-11"><a href="multinomial-and-ordinal-regression.html#cb444-11" aria-hidden="true" tabindex="-1"></a><span class="co"># softmax function, exponentiate and divide by sum</span></span>
<span id="cb444-12"><a href="multinomial-and-ordinal-regression.html#cb444-12" aria-hidden="true" tabindex="-1"></a>probabilities <span class="ot">=</span> <span class="fu">exp</span> (scores) <span class="sc">/</span> <span class="fu">rowSums</span> (<span class="fu">exp</span> (scores))</span>
<span id="cb444-13"><a href="multinomial-and-ordinal-regression.html#cb444-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb444-14"><a href="multinomial-and-ordinal-regression.html#cb444-14" aria-hidden="true" tabindex="-1"></a><span class="co"># simple version of the top right plot in Figure 12.1</span></span>
<span id="cb444-15"><a href="multinomial-and-ordinal-regression.html#cb444-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (probabilities[,<span class="dv">1</span>], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span>
<span id="cb444-16"><a href="multinomial-and-ordinal-regression.html#cb444-16" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (probabilities[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p>To turn our logistic regression into a multinomial regression, let’s imagine we want to model a third outcome category, <span class="math inline">\(Y=3\)</span>. We add a line predicting the ‘logit’, now a <em>score</em>, for this category in <a href="multinomial-and-ordinal-regression.html#eq:12-002">(12.9)</a>. We now have three lines predicting scores (bottom left, figure <a href="multinomial-and-ordinal-regression.html#fig:F12-1">12.1</a>), one of which is still horizontal (<span class="math inline">\(z_1\)</span>). This is no longer a ‘failure’ but rather the <em>reference category</em>.</p>
<p><span class="math display" id="eq:12-002">\[
\begin{equation}
\begin{split}
z_1= 0 + x \cdot 0
\\
z_2= -3 + x \cdot -3
\\
z_3= -3 + x \cdot 3
\end{split}
\tag{12.9}
\end{equation}
\]</span></p>
<p>To get the predicted probability of observing each outcome we enter these scores into the softmax function, in ‘expanded’ form in <a href="multinomial-and-ordinal-regression.html#eq:12-003">(12.10)</a>.</p>
<p><span class="math display" id="eq:12-003">\[
\begin{equation}
\begin{split}
P(Y=3) = \frac{e^{z_3}}{1 + e^{z_2} + e^{z_3}}
\\ \\
P(Y=2) = \frac{e^{z_2}}{1 + e^{z_2} + e^{z_3}}
\\ \\
P(Y=1) = \frac{1}{1 + e^{z_2} + e^{z_3}}
\end{split}
\tag{12.10}
\end{equation}
\]</span></p>
<p>You can use the code below to recreate this process, and the lines in the bottom row of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-1">12.1</a>. Note that all we’ve done is add a third line and a third score. Apart from that, the process is the same as it was for logistic regression.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="multinomial-and-ordinal-regression.html#cb445-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predictor from -3 to 3</span></span>
<span id="cb445-2"><a href="multinomial-and-ordinal-regression.html#cb445-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="fl">0.1</span>)</span>
<span id="cb445-3"><a href="multinomial-and-ordinal-regression.html#cb445-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb445-4"><a href="multinomial-and-ordinal-regression.html#cb445-4" aria-hidden="true" tabindex="-1"></a><span class="co"># lines for reference category</span></span>
<span id="cb445-5"><a href="multinomial-and-ordinal-regression.html#cb445-5" aria-hidden="true" tabindex="-1"></a>z_1 <span class="ot">=</span> <span class="dv">0</span> <span class="sc">+</span> x<span class="sc">*</span><span class="dv">0</span></span>
<span id="cb445-6"><a href="multinomial-and-ordinal-regression.html#cb445-6" aria-hidden="true" tabindex="-1"></a><span class="co"># line for category two</span></span>
<span id="cb445-7"><a href="multinomial-and-ordinal-regression.html#cb445-7" aria-hidden="true" tabindex="-1"></a>z_2 <span class="ot">=</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">+</span> x<span class="sc">*-</span><span class="dv">3</span></span>
<span id="cb445-8"><a href="multinomial-and-ordinal-regression.html#cb445-8" aria-hidden="true" tabindex="-1"></a><span class="co"># line for category three</span></span>
<span id="cb445-9"><a href="multinomial-and-ordinal-regression.html#cb445-9" aria-hidden="true" tabindex="-1"></a>z_2 <span class="ot">=</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">+</span> x<span class="sc">*</span><span class="dv">3</span></span>
<span id="cb445-10"><a href="multinomial-and-ordinal-regression.html#cb445-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb445-11"><a href="multinomial-and-ordinal-regression.html#cb445-11" aria-hidden="true" tabindex="-1"></a>scores <span class="ot">=</span> <span class="fu">cbind</span> (z1,z2,z3)</span>
<span id="cb445-12"><a href="multinomial-and-ordinal-regression.html#cb445-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb445-13"><a href="multinomial-and-ordinal-regression.html#cb445-13" aria-hidden="true" tabindex="-1"></a><span class="co"># softmax function, exponentiate and divide by sum</span></span>
<span id="cb445-14"><a href="multinomial-and-ordinal-regression.html#cb445-14" aria-hidden="true" tabindex="-1"></a>probabilities <span class="ot">=</span> <span class="fu">exp</span> (scores) <span class="sc">/</span> <span class="fu">rowSums</span> (<span class="fu">exp</span> (scores))</span>
<span id="cb445-15"><a href="multinomial-and-ordinal-regression.html#cb445-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb445-16"><a href="multinomial-and-ordinal-regression.html#cb445-16" aria-hidden="true" tabindex="-1"></a><span class="co"># simple version of the top right plot in Figure 12.1</span></span>
<span id="cb445-17"><a href="multinomial-and-ordinal-regression.html#cb445-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (probabilities[,<span class="dv">1</span>], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb445-18"><a href="multinomial-and-ordinal-regression.html#cb445-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (probabilities[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span>
<span id="cb445-19"><a href="multinomial-and-ordinal-regression.html#cb445-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (probabilities[,<span class="dv">3</span>], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p>There is one important difference between logistic and multinomial regression related to classification, or expected outcomes, based on probabilities and scores. When there are only two categories, a logit greater than 0 implies a probability greater than 0.5 of success. In previous chapters, we used this characteristic to talk about expected outcomes in specific parts of the stimulus space. Basically, in any part of the stimulus space where a positive logit is predicted, you expect to observe a ‘success’ (1). In any part of the stimulus space where a negative logit is predicted you expect to observe a ‘failure’ (0). Since the logit of failures is always fixed at exactly zero, another way to look at this is that the category with the greatest predicted logit value is expected.</p>
<p>When there are more than two possible outcome categories, there is no guarantee that a positive score for a given outcome means that category is expected. However, it is still the case that the category with the greatest score in a given condition is the predicted outcome for that condition. Another thing to keep in mind is that, unlike in logistic regression, the most probable outcome may not be very probable in a multinomial model. For example, if there are five outcome categories the most probable predicted outcome could have a probability of only 0.24 if the other categories had probabilities of 0.19. Thus, we see that in multinomial regression the most probable outcome doesn’t have to be very probable, it just needs to be more probable than the alternatives.</p>
</div>
<div id="data-and-research-questions-9" class="section level3 hasAnchor" number="12.2.3">
<h3><span class="header-section-number">12.2.3</span> Data and research questions<a href="multinomial-and-ordinal-regression.html#data-and-research-questions-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We load our packages and data below. We also add a new variable, <span class="math inline">\(y\)</span>, representing our multinomial outcome. This variable represents observations of a vector of length four whose first, second, third, and fourth elements represent observed outcomes of ‘boy’, ‘girl’, ‘man’, and ‘woman’, respectively. After this we add a variable called <code>size</code> that always equals 1 because each row in our data frame represents the observation of a single outcome. Of course, the size information may be higher than 1 for your data if some of your rows represent trials where n&gt;1. Finally, we process our quantitative predictors in the same way as in the previous chapters.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="multinomial-and-ordinal-regression.html#cb446-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb446-2"><a href="multinomial-and-ordinal-regression.html#cb446-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb446-3"><a href="multinomial-and-ordinal-regression.html#cb446-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb446-4"><a href="multinomial-and-ordinal-regression.html#cb446-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb446-5"><a href="multinomial-and-ordinal-regression.html#cb446-5" aria-hidden="true" tabindex="-1"></a><span class="co"># new dependent variable</span></span>
<span id="cb446-6"><a href="multinomial-and-ordinal-regression.html#cb446-6" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>y <span class="ot">=</span> <span class="fu">cbind</span>(<span class="at">b =</span> <span class="fu">as.numeric</span>(exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;b&#39;</span>),</span>
<span id="cb446-7"><a href="multinomial-and-ordinal-regression.html#cb446-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">g =</span> <span class="fu">as.numeric</span>(exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;g&#39;</span>),</span>
<span id="cb446-8"><a href="multinomial-and-ordinal-regression.html#cb446-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">m =</span> <span class="fu">as.numeric</span>(exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;m&#39;</span>),</span>
<span id="cb446-9"><a href="multinomial-and-ordinal-regression.html#cb446-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">w =</span> <span class="fu">as.numeric</span>(exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;w&#39;</span>))</span>
<span id="cb446-10"><a href="multinomial-and-ordinal-regression.html#cb446-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb446-11"><a href="multinomial-and-ordinal-regression.html#cb446-11" aria-hidden="true" tabindex="-1"></a><span class="co"># variable representing the size (n) of each observation. They are all 1.</span></span>
<span id="cb446-12"><a href="multinomial-and-ordinal-regression.html#cb446-12" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>size <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb446-13"><a href="multinomial-and-ordinal-regression.html#cb446-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb446-14"><a href="multinomial-and-ordinal-regression.html#cb446-14" aria-hidden="true" tabindex="-1"></a><span class="co"># preparation of quantitative predictors as in previous chapters</span></span>
<span id="cb446-15"><a href="multinomial-and-ordinal-regression.html#cb446-15" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl_original <span class="ot">=</span> exp_data<span class="sc">$</span>vtl</span>
<span id="cb446-16"><a href="multinomial-and-ordinal-regression.html#cb446-16" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl <span class="ot">=</span> exp_data<span class="sc">$</span>vtl <span class="sc">-</span> <span class="fu">mean</span> (exp_data<span class="sc">$</span>vtl)</span>
<span id="cb446-17"><a href="multinomial-and-ordinal-regression.html#cb446-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb446-18"><a href="multinomial-and-ordinal-regression.html#cb446-18" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>f0_original <span class="ot">=</span> exp_data<span class="sc">$</span>f0 </span>
<span id="cb446-19"><a href="multinomial-and-ordinal-regression.html#cb446-19" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>f0 <span class="ot">=</span> exp_data<span class="sc">$</span>f0 <span class="sc">-</span> <span class="fu">mean</span>(exp_data<span class="sc">$</span>f0)</span>
<span id="cb446-20"><a href="multinomial-and-ordinal-regression.html#cb446-20" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>f0 <span class="ot">=</span> exp_data<span class="sc">$</span>f0 <span class="sc">/</span> <span class="dv">100</span></span></code></pre></div>
<p>Below, we print out the first six instances of our dependent variable and compare this to the first six values of the <code>C</code> (apparent category) variable in our data frame. Each row in the matrix below represents a single observation of our multinomial variable. We can see that the dependent variable indicates which category was selected for a given trial using a 1 in the appropriate column and a 0 in the others.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="multinomial-and-ordinal-regression.html#cb447-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first 6 elements of dependent variable</span></span>
<span id="cb447-2"><a href="multinomial-and-ordinal-regression.html#cb447-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (exp_data<span class="sc">$</span>y)</span>
<span id="cb447-3"><a href="multinomial-and-ordinal-regression.html#cb447-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      b g m w</span></span>
<span id="cb447-4"><a href="multinomial-and-ordinal-regression.html#cb447-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] 0 1 0 0</span></span>
<span id="cb447-5"><a href="multinomial-and-ordinal-regression.html#cb447-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] 0 0 0 1</span></span>
<span id="cb447-6"><a href="multinomial-and-ordinal-regression.html#cb447-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,] 0 1 0 0</span></span>
<span id="cb447-7"><a href="multinomial-and-ordinal-regression.html#cb447-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,] 0 1 0 0</span></span>
<span id="cb447-8"><a href="multinomial-and-ordinal-regression.html#cb447-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,] 1 0 0 0</span></span>
<span id="cb447-9"><a href="multinomial-and-ordinal-regression.html#cb447-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,] 1 0 0 0</span></span>
<span id="cb447-10"><a href="multinomial-and-ordinal-regression.html#cb447-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb447-11"><a href="multinomial-and-ordinal-regression.html#cb447-11" aria-hidden="true" tabindex="-1"></a><span class="co"># first 6 elements of apparent speaker category factor</span></span>
<span id="cb447-12"><a href="multinomial-and-ordinal-regression.html#cb447-12" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (exp_data<span class="sc">$</span>C)</span>
<span id="cb447-13"><a href="multinomial-and-ordinal-regression.html#cb447-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;g&quot; &quot;w&quot; &quot;g&quot; &quot;g&quot; &quot;b&quot; &quot;b&quot;</span></span></code></pre></div>
<p>We will try to use our data to answer the following research question:</p>
<p>(Q1) Can we use speaker f0 and VTL to predict apparent speaker category?</p>
<p>Keep in mind we’re trying to predict <em>apparent</em> and not <em>veridical</em> speaker category. Our consideration of the accuracy or utility of this model will depend on how well it represents listener classifications of speakers, no matter how wrong or right these may be. Thus, a model with perfect classification of speakers into veridical categories is not the goal: If listeners make predictable mistakes we want the model to make the same ‘mistakes’.</p>
</div>
<div id="description-of-our-model-3" class="section level3 hasAnchor" number="12.2.4">
<h3><span class="header-section-number">12.2.4</span> Description of our model<a href="multinomial-and-ordinal-regression.html#description-of-our-model-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our model formula is largely similar to those we have seen before, with two minor changes. Beside the dependent variable, separated by a pipe (<code>|</code>), we need to include a variable that indicates the integer number of trials for each observation. For us this will always be 1, but in many situations this can be a wider range of values. We want to predict our counts of categorizations with respect to the value of speaker VTL and f0, and so the model formula we are going to use is seen below.</p>
<p><code>y|trials(size) ~ vtl+f0 + (vtl+f0|x|L) + (1|y|S)</code></p>
<p>Since we’re predicting category membership with two quantitative predictors, we know that the surfaces our model defines are planes. In the models we’ve fit to this point, we had a single dependent variable and a single plane for a single condition. So, our model formulas previously were something like:</p>
<p><code>y ~ vtl+f0 + (vtl+f0|S) + (1|L)</code></p>
<p>However, in a multinomial regression we have one plane for each response category, so our formula above could really be thought of as:</p>
<p><code>y_1 ~ vtl+f0 + (vtl+f0|x|L) + (1|y|S)</code></p>
<p><code>y_2 ~ vtl+f0 + (vtl+f0|x|L) + (1|y|S)</code></p>
<p><code>y_3 ~ vtl+f0 + (vtl+f0|x|L) + (1|y|S)</code></p>
<p><code>y_4 ~ vtl+f0 + (vtl+f0|x|L) + (1|y|S)</code></p>
<p>As noted in section <a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2">8.6.1</a>, the <code>|x|</code> notation tells our model that the <code>L</code> predictors are related across the prediction equations for each outcome category. In doing this, we estimate the listener effects as coming from a single nine-dimensional normal distribution rather than three separate three dimensional normal variables. Similarly, by adding <code>|y|</code> before our <code>S</code> predictor we model the speaker effects for each outcome category as coming from a single three-dimensional normal distribution rather than three unidimensional ones, and also model the correlations between the dimensions.</p>
<p>Since our multinomial model predicts a response surface for each outcome category, our multinomial model will involve four times as many coefficients as an equivalent Gaussian model. Of course, we noted above that the coefficients of the reference category are set to zero and not estimated. This means that, in general, a multinomial model predicting a variable with <span class="math inline">\(J\)</span> possible outcomes results in <span class="math inline">\(J-1\)</span> times as many parameters as an equivalent model predicting a quantitative variable. Our full model specification is pretty large, so we split it into two parts. First, the part specifying our predictions and combinations of predictors:</p>
<p><span class="math display" id="eq:12-7a">\[
\begin{equation}
\begin{split}
y_{1[i]},y_{2[i]},y_{3[i]},y_{4[i]} \sim \mathrm{multinomial}(p_{1[i]},p_{2[i]},p_{3[i]},p_{4[i]}, size_{[i]}) \\ \\
\mathrm{for} \; j = 1, \dots, 4\\ \\
p_{j[i]} = \frac{e^{z_{j[i]}}}{\sum_{j=1}^{J} e^{z_{j[i]}}} \\
\\
z_{j[i]} = \mathrm{a}_j + b_{j[i]} \cdot \mathrm{vtl}_{[i]} + c_{j[i]} \cdot \mathrm{f0}_{[i]}  \\
a_{j[i]} = \mathrm{Intercept}_j + L_{j[\mathsf{L}_{[i]}]} + S_{j[\mathsf{S}_{[i]}]} \\
b_{j[i]} =  VTL_j + VTL_j \colon L_{j[\mathsf{L}_{[i]}]} \\
c_{j[i]} =  F0_j + F0_j \colon L_{j[\mathsf{L}_{[i]}]} \\
\end{split}
\tag{12.11}
\end{equation}
\]</span></p>
<p>And a plain English description of this part:</p>
<blockquote>
<p>Our vector of counts (<span class="math inline">\(y_{1[i]},y_{2[i]},y_{3[i]},y_{4[i]}\)</span>) is assumed to come from a multinomial distribution with unknown parameters <span class="math inline">\(p_{1[i]},p_{2[i]},p_{3[i]},p_{4[i]}\)</span>, and an n equal to 1 (size) for all trials. For each of our j response categories, the probability of observing that outcome on a trial i, <span class="math inline">\(p_{j[i]}\)</span>, is found by combining the score for each category for that trial (<span class="math inline">\(z_{j[i]}\)</span>) using the softmax link function. The score for a trial is equal to a trial-dependent combination of an intercept, an effect for VTL, and an effect for f0. The intercept for category j for trial i (<span class="math inline">\(a_{j[i]}\)</span>) varies according to an overall model intercept for that category (<span class="math inline">\(\mathrm{Intercept}_{j}\)</span>) and speaker (<span class="math inline">\(S_{j[\mathsf{S}_{[i]}]}\)</span>) and listener-dependent (<span class="math inline">\(L_{j[\mathsf{L}_{[i]}]}\)</span>) variations from this. The VTL slope for category j for trial i (<span class="math inline">\(b_{j[i]}\)</span>) varies according to an overall VTL slope for that category (<span class="math inline">\(VTL_{j}\)</span>) and listener-dependent (<span class="math inline">\(VTL \colon L_{j[\mathsf{L}_{[i]}]}\)</span>) variations from this. The f0 slope for category j for trial i (<span class="math inline">\(c_{j[i]}\)</span>) varies according to an overall f0 slope for that category (<span class="math inline">\(F0_{j}\)</span>) and listener-dependent (<span class="math inline">\(F0 \colon L_{j[\mathsf{L}_{[i]}]}\)</span>) variations from this.</p>
</blockquote>
<p>In <a href="multinomial-and-ordinal-regression.html#eq:12-7b">(12.12)</a> we specify the rest of our model (the priors):</p>
<p><span class="math display" id="eq:12-7b">\[
\begin{equation}
\begin{split}
\mathrm{for} \; j = 2, \dots, 4\\
\mathrm{Intercept}_j \sim \mathrm{t}(3, 0, 3) \\
VTL_j, F0_j \sim \mathrm{t}(3, 0, 3) \\
\sigma_{L_j}, \sigma_{VTL_j \colon L_j}, \sigma_{F0_j \colon L_j}, \sigma_{S_j} \sim \mathrm{t}(3, 0, 3) \\
\\
\begin{bmatrix} S_{2[\bullet]} \\ S_{3[\bullet]} \\ S_{4[\bullet]} \end{bmatrix}    
\sim \mathrm{MVNormal} \left(\, \begin{bmatrix} 0\\ 0 \\ 0 \\ \end{bmatrix}, \mathrm{\Sigma_S} \right) \\
\\
\begin{bmatrix}
L_{2[\bullet]} \\ VTL_2 \colon L_{2[\bullet]} \\F0_2 \colon L_{2[\bullet]} \\
L_{3[\bullet]} \\ VTL_3 \colon L_{3[\bullet]} \\F0_3 \colon L_{3[\bullet]} \\
L_{4[\bullet]} \\ VTL_4 \colon L_{4[\bullet]} \\F0_4 \colon L_{4[\bullet]} \\
\end{bmatrix}   
\sim \mathrm{MVNormal} \left(\, \begin{bmatrix} 0\\ 0 \\ 0 \\ 0\\ 0 \\ 0 \\
0\\ 0 \\ 0 \\\end{bmatrix}, \mathrm{\Sigma_L} \right) \\
R_S \sim \mathrm{LKJCorr} (2) \\
R_L \sim \mathrm{LKJCorr} (2)
\end{split}
\tag{12.12}
\end{equation}
\]</span></p>
<p>And provide a plan English description of this second part:</p>
<blockquote>
<p>We specify priors for response categories 2 to four since all parameters are set to zero for the first (reference) category. The ‘fixed’ effects and correlations were given prior distributions appropriate for their expected range of values. The three speaker dependent terms for each listener (one for each modeled outcome category) were drawn from a three dimensional normal distribution with standard deviations (<span class="math inline">\(\sigma_{S_2}, \sigma_{S_2}, \sigma_{S_3}\)</span>) and a correlation matrix (<span class="math inline">\(R_S\)</span>) that was estimated from the data. The nine listener effects were drawn from a nine-dimensional normal distribution with standard deviations (<span class="math inline">\(\sigma_{L_2}, \sigma_{VTL_2 \colon L_2}, \sigma_{F0_3 \colon L_3}, \, \dots\)</span>) and a correlation matrix (<span class="math inline">\(R_L\)</span>) that was estimated from the data.</p>
</blockquote>
</div>
<div id="fitting-the-model-3" class="section level3 hasAnchor" number="12.2.5">
<h3><span class="header-section-number">12.2.5</span> Fitting the model<a href="multinomial-and-ordinal-regression.html#fitting-the-model-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is one major difference in how we need to specify priors for multinomial models: We need to specify priors individually for each response category with modeled parameters. This is done by passing the name of the categorical variable to the <code>dpar</code> parameter. Above, we named our response variables according to the letters we have been using throughout this text. We can see this below.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="multinomial-and-ordinal-regression.html#cb448-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span> (exp_data<span class="sc">$</span>y)</span>
<span id="cb448-2"><a href="multinomial-and-ordinal-regression.html#cb448-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;b&quot; &quot;g&quot; &quot;m&quot; &quot;w&quot;</span></span></code></pre></div>
<p>The name passed to <code>dpar</code> will be <code>muCategory</code> where <code>Category</code> corresponds to the category name. This means we need to specify priors for <code>mug</code>, <code>mum</code>, and <code>muw</code>, but not <code>mub</code>. We specify our priors below:</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="multinomial-and-ordinal-regression.html#cb449-1" aria-hidden="true" tabindex="-1"></a>multinomial_prior <span class="ot">=</span> </span>
<span id="cb449-2"><a href="multinomial-and-ordinal-regression.html#cb449-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;mug&quot;</span>),</span>
<span id="cb449-3"><a href="multinomial-and-ordinal-regression.html#cb449-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;mug&quot;</span>),</span>
<span id="cb449-4"><a href="multinomial-and-ordinal-regression.html#cb449-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;mug&quot;</span>),</span>
<span id="cb449-5"><a href="multinomial-and-ordinal-regression.html#cb449-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;mum&quot;</span>),</span>
<span id="cb449-6"><a href="multinomial-and-ordinal-regression.html#cb449-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;mum&quot;</span>),</span>
<span id="cb449-7"><a href="multinomial-and-ordinal-regression.html#cb449-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;mum&quot;</span>),</span>
<span id="cb449-8"><a href="multinomial-and-ordinal-regression.html#cb449-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;muw&quot;</span>),</span>
<span id="cb449-9"><a href="multinomial-and-ordinal-regression.html#cb449-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;muw&quot;</span>),</span>
<span id="cb449-10"><a href="multinomial-and-ordinal-regression.html#cb449-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;muw&quot;</span>),</span>
<span id="cb449-11"><a href="multinomial-and-ordinal-regression.html#cb449-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>))</span></code></pre></div>
<p>And here is the code to fit our model, using the <code>multinomial</code> family for the first time:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="multinomial-and-ordinal-regression.html#cb450-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb450-2"><a href="multinomial-and-ordinal-regression.html#cb450-2" aria-hidden="true" tabindex="-1"></a>model_multinomial <span class="ot">=</span> </span>
<span id="cb450-3"><a href="multinomial-and-ordinal-regression.html#cb450-3" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (y<span class="sc">|</span><span class="fu">trials</span>(size) <span class="sc">~</span> vtl<span class="sc">+</span>f0 <span class="sc">+</span> (vtl<span class="sc">+</span>f0<span class="sc">|</span>x<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>y<span class="sc">|</span>S), </span>
<span id="cb450-4"><a href="multinomial-and-ordinal-regression.html#cb450-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>exp_data, <span class="at">family=</span><span class="st">&quot;multinomial&quot;</span>, <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, </span>
<span id="cb450-5"><a href="multinomial-and-ordinal-regression.html#cb450-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>, <span class="at">prior =</span> multinomial_prior)</span></code></pre></div>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="multinomial-and-ordinal-regression.html#cb451-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb451-2"><a href="multinomial-and-ordinal-regression.html#cb451-2" aria-hidden="true" tabindex="-1"></a>model_multinomial <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&quot;12_model_multinomial.RDS&quot;</span>)</span></code></pre></div>
<p>By the way, if you wanted to fit the model above using the categorical distribution, you would use the code below. Notice that there are only three changes compared to the code to fit our multinomial model above. First, the <code>family</code> parameter is now equal to <code>categorical</code>. Second, the <code>trials</code> specification is now omitted since the number of observations for each trial is now 1. Finally, we can use our vector identifying the outcome category directly as our dependent variable.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="multinomial-and-ordinal-regression.html#cb452-1" aria-hidden="true" tabindex="-1"></a>model_categorical <span class="ot">=</span> </span>
<span id="cb452-2"><a href="multinomial-and-ordinal-regression.html#cb452-2" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (C <span class="sc">~</span> vtl<span class="sc">+</span>f0 <span class="sc">+</span> (vtl<span class="sc">+</span>f0<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), </span>
<span id="cb452-3"><a href="multinomial-and-ordinal-regression.html#cb452-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>exp_data, <span class="at">family=</span><span class="st">&quot;categorical&quot;</span>, <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, </span>
<span id="cb452-4"><a href="multinomial-and-ordinal-regression.html#cb452-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>, <span class="at">prior =</span> multinomial_prior)</span></code></pre></div>
<p>Why would we ever use a multinomial analysis when the categorical distribution seems simpler? First, in some cases individual trials might contain multiple observations such that an individual analysis is not possible. However, a multinomial analysis can also be much faster by substantially reducing the number of individual observations that need to be taken into account when calculating likelihoods. For example, imagine we were only interested in predicting the probability that a listener would report each speaker category. If we run a model like this:</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="multinomial-and-ordinal-regression.html#cb453-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">brm</span> (C <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>L), <span class="at">family=</span><span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> exp_data)</span></code></pre></div>
<p>Then our data has 2085 rows because we have 139 rows for each of 15 listeners. However, if we create a table like this:</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="multinomial-and-ordinal-regression.html#cb454-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span> (exp_data<span class="sc">$</span>C, exp_data<span class="sc">$</span>L)</span>
<span id="cb454-2"><a href="multinomial-and-ordinal-regression.html#cb454-2" aria-hidden="true" tabindex="-1"></a><span class="do">##    </span></span>
<span id="cb454-3"><a href="multinomial-and-ordinal-regression.html#cb454-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      1  2  3  4  5  6  7  8  9 10 11 12 13 14 15</span></span>
<span id="cb454-4"><a href="multinomial-and-ordinal-regression.html#cb454-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   b 18 29 29 40 27 30 41 26 22  5 33 19 51 42 29</span></span>
<span id="cb454-5"><a href="multinomial-and-ordinal-regression.html#cb454-5" aria-hidden="true" tabindex="-1"></a><span class="do">##   g 38 34 21 39 32 27 23 23 31 45 20 31 12 20 30</span></span>
<span id="cb454-6"><a href="multinomial-and-ordinal-regression.html#cb454-6" aria-hidden="true" tabindex="-1"></a><span class="do">##   m 39 41 45 40 42 44 44 41 43 43 42 45 42 41 43</span></span>
<span id="cb454-7"><a href="multinomial-and-ordinal-regression.html#cb454-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   w 44 35 44 20 38 38 31 49 43 46 44 44 34 36 37</span></span></code></pre></div>
<p>And use this as our data, then we can treat each column above as a single observation from a multinomial distribution with an n equal to 139. When treated this way, our data has only 15 rows and, as a result, fitting the model below will be much (much) faster.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="multinomial-and-ordinal-regression.html#cb455-1" aria-hidden="true" tabindex="-1"></a>new_data <span class="ot">=</span> <span class="fu">data.frame</span> (<span class="at">size =</span> <span class="dv">139</span>, <span class="at">L =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>))</span>
<span id="cb455-2"><a href="multinomial-and-ordinal-regression.html#cb455-2" aria-hidden="true" tabindex="-1"></a>new_data<span class="sc">$</span>y <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">table</span> (exp_data<span class="sc">$</span>L, exp_data<span class="sc">$</span>C))</span>
<span id="cb455-3"><a href="multinomial-and-ordinal-regression.html#cb455-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb455-4"><a href="multinomial-and-ordinal-regression.html#cb455-4" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">brm</span> (y<span class="sc">|</span><span class="fu">trials</span>(size) <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>L), <span class="at">family=</span><span class="st">&quot;multinomial&quot;</span>, <span class="at">data =</span> new_data)</span></code></pre></div>
<p>This is not an option for us since we want to predict categorizations based on the specific combination of listener and speaker, and we only have one observation for each unique combination in our data. However, it is worth considering if a categorical or multinomial analysis might best suit your specific situation.</p>
</div>
<div id="interpreting-the-model-2" class="section level3 hasAnchor" number="12.2.6">
<h3><span class="header-section-number">12.2.6</span> Interpreting the model<a href="multinomial-and-ordinal-regression.html#interpreting-the-model-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below we see the model fixed effects, the intercept, VTL slope, and the f0 slope for each category with estimated parameters (i.e. no reference category <code>b</code>). The set of parameters for each category defines a category-specific plane whose value along the <span class="math inline">\(z\)</span> dimension can be predicted based on the values of f0 and VTL. We have four planes and four values of <span class="math inline">\(z_j\)</span> for any given location in the two dimensional space defined by f0 and VTL. For each point, we can select as the most probable category the one whose value of <span class="math inline">\(z_j\)</span> is highest at the point in the space. Another way to look at this is that we select as the most probable outcome the category whose plane is highest at any given location in the space.</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="multinomial-and-ordinal-regression.html#cb456-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">fixef</span> (model_multinomial)</span>
<span id="cb456-2"><a href="multinomial-and-ordinal-regression.html#cb456-2" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Est.Error    Q2.5   Q97.5</span></span>
<span id="cb456-3"><a href="multinomial-and-ordinal-regression.html#cb456-3" aria-hidden="true" tabindex="-1"></a><span class="do">## mug_Intercept  -2.4516    0.4508 -3.3802 -1.6018</span></span>
<span id="cb456-4"><a href="multinomial-and-ordinal-regression.html#cb456-4" aria-hidden="true" tabindex="-1"></a><span class="do">## mum_Intercept  -2.1677    0.4671 -3.1804 -1.3566</span></span>
<span id="cb456-5"><a href="multinomial-and-ordinal-regression.html#cb456-5" aria-hidden="true" tabindex="-1"></a><span class="do">## muw_Intercept  -0.6317    0.4336 -1.5063  0.2166</span></span>
<span id="cb456-6"><a href="multinomial-and-ordinal-regression.html#cb456-6" aria-hidden="true" tabindex="-1"></a><span class="do">## mug_vtl        -2.0876    0.3746 -2.8648 -1.3766</span></span>
<span id="cb456-7"><a href="multinomial-and-ordinal-regression.html#cb456-7" aria-hidden="true" tabindex="-1"></a><span class="do">## mug_f0          1.4335    0.6987  0.1241  2.8640</span></span>
<span id="cb456-8"><a href="multinomial-and-ordinal-regression.html#cb456-8" aria-hidden="true" tabindex="-1"></a><span class="do">## mum_vtl         3.0820    0.5938  2.0761  4.3871</span></span>
<span id="cb456-9"><a href="multinomial-and-ordinal-regression.html#cb456-9" aria-hidden="true" tabindex="-1"></a><span class="do">## mum_f0         -2.6146    1.0238 -4.7816 -0.7190</span></span>
<span id="cb456-10"><a href="multinomial-and-ordinal-regression.html#cb456-10" aria-hidden="true" tabindex="-1"></a><span class="do">## muw_vtl         0.9464    0.4613  0.0266  1.8643</span></span>
<span id="cb456-11"><a href="multinomial-and-ordinal-regression.html#cb456-11" aria-hidden="true" tabindex="-1"></a><span class="do">## muw_f0          2.2001    1.0943  0.1476  4.4024</span></span></code></pre></div>
<p>Since the predicted value, the <em>linear predictor</em>, is the score for each category, the parameters above are difficult to interpret in isolation. This is because scores need to be interpreted relative to the baseline category value of 0 (for boys), or relative to one another, rather than absolutely. For example, the negative coefficient for VTL for ‘girl’ (<code>mug_vtl</code>) indicates that a longer VTL made a girl response less likely. We can also see that increasing f0 made a ‘woman’ response more likely (<code>muw_f0</code>), and a ‘man’ response less likely (<code>mum_f0</code>). However, it’s important to remember that these effects are all relative to the effects for the boy category, and not ‘absolute’. For example, the ‘boy’ slope for VTL is 0, meaning it is 2.09 higher than the ‘girl’ slope for the same predictor (which is -2.09). We could say this means that longer VTLs suggest a ‘boy’ response is more likely with respect to a ‘girl’ response. However, a slope of 0 is smaller than the VTL slope for ‘man’ of 3.08. So, a longer VTL means that a ‘boy’ response is <em>less</em> likely with respect to a ‘man’ response. As with all of the parameters in our model, these comparisons can be made more formally with the <code>hypothesis</code> (or <code>short_hypothesis</code>) function as seen below.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="multinomial-and-ordinal-regression.html#cb457-1" aria-hidden="true" tabindex="-1"></a><span class="co"># difference in VTL slopes between:</span></span>
<span id="cb457-2"><a href="multinomial-and-ordinal-regression.html#cb457-2" aria-hidden="true" tabindex="-1"></a><span class="fu">short_hypothesis</span> (model_multinomial, </span>
<span id="cb457-3"><a href="multinomial-and-ordinal-regression.html#cb457-3" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">c</span>(<span class="st">&quot;mug_vtl - mum_vtl = 0&quot;</span>,    <span class="co"># girls and men</span></span>
<span id="cb457-4"><a href="multinomial-and-ordinal-regression.html#cb457-4" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;mug_vtl -  0 = 0&quot;</span>,         <span class="co"># girls and boys  </span></span>
<span id="cb457-5"><a href="multinomial-and-ordinal-regression.html#cb457-5" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;mum_vtl - 0 = 0&quot;</span>))         <span class="co"># men and boys</span></span>
<span id="cb457-6"><a href="multinomial-and-ordinal-regression.html#cb457-6" aria-hidden="true" tabindex="-1"></a><span class="do">##    Estimate Est.Error   Q2.5  Q97.5            hypothesis</span></span>
<span id="cb457-7"><a href="multinomial-and-ordinal-regression.html#cb457-7" aria-hidden="true" tabindex="-1"></a><span class="do">## H1   -5.170    0.6927 -6.612 -3.883 (mug_vtl-mum_vtl) = 0</span></span>
<span id="cb457-8"><a href="multinomial-and-ordinal-regression.html#cb457-8" aria-hidden="true" tabindex="-1"></a><span class="do">## H2   -2.088    0.3746 -2.865 -1.377       (mug_vtl-0) = 0</span></span>
<span id="cb457-9"><a href="multinomial-and-ordinal-regression.html#cb457-9" aria-hidden="true" tabindex="-1"></a><span class="do">## H3    3.082    0.5938  2.076  4.387       (mum_vtl-0) = 0</span></span></code></pre></div>
<p>We might wonder how well our model can predict listener judgements. We can do this by finding the predicted probability for each category predicted by our model, as seen below. Note that we use the <code>fitted</code> rather than <code>predict</code> function. This is because we are interested in the expected values predicted by our model. In contrast, <code>predict</code> would give us <em>posterior predictions</em> of the actual dependent variable which would incorporate our modeled noise.</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="multinomial-and-ordinal-regression.html#cb458-1" aria-hidden="true" tabindex="-1"></a>multi_pred_re <span class="ot">=</span> <span class="fu">fitted</span> (model_multinomial)</span></code></pre></div>
<p>The result of this is a three dimensional matrix. The first two dimensions represent the usual summary matrices generated by <code>brms</code>, and the third dimension represents the different response categories. Below we see the two-dimensional summary matrix for the first response category (boy). The first column is the posterior probability of category membership for each observation, the second is the standard error for each prediction, and the third and fourth represent the 2.5% and 97.5% credible intervals.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="multinomial-and-ordinal-regression.html#cb459-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(multi_pred_re[,,<span class="dv">1</span>])</span>
<span id="cb459-2"><a href="multinomial-and-ordinal-regression.html#cb459-2" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error    Q2.5  Q97.5</span></span>
<span id="cb459-3"><a href="multinomial-and-ordinal-regression.html#cb459-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   0.2921    0.1244 0.09423 0.5742</span></span>
<span id="cb459-4"><a href="multinomial-and-ordinal-regression.html#cb459-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   0.2104    0.1036 0.05993 0.4504</span></span>
<span id="cb459-5"><a href="multinomial-and-ordinal-regression.html#cb459-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   0.2804    0.1355 0.07635 0.5892</span></span>
<span id="cb459-6"><a href="multinomial-and-ordinal-regression.html#cb459-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   0.4052    0.1483 0.14678 0.7078</span></span>
<span id="cb459-7"><a href="multinomial-and-ordinal-regression.html#cb459-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]   0.4385    0.1408 0.18733 0.7212</span></span>
<span id="cb459-8"><a href="multinomial-and-ordinal-regression.html#cb459-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   0.2218    0.1229 0.04973 0.5123</span></span></code></pre></div>
<p>Below we see the same information for the second category (girl):</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="multinomial-and-ordinal-regression.html#cb460-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(multi_pred_re[,,<span class="dv">2</span>])</span>
<span id="cb460-2"><a href="multinomial-and-ordinal-regression.html#cb460-2" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error   Q2.5  Q97.5</span></span>
<span id="cb460-3"><a href="multinomial-and-ordinal-regression.html#cb460-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   0.6190    0.1307 0.3396 0.8427</span></span>
<span id="cb460-4"><a href="multinomial-and-ordinal-regression.html#cb460-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   0.6100    0.1347 0.3243 0.8421</span></span>
<span id="cb460-5"><a href="multinomial-and-ordinal-regression.html#cb460-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   0.6624    0.1380 0.3614 0.8848</span></span>
<span id="cb460-6"><a href="multinomial-and-ordinal-regression.html#cb460-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   0.4507    0.1401 0.1879 0.7200</span></span>
<span id="cb460-7"><a href="multinomial-and-ordinal-regression.html#cb460-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]   0.5003    0.1337 0.2454 0.7487</span></span>
<span id="cb460-8"><a href="multinomial-and-ordinal-regression.html#cb460-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   0.6879    0.1367 0.3823 0.9070</span></span></code></pre></div>
<p>We can also just select the second dimension from each matrix, resulting in a two-dimensional matrix containing information regarding the posterior probability of being classified into each category across columns, for each observation across rows.</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="multinomial-and-ordinal-regression.html#cb461-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(multi_pred_re[,<span class="dv">1</span>,])</span>
<span id="cb461-2"><a href="multinomial-and-ordinal-regression.html#cb461-2" aria-hidden="true" tabindex="-1"></a><span class="do">##      P(Y = b) P(Y = g)  P(Y = m) P(Y = w)</span></span>
<span id="cb461-3"><a href="multinomial-and-ordinal-regression.html#cb461-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   0.2921   0.6190 0.0017775  0.08714</span></span>
<span id="cb461-4"><a href="multinomial-and-ordinal-regression.html#cb461-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   0.2104   0.6100 0.0014059  0.17815</span></span>
<span id="cb461-5"><a href="multinomial-and-ordinal-regression.html#cb461-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   0.2804   0.6624 0.0016820  0.05551</span></span>
<span id="cb461-6"><a href="multinomial-and-ordinal-regression.html#cb461-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   0.4052   0.4507 0.0024148  0.14168</span></span>
<span id="cb461-7"><a href="multinomial-and-ordinal-regression.html#cb461-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]   0.4385   0.5003 0.0036467  0.05764</span></span>
<span id="cb461-8"><a href="multinomial-and-ordinal-regression.html#cb461-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   0.2218   0.6879 0.0003084  0.09000</span></span></code></pre></div>
<p>Since these are probabilities, the sum of each row equals one because only these four outcomes exist. For example, the first row above tells us that, according to our model, the first observation has a 0.29 probability of being identified as a boy, 0.62 probability of being identified as a girl, 0.002 probability of being identified as a man, and a 0.09 probability of being identified as a woman. We can use the code below to find the ‘winning’ category for each observation, that is, the category with the highest posterior probability in each row. We use the resulting column numbers to get category labels.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="multinomial-and-ordinal-regression.html#cb462-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find highest posterior probability from each category</span></span>
<span id="cb462-2"><a href="multinomial-and-ordinal-regression.html#cb462-2" aria-hidden="true" tabindex="-1"></a>predicted <span class="ot">=</span> <span class="fu">apply</span> (multi_pred_re[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span>
<span id="cb462-3"><a href="multinomial-and-ordinal-regression.html#cb462-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (predicted)</span>
<span id="cb462-4"><a href="multinomial-and-ordinal-regression.html#cb462-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2 2 2 2 2 2</span></span>
<span id="cb462-5"><a href="multinomial-and-ordinal-regression.html#cb462-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb462-6"><a href="multinomial-and-ordinal-regression.html#cb462-6" aria-hidden="true" tabindex="-1"></a><span class="co"># use modal category to get a category label</span></span>
<span id="cb462-7"><a href="multinomial-and-ordinal-regression.html#cb462-7" aria-hidden="true" tabindex="-1"></a>predicted_category <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;b&quot;</span>,<span class="st">&quot;g&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;w&quot;</span>)[predicted]</span>
<span id="cb462-8"><a href="multinomial-and-ordinal-regression.html#cb462-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (predicted_category)</span>
<span id="cb462-9"><a href="multinomial-and-ordinal-regression.html#cb462-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;g&quot; &quot;g&quot; &quot;g&quot; &quot;g&quot; &quot;g&quot; &quot;g&quot;</span></span></code></pre></div>
<p>Below, we cross-tabulate predicted and observed classifications. Observed speaker classifications vary across rows, and model predictions of these classifications vary across columns. This is a <em>confusion matrix</em>, introduced in chapter 5. Correct classifications fall on the main diagonal and all other values indicate mistakes. For example, we see that there were 269 correct classifications of boys as boys, and 78 incorrect predictions of boys as girls. A majority of observations fall along the main diagonal indicating that the model was relatively good at predicting listener behavior.</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="multinomial-and-ordinal-regression.html#cb463-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span> (<span class="at">observed_category =</span> exp_data<span class="sc">$</span>C, predicted_category)</span>
<span id="cb463-2"><a href="multinomial-and-ordinal-regression.html#cb463-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                  predicted_category</span></span>
<span id="cb463-3"><a href="multinomial-and-ordinal-regression.html#cb463-3" aria-hidden="true" tabindex="-1"></a><span class="do">## observed_category   b   g   m   w</span></span>
<span id="cb463-4"><a href="multinomial-and-ordinal-regression.html#cb463-4" aria-hidden="true" tabindex="-1"></a><span class="do">##                 b 269  78  27  67</span></span>
<span id="cb463-5"><a href="multinomial-and-ordinal-regression.html#cb463-5" aria-hidden="true" tabindex="-1"></a><span class="do">##                 g  74 290   0  62</span></span>
<span id="cb463-6"><a href="multinomial-and-ordinal-regression.html#cb463-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                 m   5   1 624   5</span></span>
<span id="cb463-7"><a href="multinomial-and-ordinal-regression.html#cb463-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                 w  30  19   9 525</span></span></code></pre></div>
<p>Below we find the probability of observing a correct classification overall, and individually for each category. We see that the model was able to predict listener judgments with a high degree of accuracy overall, but that some categories (man) were easier to predict than others (boy).</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="multinomial-and-ordinal-regression.html#cb464-1" aria-hidden="true" tabindex="-1"></a><span class="co"># overall correct</span></span>
<span id="cb464-2"><a href="multinomial-and-ordinal-regression.html#cb464-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (predicted_category <span class="sc">==</span> exp_data<span class="sc">$</span>C)</span>
<span id="cb464-3"><a href="multinomial-and-ordinal-regression.html#cb464-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8192</span></span>
<span id="cb464-4"><a href="multinomial-and-ordinal-regression.html#cb464-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb464-5"><a href="multinomial-and-ordinal-regression.html#cb464-5" aria-hidden="true" tabindex="-1"></a><span class="co"># correct predictions by category</span></span>
<span id="cb464-6"><a href="multinomial-and-ordinal-regression.html#cb464-6" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">xtabs</span> (<span class="sc">~</span> exp_data<span class="sc">$</span>C <span class="sc">+</span> predicted_category)</span>
<span id="cb464-7"><a href="multinomial-and-ordinal-regression.html#cb464-7" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(tab) <span class="sc">/</span> <span class="fu">rowSums</span>(tab)</span>
<span id="cb464-8"><a href="multinomial-and-ordinal-regression.html#cb464-8" aria-hidden="true" tabindex="-1"></a><span class="do">##      b      g      m      w </span></span>
<span id="cb464-9"><a href="multinomial-and-ordinal-regression.html#cb464-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.6100 0.6808 0.9827 0.9005</span></span></code></pre></div>
</div>
<div id="c12-multinomial-territorial-maps" class="section level3 hasAnchor" number="12.2.7">
<h3><span class="header-section-number">12.2.7</span> Multinomial models and territorial maps<a href="multinomial-and-ordinal-regression.html#c12-multinomial-territorial-maps" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We discussed territorial maps for two categories along two dimensions in chapter 11 where we predicted the perception of femaleness based on speaker f0 and VTL. As we discussed then, when categories are represented by planes the boundary between categories is defined by the line formed by the intersection of the planes. When we did (dichotomous) logistic regression in chapter 11, we found the intersection of one plane with a horizontal plane at <span class="math inline">\(z=0\)</span>. In this chapter we still have a plane such that <span class="math inline">\(z=0\)</span> for all x and y (the reference category plane), but we also have three other planes with possible non-zero slopes along each dimension. The intersection of the planes representing pairs of categories forms the boundary between those two categories. Since we have 6 unique pairings of our four response categories, we will have six boundary lines.</p>
<p>To find the line formed by the intersection of planes, first we define the values of two planes, <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> based on their respective coefficients as in <a href="multinomial-and-ordinal-regression.html#eq:12-8">(12.13)</a>. We use <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> for the intercept, x axis, and y axis slope for the first plane and <span class="math inline">\(d\)</span>, <span class="math inline">\(e\)</span>, and <span class="math inline">\(f\)</span> for the corresponding coefficients for the second plane.</p>
<p><span class="math display" id="eq:12-8">\[
\begin{equation}
\begin{split}
z_1 = \mathrm{a} + \mathrm{b} \cdot x +  \mathrm{c} \cdot y \\
z_2 = \mathrm{d} + \mathrm{e} \cdot x +  \mathrm{f} \cdot y
\end{split}
\tag{12.13}
\end{equation}
\]</span></p>
<p>We set <span class="math inline">\(z_1\)</span> equal to <span class="math inline">\(z_2\)</span> to find the place where the planes equal (i.e. their intersection) as in <a href="multinomial-and-ordinal-regression.html#eq:12-9">(12.14)</a>.</p>
<p><span class="math display" id="eq:12-9">\[
\begin{equation}
\begin{split}
z_1 = z_2 \\
\mathrm{a} + \mathrm{b} \cdot x +  \mathrm{c} \cdot y = \mathrm{d} + \mathrm{e} \cdot x +  \mathrm{f} \cdot y
\end{split}
\tag{12.14}
\end{equation}
\]</span></p>
<p>In order to turn <a href="multinomial-and-ordinal-regression.html#eq:12-9">(12.14)</a> into an equation resembling that of a line (<span class="math inline">\(y = a + b \cdot x\)</span>), we need to isolate y on the left hand side as seen in <a href="multinomial-and-ordinal-regression.html#eq:12-10">(12.15)</a>.</p>
<p><span class="math display" id="eq:12-10">\[
\begin{equation}
y = \frac {-\mathrm{a} + \mathrm{d}}{\mathrm{c} - \mathrm{f}} + \frac{-\mathrm{b} + \mathrm{e}}{\mathrm{c} - \mathrm{f}} \cdot x
\tag{12.15}
\end{equation}
\]</span></p>
<p>The equation above is actually much simpler than it looks. Since the parameters (<span class="math inline">\(a,b,c,d,e,f\)</span>) are just constants, the intercept (<span class="math inline">\(-a+d/c-f\)</span>) and slope (<span class="math inline">\(-b+e/c-f\)</span>) simplify to scalars. As mentioned above, we get 6 such line equations, one for the boundary between all possible pairs of each of the four planes. Figure <a href="multinomial-and-ordinal-regression.html#fig:F12-2">12.2</a> presents each of these six boundaries compared to the modal classification for each speaker.</p>
<div class="figure"><span style="display:block;" id="fig:F12-2"></span>
<img src="_main_files/figure-html/F12-2-1.jpeg" alt="Each plot compares a single estimated category boundary between two groups at a time for boys (b), girls (g), men (m), and women (w)." width="4800" />
<p class="caption">
Figure 12.2: Each plot compares a single estimated category boundary between two groups at a time for boys (b), girls (g), men (m), and women (w).
</p>
</div>
<p>All six boundary lines are presented in the left plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-3">12.3</a>. One problem with considering the figure in this way is that many of the boundaries are not very relevant. For example, the boundary between girl and man will hardly matter for classification because the ‘woman’ and ‘boy’ categories fall almost entirely between the ‘girl’ and ‘man’ categories. As a result, listeners will rarely be deciding between a ‘man’ or ‘girl’ response when ‘boy’ and ‘woman’ are available.</p>
<p>The right plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-3">12.3</a> presents only those boundaries that are <em>relevant</em>, resulting in a <em>territorial map</em> of our categories along f0 and VTL (see section <a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification">10.5.5</a>). This can be stated more formally as: The figure contains only those boundaries that represents scores equal to or greater than the maximal score for that location, from among the response categories. For example, in the left plot we see that the boy-woman boundary bisects the area where ‘man’ is a modal response (bottom-right in the left plot). This line segment does not feature in the right plot because it has a lower value than the ‘man’ plane in this area. One way to imagine how territorial maps relate to our planes is to imagine that our planes were opaque but were immaterial such that they can intersect and continue past each other. If we were to look ‘down’ the z axis onto these planes, we would see something very much like the right plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-3">12.3</a>; we would see only those plane sections, and only those intersections, that were ‘above’ any other plane.</p>
<div class="figure"><span style="display:block;" id="fig:F12-3"></span>
<img src="_main_files/figure-html/F12-3-1.jpeg" alt="(left) Points represent modal classifications for individual speakers. Lines represent the six category boundaries presented in figure \@ref(fig:F12-2). (right) Territorial map comparing locations associated with the clasification of speakers into boys (b), girls (g), men (m), and women (w)." width="4800" />
<p class="caption">
Figure 12.3: (left) Points represent modal classifications for individual speakers. Lines represent the six category boundaries presented in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-2">12.2</a>. (right) Territorial map comparing locations associated with the clasification of speakers into boys (b), girls (g), men (m), and women (w).
</p>
</div>
<p>We can find the boundaries of each of the ‘territories’ in the territorial map <em>analytically</em> by finding each boundary, the intersections of all boundaries, and record only the line segments whose scores where greater than or equal to all of the category planes. Each of these steps is generally straightforward but we know of no algorithm to implement this process easily for any number of response categories. The <code>make_map</code> function in the <code>bmmb</code> package can be used to generate territorial maps for any number of response categories in a two-dimensional stimulus space. However, rather than find the territorial map analytically, <code>make_map</code> estimates its characteristics <em>numerically</em>, that is, based on a bunch of guesses and not exact solutions.</p>
<p>The <code>make_map</code> function takes in a matrix representing the coefficients for each of the response categories. In the code below we get estimates for our fixed effects and combine these into a matrix, including a column of zeros to represent the ‘boy’ coefficients. Each column represents the coefficients for a single response category, and intercepts, x axis coefficients and y axis coefficients vary across rows. The rows and columns of the matrix below have been named to make this organization clearer, and you should compare the values below to our model fixed effects shown above (e.g., <code>fixef (model_multinomial)</code>).</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="multinomial-and-ordinal-regression.html#cb465-1" aria-hidden="true" tabindex="-1"></a>parameters <span class="ot">=</span> <span class="fu">unname</span> (<span class="fu">fixef</span> (model_multinomial)[,<span class="dv">1</span>])</span>
<span id="cb465-2"><a href="multinomial-and-ordinal-regression.html#cb465-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-3"><a href="multinomial-and-ordinal-regression.html#cb465-3" aria-hidden="true" tabindex="-1"></a>parameters <span class="ot">=</span> <span class="fu">cbind</span>(<span class="at">b =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb465-4"><a href="multinomial-and-ordinal-regression.html#cb465-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">g =</span> parameters[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">5</span>)], </span>
<span id="cb465-5"><a href="multinomial-and-ordinal-regression.html#cb465-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">m =</span> parameters[<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">7</span>)],</span>
<span id="cb465-6"><a href="multinomial-and-ordinal-regression.html#cb465-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">w =</span> parameters[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">8</span>,<span class="dv">9</span>)])</span>
<span id="cb465-7"><a href="multinomial-and-ordinal-regression.html#cb465-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(parameters)<span class="ot">=</span><span class="fu">c</span>(<span class="st">&quot;Intercept&quot;</span>,<span class="st">&quot;vtl_slope&quot;</span>,<span class="st">&quot;f0_slope&quot;</span>)</span>
<span id="cb465-8"><a href="multinomial-and-ordinal-regression.html#cb465-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-9"><a href="multinomial-and-ordinal-regression.html#cb465-9" aria-hidden="true" tabindex="-1"></a>parameters</span>
<span id="cb465-10"><a href="multinomial-and-ordinal-regression.html#cb465-10" aria-hidden="true" tabindex="-1"></a><span class="do">##           b      g      m       w</span></span>
<span id="cb465-11"><a href="multinomial-and-ordinal-regression.html#cb465-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept 0 -2.452 -2.168 -0.6317</span></span>
<span id="cb465-12"><a href="multinomial-and-ordinal-regression.html#cb465-12" aria-hidden="true" tabindex="-1"></a><span class="do">## vtl_slope 0 -2.088  3.082  0.9464</span></span>
<span id="cb465-13"><a href="multinomial-and-ordinal-regression.html#cb465-13" aria-hidden="true" tabindex="-1"></a><span class="do">## f0_slope  0  1.434 -2.615  2.2001</span></span></code></pre></div>
<p>This matrix can be passed to the <code>make_map</code> function, which uses it to calculate the territorial map. It operates in the following manner. First, it generates a 1000 by 1000 point grid spanning the desired x and y axis range. The density of points can be set with the <code>density</code> parameter and the x and y axis range can be set manually or by passing points with the same ranges to the function with the <code>points</code> parameter. After this, the function uses the parameters given to calculate the score for each category, for every point in the grid, and collects the winning category for each location. After this, the convex hull encompassing the points for each category is found, and the polygon representing the convex hull for each category is returned in a list by the function. This approach is not <em>exact</em>, and so is not appropriate for making big claims about your results. However, it can be made arbitrarily accurate by increasing the density of the points, and this approach is sufficient for creating plots to convey the results of your analyses.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="multinomial-and-ordinal-regression.html#cb466-1" aria-hidden="true" tabindex="-1"></a>territorial_map <span class="ot">=</span> </span>
<span id="cb466-2"><a href="multinomial-and-ordinal-regression.html#cb466-2" aria-hidden="true" tabindex="-1"></a>  bmmb<span class="sc">::</span><span class="fu">make_map</span> (parameters, <span class="at">points =</span> <span class="fu">cbind</span>(exp_data<span class="sc">$</span>vtl,exp_data<span class="sc">$</span>f0))</span></code></pre></div>
<p>The output of <code>make_map</code> can be passed to the <code>plot_map</code> function to plot the resulting territorial maps easily. We used this process to create the territorial maps of the figures in this chapter.</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="multinomial-and-ordinal-regression.html#cb467-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">plot_map</span> (territorial_map, <span class="at">colors =</span> bmmb<span class="sc">::</span>cols[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>],<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p>We noticed something very strange when we made the territorial map in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-3">12.3</a>: The ‘boy’ territory contains a large number of woman responses. In addition, the boy-woman boundary seems strangely high along f0. If that is really the boundary between boys and women in the VTL by f0 space, then a large number of adult female speakers fall into territory where ‘boy’ responses are expected. If this is the case then how/why is the model classifying all those women correctly? The answer to this question may lie in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-4">12.4</a>.</p>
<p>In the left plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-4">12.4</a>, point colors represent modal listener judgments for each speaker. In the middle plot we see model predictions including speaker and listener random effects. When these are included the model predictions look very much like the listener judgments and the classifications don’t make much sense relative to the territorial map. In the right plot of the same figure we see model predictions when random effects are <em>not</em> included. In this case we see that model predictions do <em>not</em> look like the listener judgments, but the classifications <em>do</em> make sense relative to the territorial map. So, it seems that the random effects are causing this strange behavior in our model.</p>
<div class="figure"><span style="display:block;" id="fig:F12-4"></span>
<img src="_main_files/figure-html/F12-4-1.jpeg" alt="Territorial maps compared to modal classifications of speakers as boys (b), girls (g), men (m), and women (w). Point colors reflect classifications made by (left) listeners, (middle) predictions including random effects (RE), and (right) predictions excluding random effects." width="4800" />
<p class="caption">
Figure 12.4: Territorial maps compared to modal classifications of speakers as boys (b), girls (g), men (m), and women (w). Point colors reflect classifications made by (left) listeners, (middle) predictions including random effects (RE), and (right) predictions excluding random effects.
</p>
</div>
<p>Figure <a href="multinomial-and-ordinal-regression.html#fig:F12-5">12.5</a> presents the speaker random intercepts. Since there are three modeled categories, there are three random intercepts for each speaker. We can see that, in general, the speaker intercepts have small values that vary around zero, and have credible intervals that mostly include zero. However, many female speakers have intercepts that have large non-zero values for the female category, and whose credible intervals omit zero and very small values.</p>
<div class="figure"><span style="display:block;" id="fig:F12-5"></span>
<img src="_main_files/figure-html/F12-5-1.jpeg" alt="Each row presents posterior means and 95% credible intervals for speaker-dependent intercept terms for each category. Colors represent veridical boys (b), girls (g), men (m), and women (w)." width="4800" />
<p class="caption">
Figure 12.5: Each row presents posterior means and 95% credible intervals for speaker-dependent intercept terms for each category. Colors represent veridical boys (b), girls (g), men (m), and women (w).
</p>
</div>
<p>Here’s what we believe is happening. Think of a female response plane with a specific intercept, VTL slope and f0 slope. If female speaker categorizations were very predictable based on the value of this plane at a given location, all the female speaker effects would be zero or near zero. This is because if the score based on the plane explains classification, there is nothing left for the speaker intercept to explain. We can see this sort of behavior for adult male speakers: Adult male speakers are mostly easy to identify because of their unusually low f0s.</p>
<p>In contrast, consider a situation where individual female speakers fall way off the plane, in a seemingly random but consistent manner. For example, imagine a female speaker whose score for the female category is way higher than the plane says it should be, but that every listener seems to behave this way for this speaker. This would mean that this is a consistent classification for this voice that cannot be explained by the score according to the plane. In other words, the classification cannot be predicted based on the linear combination of voice VTL and f0. In such a situation, this consistent classification would be ‘explained’ by the speaker intercept. Effectively, this tells your model “use the general female plane, but move it up/down this much specifically for this speaker”. Why are we moving it up/down for this speaker? Our model can’t tell us, since we’re just modeling this with an intercept (and not an additional predictor). However, it <em>can</em> tell us that the up/down adjustment is consistent for the speaker across listeners and improves model performance.</p>
<p>We can think about what these speaker effects might reflect. The information regarding gender and age in voices is complicated, and likely reflects subtle stylistic and rhythmic cues. Basically, sounding ‘feminine’ and ‘masculine’ is much more complicated than just f0 or VTL (see section <a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-perception-of-chars">13.1.6</a>). This is analogous to the fact that women tend to be shorter than men, but it would be ridiculous to suggest that masculinity/femininity are entirely predictable based on the height of a person. Size is perhaps an aspect of perceived femininity/masculinity but the whole of it is substantially more complicated.</p>
<p>We again consider our model prediction accuracy, but this time make predictions without any random effects.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="multinomial-and-ordinal-regression.html#cb468-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions with no random effects</span></span>
<span id="cb468-2"><a href="multinomial-and-ordinal-regression.html#cb468-2" aria-hidden="true" tabindex="-1"></a>multi_pred <span class="ot">=</span> <span class="fu">fitted</span> (model_multinomial, <span class="at">re_formula =</span> <span class="cn">NA</span>)</span></code></pre></div>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="multinomial-and-ordinal-regression.html#cb469-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find winners</span></span>
<span id="cb469-2"><a href="multinomial-and-ordinal-regression.html#cb469-2" aria-hidden="true" tabindex="-1"></a>predicted_no_re <span class="ot">=</span> <span class="fu">apply</span> (multi_pred[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span>
<span id="cb469-3"><a href="multinomial-and-ordinal-regression.html#cb469-3" aria-hidden="true" tabindex="-1"></a><span class="co"># get winner labels</span></span>
<span id="cb469-4"><a href="multinomial-and-ordinal-regression.html#cb469-4" aria-hidden="true" tabindex="-1"></a>predicted_category_no_re <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;b&quot;</span>,<span class="st">&quot;g&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;w&quot;</span>)[predicted_no_re]</span>
<span id="cb469-5"><a href="multinomial-and-ordinal-regression.html#cb469-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb469-6"><a href="multinomial-and-ordinal-regression.html#cb469-6" aria-hidden="true" tabindex="-1"></a><span class="co"># overall correct</span></span>
<span id="cb469-7"><a href="multinomial-and-ordinal-regression.html#cb469-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (predicted_category_no_re <span class="sc">==</span> exp_data<span class="sc">$</span>C)</span>
<span id="cb469-8"><a href="multinomial-and-ordinal-regression.html#cb469-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.5871</span></span>
<span id="cb469-9"><a href="multinomial-and-ordinal-regression.html#cb469-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb469-10"><a href="multinomial-and-ordinal-regression.html#cb469-10" aria-hidden="true" tabindex="-1"></a><span class="co"># correct predictions by category</span></span>
<span id="cb469-11"><a href="multinomial-and-ordinal-regression.html#cb469-11" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">xtabs</span> (<span class="sc">~</span> exp_data<span class="sc">$</span>C <span class="sc">+</span> predicted_category_no_re)</span>
<span id="cb469-12"><a href="multinomial-and-ordinal-regression.html#cb469-12" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(tab) <span class="sc">/</span> <span class="fu">rowSums</span>(tab)</span>
<span id="cb469-13"><a href="multinomial-and-ordinal-regression.html#cb469-13" aria-hidden="true" tabindex="-1"></a><span class="do">##      b      g      m      w </span></span>
<span id="cb469-14"><a href="multinomial-and-ordinal-regression.html#cb469-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.2993 0.7559 0.9827 0.2504</span></span></code></pre></div>
<p>The picture without random effects is a bit grim. Overall correct prediction is not too bad (relative to chance at 25%), however, some of the individual categories are predicted very poorly. As expected based on the territorial maps in <a href="multinomial-and-ordinal-regression.html#fig:F12-4">12.4</a>, the accurate prediction of ‘woman’ classifications is particularly bad with only 21% of ‘woman’ responses being accurately predicted as such.</p>
</div>
<div id="refitting-the-model-without-speaker-random-effects" class="section level3 hasAnchor" number="12.2.8">
<h3><span class="header-section-number">12.2.8</span> Refitting the model without speaker random effects<a href="multinomial-and-ordinal-regression.html#refitting-the-model-without-speaker-random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The results of our previous model suggest that the inclusion of speaker random effects may lead to some unintended, and undesirable, consequences. The predictions made by our initial model are only good as long as we include the speaker random effects. However, these do not let us understand the prediction of new speakers based solely on speaker f0 and VTL. Further, the territorial map we generate using the model is a bit unreliable because of the influence of the speaker random effects included in the model. We will try fitting the model without speaker effects to see if we get a territorial map that is a better match for our listener judgments. We use the same priors we used before, and fit the same model save for the absence of speaker effects in our formula:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="multinomial-and-ordinal-regression.html#cb470-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb470-2"><a href="multinomial-and-ordinal-regression.html#cb470-2" aria-hidden="true" tabindex="-1"></a>model_multinomial_noS <span class="ot">=</span> </span>
<span id="cb470-3"><a href="multinomial-and-ordinal-regression.html#cb470-3" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (y<span class="sc">|</span><span class="fu">trials</span>(size) <span class="sc">~</span> vtl<span class="sc">+</span>f0 <span class="sc">+</span> (vtl<span class="sc">+</span>f0<span class="sc">|</span>x<span class="sc">|</span>L), <span class="at">data=</span>exp_data, </span>
<span id="cb470-4"><a href="multinomial-and-ordinal-regression.html#cb470-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">family=</span><span class="fu">multinomial</span>(), <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, <span class="at">warmup=</span><span class="dv">1000</span>, </span>
<span id="cb470-5"><a href="multinomial-and-ordinal-regression.html#cb470-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>, <span class="at">prior =</span> multinomial_prior)</span></code></pre></div>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="multinomial-and-ordinal-regression.html#cb471-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb471-2"><a href="multinomial-and-ordinal-regression.html#cb471-2" aria-hidden="true" tabindex="-1"></a>model_multinomial_noS <span class="ot">=</span> </span>
<span id="cb471-3"><a href="multinomial-and-ordinal-regression.html#cb471-3" aria-hidden="true" tabindex="-1"></a>  bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&quot;../models/12_model_multinomial_noS.RDS&quot;</span>)</span></code></pre></div>
<p>We jump straight to plotting the territorial map represented by this model, again comparing listener judgments to model predictions with and without random effects (<a href="multinomial-and-ordinal-regression.html#fig:F12-6">12.6</a>). We can see that the territorial maps in the figure seem to be a better match to the data.</p>
<div class="figure"><span style="display:block;" id="fig:F12-6"></span>
<img src="_main_files/figure-html/F12-6-1.jpeg" alt="Territorial maps compared to modal classifications of speakers as boys (b), girls (g), men (m), and women (w). Point colors reflect classifications made by (left) listeners, (middle) predictions including random effects (RE), and (right) predictions excluding random effects." width="4800" />
<p class="caption">
Figure 12.6: Territorial maps compared to modal classifications of speakers as boys (b), girls (g), men (m), and women (w). Point colors reflect classifications made by (left) listeners, (middle) predictions including random effects (RE), and (right) predictions excluding random effects.
</p>
</div>
<p>Figure <a href="multinomial-and-ordinal-regression.html#fig:F12-7">12.7</a> compares the territorial maps represented by our models with (left) and without (right) speaker information. In addition to being a better fit to the data, we think the new territorial maps make more ‘sense’. This is because the map with speaker effects suggests that the difference between women and boys was almost entirely due to f0, with women being associated with the higher f0. This is despite the fact that veridical boys, on average, have shorter VTLs <em>and</em> higher f0s than adult females. The new map has basically no effect for f0 between these categories and makes it primarily a VTL difference, which seems to be a better fit to the data.</p>
<div class="figure"><span style="display:block;" id="fig:F12-7"></span>
<img src="_main_files/figure-html/F12-7-1.jpeg" alt="Territorial maps compared to modal classifications of speakers as boys (b), girls (g), men (m), and women (w). Point colors reflect classifications made by (left) the model with speaker effects, and (right) the model without speaker effects." width="4800" />
<p class="caption">
Figure 12.7: Territorial maps compared to modal classifications of speakers as boys (b), girls (g), men (m), and women (w). Point colors reflect classifications made by (left) the model with speaker effects, and (right) the model without speaker effects.
</p>
</div>
<p>One concern with omitting the speaker effects is that our model doesn’t ‘know’ that we only had 139 speakers in our experiment. This can have the effect of artificially decreasing our credible intervals, as we saw in figure <a href="c6.html#fig:F6-5">6.6</a>. Figure <a href="multinomial-and-ordinal-regression.html#fig:F12-8">12.8</a> compares the fixed effects between our two models, suggesting that the omission of the speaker effects has not had a large impact on most parameters or their credible intervals. The two largest differences are for the woman category: The intercept went from slightly negative to slightly positive, and the effect for f0 went from zero to positive.</p>
<div class="figure"><span style="display:block;" id="fig:F12-8"></span>
<img src="_main_files/figure-html/F12-8-1.jpeg" alt="A comparison of the fixed effects means and 95% credible intervals provided by the two multinomial models we are considering." width="4800" />
<p class="caption">
Figure 12.8: A comparison of the fixed effects means and 95% credible intervals provided by the two multinomial models we are considering.
</p>
</div>
<p>We will make new predictions for this model, with and without random effects:</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="multinomial-and-ordinal-regression.html#cb472-1" aria-hidden="true" tabindex="-1"></a>multi_pred_re_noS <span class="ot">=</span> <span class="fu">fitted</span> (model_multinomial_noS)</span>
<span id="cb472-2"><a href="multinomial-and-ordinal-regression.html#cb472-2" aria-hidden="true" tabindex="-1"></a>multi_pred_noS <span class="ot">=</span> <span class="fu">fitted</span> (model_multinomial_noS, <span class="at">re_formula =</span> <span class="cn">NA</span>)</span></code></pre></div>
<p>We find the maximum a posteriori speaker classification and get a label for each trial for our predictions with random effects.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="multinomial-and-ordinal-regression.html#cb473-1" aria-hidden="true" tabindex="-1"></a>predicted_noS <span class="ot">=</span> <span class="fu">apply</span> (multi_pred_re_noS[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span>
<span id="cb473-2"><a href="multinomial-and-ordinal-regression.html#cb473-2" aria-hidden="true" tabindex="-1"></a>predicted_category_noS <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;b&quot;</span>,<span class="st">&quot;g&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;w&quot;</span>)[predicted_noS]</span></code></pre></div>
<p>We calculate correct classifications overall and by category. Performance is not as good as the model with speaker effects when random effects are included (82%) but not as bad as for the same model when random effects are not included (59%). In addition, we see very good classification of all categories except for ‘boy’.</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="multinomial-and-ordinal-regression.html#cb474-1" aria-hidden="true" tabindex="-1"></a><span class="co"># overall correct</span></span>
<span id="cb474-2"><a href="multinomial-and-ordinal-regression.html#cb474-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (predicted_category_noS <span class="sc">==</span> exp_data<span class="sc">$</span>C)</span>
<span id="cb474-3"><a href="multinomial-and-ordinal-regression.html#cb474-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.742</span></span>
<span id="cb474-4"><a href="multinomial-and-ordinal-regression.html#cb474-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb474-5"><a href="multinomial-and-ordinal-regression.html#cb474-5" aria-hidden="true" tabindex="-1"></a><span class="co"># correct predictions by category</span></span>
<span id="cb474-6"><a href="multinomial-and-ordinal-regression.html#cb474-6" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">xtabs</span> (<span class="sc">~</span> exp_data<span class="sc">$</span>C <span class="sc">+</span> predicted_category_noS)</span>
<span id="cb474-7"><a href="multinomial-and-ordinal-regression.html#cb474-7" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(tab) <span class="sc">/</span> <span class="fu">rowSums</span>(tab)</span>
<span id="cb474-8"><a href="multinomial-and-ordinal-regression.html#cb474-8" aria-hidden="true" tabindex="-1"></a><span class="do">##      b      g      m      w </span></span>
<span id="cb474-9"><a href="multinomial-and-ordinal-regression.html#cb474-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.3673 0.6737 0.9827 0.8130</span></span></code></pre></div>
<p>Importantly, we see that classification barely changes when random effects are not included, except for boy classifications which drop even below chance.</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="multinomial-and-ordinal-regression.html#cb475-1" aria-hidden="true" tabindex="-1"></a>predicted_noS_no_re <span class="ot">=</span> <span class="fu">apply</span> (multi_pred_noS[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span>
<span id="cb475-2"><a href="multinomial-and-ordinal-regression.html#cb475-2" aria-hidden="true" tabindex="-1"></a>predicted_category_noS_no_re <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;b&quot;</span>,<span class="st">&quot;g&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;w&quot;</span>)[predicted_noS_no_re]</span>
<span id="cb475-3"><a href="multinomial-and-ordinal-regression.html#cb475-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb475-4"><a href="multinomial-and-ordinal-regression.html#cb475-4" aria-hidden="true" tabindex="-1"></a><span class="co"># overall correct</span></span>
<span id="cb475-5"><a href="multinomial-and-ordinal-regression.html#cb475-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (predicted_category_noS_no_re <span class="sc">==</span> exp_data<span class="sc">$</span>C)</span>
<span id="cb475-6"><a href="multinomial-and-ordinal-regression.html#cb475-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.7223</span></span>
<span id="cb475-7"><a href="multinomial-and-ordinal-regression.html#cb475-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb475-8"><a href="multinomial-and-ordinal-regression.html#cb475-8" aria-hidden="true" tabindex="-1"></a><span class="co"># correct predictions by category</span></span>
<span id="cb475-9"><a href="multinomial-and-ordinal-regression.html#cb475-9" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">xtabs</span> (<span class="sc">~</span> exp_data<span class="sc">$</span>C <span class="sc">+</span> predicted_category_noS_no_re)</span>
<span id="cb475-10"><a href="multinomial-and-ordinal-regression.html#cb475-10" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(tab) <span class="sc">/</span> <span class="fu">rowSums</span>(tab)</span>
<span id="cb475-11"><a href="multinomial-and-ordinal-regression.html#cb475-11" aria-hidden="true" tabindex="-1"></a><span class="do">##       b       g       m       w </span></span>
<span id="cb475-12"><a href="multinomial-and-ordinal-regression.html#cb475-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.09977 0.72770 0.98268 0.90566</span></span></code></pre></div>
</div>
<div id="answering-our-research-questions-2" class="section level3 hasAnchor" number="12.2.9">
<h3><span class="header-section-number">12.2.9</span> Answering our research questions<a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use our models to answer the question we posed above:</p>
<p>(Q1) Can we use speaker f0 and VTL to predict their apparent speaker category?</p>
<p>Yes, we can use speaker f0 and VTL to predict apparent speaker with relatively high accuracy. If we consider a baseline correctness by chance of 25%, then our classification of 72% from two predictors (with no random effects) isn’t bad at all. However, the importance of the speaker random effects for female speakers strongly suggests that there is ‘something else’ being used to identify women as women and boys as boys, apart from VTL and f0. So, the results of our first model tells us that we should probably try to understand what that ‘something else’ is.</p>
<p>The speaker random effects are not particularly useful for understanding the categorization of new speakers. However, understanding what causes the variation in these random effects is. Our model tells us that even though we know there is something else to it, we can still predict categorization fairly well using just speaker VTL and f0. So, the first model may be better to really try to understand what listeners are doing when they classify individual voices, while the second model may be better to understand classification from just f0 and VTL.</p>
</div>
</div>
<div id="ordinal-logistic-regression" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Ordinal (logistic) regression<a href="multinomial-and-ordinal-regression.html#ordinal-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ordinal logistic regression involves the prediction of ordered categories. Although in some senses ordinal regression is ‘simpler’ than multinomial regression, we decided to present it after multinomial regression. This is because, as discussed above, multinomial regression is mostly just a multivariate generalization of logistic regression (discussed in detail in chapter 10). In contrast, ordinal regression, although related to logistic regression, is in many ways conceptually its own thing. We will present one way of thinking about ordinal logistic regression however, as usual, there are other ways to think of it. For more information about ordinal regression, please see Agresti (2003), Agresti (2018), and Kruschke (2014) (the same citations we provided for multinomial models, actually).</p>
<p>A simple example of an ordinal variable is first, second, and third place in a race. These values are categorical and not on an interval or ratio scale. For example fourth place is not ‘double’ anything with respect to second place, and the distance between first and second doesn’t necessarily represent the same difference as the difference between second and third. However, there is clearly an inherent ordering in the categories such that first &lt; second &lt; third &lt; fourth &lt; and so on. A common experimental example of ordered categorical data arises from survey data that asks listeners to respond to questions using a small number of discrete, categorical choices. For example, respondents might be asked to evaluate whether they 1) strongly agree, 2) somewhat agree, 3) are neutral, 4) somewhat disagree, or 5) strongly disagree with some proposition. This is often referred to as a <strong>Likert scale</strong> and the data resulting from this is often referred to as <strong>Likert scale data</strong>.</p>
<p>Likert scale data was originally meant to have response scales ranging from agreement to disagreement (as above). However, the concept of Likert scale data can be generalized to a very broad range of possible questions. For example, rather than height in centimeters, we might have asked listeners “use this 10 point scale to indicate how tall this person sounds”. This could be made into a more ‘traditional’ Likert scale by asking it in the form “this person sounds very tall” and asking people to agree or disagree on a 10 point scale. Rather than ask for binary gender judgments, we might have asked “Use this 7 point scale to judge the perceived femininity/masculinity of the speaker” (or the more traditional “this person sounds very feminine” and asking people to agree or disagree on a 7 point scale).</p>
<p>Although Likert scale data is sometimes treated as quantitative, there are some characteristics of this data that make an ordinal analysis a better fit. First, it is not necessarily the case that the ‘distance’ between adjacent responses is equal across all categories. We can say with absolute certainty that the difference between 150 and 151 cm is equal to the difference between 151 and 152 cm. However, is the difference between strongly agree and somewhat agree the same as the difference between somewhat agree and neutral? This is difficult if not impossible to determine. Further, the small number of outcome categories may result in subjects reserving extreme categories for extraordinary cases. What if a subject somewhat strongly agrees? There is no such option, and this subject may instead indicate they somewhat agree lest something they <em>really</em> strongly agree with comes along later.</p>
<p>Here’s what we said in chapter 1 about when to treat a variable as quantitative:</p>
<ul>
<li><p>Is the variable on a ratio or interval scale? This is a prerequisite for a quantitative value to be used as a dependent variable. An interval scale means that distances are meaningful, and a ratio scale means that 0 is meaningful.</p></li>
<li><p>Is the underlying value continuous? Many variables are discrete in practice due to limitations in measurement. However, if the underlying value is continuous (e.g., height, time) then this can motivate treating the measurement as a quantitative dependent variable since fractional values ‘make sense’. For example, even if you measure time only down to the nearest millisecond, a value of 0.5 milliseconds is possible and interpretable. In contrast, a value of 0.5 people is not.</p></li>
<li><p>Are there a large number (&gt;50) of possible values the measured variable can take? For example a die can only take on 6 quantitative values, which is not enough.</p></li>
<li><p>Are most/all of the observed values far from their bounds? Human height does not really get much smaller than about 50 cm and longer than about 220 cm, so it is technically bounded. However, in most cases our observations are expected to not be clustered at the boundaries.</p></li>
</ul>
<p>If survey participants were given a survey with 5 discrete numerical categories and were told that this represented some continuous value ‘agreement’, then we could perhaps argue that that 5-point Likert scale data abides by the first two considerations above. However, our data runs afoul of the bottom two considerations: There are only a small number of discrete outcomes, with nothing ‘in between’, and many of the observed values are likely to be near the boundaries and are likely to be constrained by these.</p>
<p>We don’t actually have any ordinal variables in our experiment, but we still wanted to provide an example of an ordinal analysis. To do this, we’re going to make a fake ordinal variable that we think is actually relatively plausible. In figure <a href="multinomial-and-ordinal-regression.html#fig:F12-9">12.9</a>, we see a boxplot of apparent height judgments organized by apparent speaker category, with boys and girls collapsed into one category, ‘child’. Note that apparent children are judged to be shortest, apparent women are judged to be a little taller than that, and apparent men are judged to be a little taller than women. In addition, there is not very much overlap in apparent heights between categories. Based on the above, we could potentially think of our apparent speaker categories as reflecting classifications of speakers into small (boys, girls), medium (women), and large (men). We might imagine that if we <em>had</em> asked listeners to identify speakers as small, medium, or large, listeners would have labelled most apparent children as small, most apparent women as medium, and most apparent men as large.</p>
<div class="figure"><span style="display:block;" id="fig:F12-9"></span>
<img src="_main_files/figure-html/F12-9-1.jpeg" alt="(left) Distribution of apparent height judgments for apparent children, women, and men. (right) A hypothetical linear relationship between apparent height and speaker vocal-tract length. The apparent height axis is divided into three size groups: Small, medium, and large. Points indicate three possible apparent height judgments, leading to three different size-group classifications." width="4800" />
<p class="caption">
Figure 12.9: (left) Distribution of apparent height judgments for apparent children, women, and men. (right) A hypothetical linear relationship between apparent height and speaker vocal-tract length. The apparent height axis is divided into three size groups: Small, medium, and large. Points indicate three possible apparent height judgments, leading to three different size-group classifications.
</p>
</div>
<p>The right plot in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-9">12.9</a> presents the y axis divided into three sections. The boundaries between sections are halfway between the means of apparent children and adult females (155.7 cm), and apparent adult females and adult males (170 cm). We can think of these sections as reflecting the expected categorization of speakers into small, medium, or large based on their apparent height. We can see how this might work using the code below. First, we convert our observed speaker classifications into a new variable that we will treat as ordinal. This variable is <em>size group</em> (<code>SG</code>), and it has the values 1 (small), 2 (medium), and 3 (large).</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="multinomial-and-ordinal-regression.html#cb476-1" aria-hidden="true" tabindex="-1"></a>SG <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb476-2"><a href="multinomial-and-ordinal-regression.html#cb476-2" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;b&#39;</span> <span class="sc">|</span> exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;g&#39;</span>] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb476-3"><a href="multinomial-and-ordinal-regression.html#cb476-3" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;w&#39;</span>] <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb476-4"><a href="multinomial-and-ordinal-regression.html#cb476-4" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;m&#39;</span>] <span class="ot">=</span> <span class="dv">3</span></span></code></pre></div>
<p>Then, we create a new variable that will hold our size group predictions, <code>SG_hat</code>. After this, we set this variable to 1 if apparent height is less than the boundary between small and medium (155.7 cm), to 2 if apparent height is greater than the boundary between small and medium (155.7), but less than the boundary between medium and large (170 cm), and to 3 if it is greater than the boundary between medium and large (170 cm).</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="multinomial-and-ordinal-regression.html#cb477-1" aria-hidden="true" tabindex="-1"></a>SG_hat <span class="ot">=</span> exp_data<span class="sc">$</span>height <span class="sc">*</span> <span class="dv">0</span></span>
<span id="cb477-2"><a href="multinomial-and-ordinal-regression.html#cb477-2" aria-hidden="true" tabindex="-1"></a>SG_hat[exp_data<span class="sc">$</span>height <span class="sc">&lt;=</span> <span class="fl">155.7</span>] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb477-3"><a href="multinomial-and-ordinal-regression.html#cb477-3" aria-hidden="true" tabindex="-1"></a>SG_hat[exp_data<span class="sc">$</span>height <span class="sc">&gt;</span> <span class="fl">155.7</span> <span class="sc">&amp;</span> exp_data<span class="sc">$</span>height <span class="sc">&lt;=</span> <span class="dv">170</span>] <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb477-4"><a href="multinomial-and-ordinal-regression.html#cb477-4" aria-hidden="true" tabindex="-1"></a>SG_hat[exp_data<span class="sc">$</span>height <span class="sc">&gt;=</span> <span class="dv">170</span>] <span class="ot">=</span> <span class="dv">3</span></span></code></pre></div>
<p>This relatively primitive ‘model’ is able to correctly predict size group responses in 75% of cases.</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="multinomial-and-ordinal-regression.html#cb478-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG <span class="sc">==</span> SG_hat)</span>
<span id="cb478-2"><a href="multinomial-and-ordinal-regression.html#cb478-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.7549</span></span></code></pre></div>
<p>The line in the right plot in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-9">12.9</a> reflects the relationship between speaker VTL and apparent height. We know from earlier chapters that a longer speaker VTL is associated with taller apparent speakers. Let’s assume for the sake of simplicity that this relationship is causal and that there is a direct connection between speaker VTL and apparent height. This means that when we hear a voice, we can be thought of as sliding along the line in the plot based on the VTL of the speaker. Then, based on the apparent height value at a given x axis location, we can arrive at a size group classification based on the method above. Note that in this scenario we were not collecting or observing apparent speaker height but rather predicting size group using VTL. However, we assume that apparent height underlies this process and connects the two variables we do observe. By assuming that apparent height was driving this process but not trying to measure it directly we are treating it as a <em>latent variable</em>.</p>
<p>The latent variable was presented as apparent height, however, the latent variable could have had any name and used any other units. For example, in our situation we can think of this variable as ‘size’ or ‘bigness’, but its name is not important, and neither is us having a rock-solid conceptual grasp of what it is exactly. Instead, all we need to understand is that we think there is some latent variable, something like ‘bigness’, that allows listeners to classify speakers into small, medium, and large groups based on their voices.</p>
<p>The right side of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-10">12.10</a> is labelled “bigness” to correspond to our hypothetical latent variable. This variable is just apparent height divided by 50, but the point is that the values of this axis are entirely arbitrary. Notice that the change in axis units has absolutely no effect on the way our conceptual model works. If the latent variable units are 50 times smaller than some other variable, that just means the slope of the line relating x to y is 50 times smaller and the boundaries between categories are 50 times closer. The same logic applies to the relationship between our latent dependent variable and our predictors.</p>
<div class="figure"><span style="display:block;" id="fig:F12-10"></span>
<img src="_main_files/figure-html/F12-10-1.jpeg" alt="Our three possible size group classifications from figure 12.9 are presented again, this time with error distributions around predicted values. The right side of the plot shows an alternate y axis, the made up latent variable 'bigness'." width="4800" />
<p class="caption">
Figure 12.10: Our three possible size group classifications from figure 12.9 are presented again, this time with error distributions around predicted values. The right side of the plot shows an alternate y axis, the made up latent variable ‘bigness’.
</p>
</div>
<p>Our ‘model’ so far would do a reasonable job of predicting classification but contains no random component. As a result, given a certain speaker VTL it would always predict the same size group response. In figure <a href="quantitative-predictors-and-their-interactions-with-factors.html#fig:F9-1">9.1</a> we showed that bivariate linear regression with normally-distributed errors can be thought of as a normal distribution sliding along a line based on the value of the dependent variable. The y axis value of the line at an x axis location tells you the expected value of y for that value of x. However, we will hardly (if ever) observe the expected value exactly. Instead, we assume that we have normal errors centered around our expected values.</p>
<p>We show a similar situation in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-10">12.10</a>, with a probability distribution sliding along a line. However, in the case of ordinal regression the values that this distribution generates are not the observed values of the ordinal dependent variable, but rather random values of the <em>latent</em> variable (e.g. ‘bigness’). The observed ordinal variable is then based on the value of the latent variable with respect to the boundaries in the space (the horizontal lines in the figure). In this chapter we will use a <em>logistic</em> distribution as the error distribution (more detail on this in the next section), hence the name, ordinal <em>logistic</em> regression. We will do this because it is commonly used and has some useful characteristics, but a basically equivalent model can be constructed using a normal distribution to represent randomness in the latent variable.</p>
<p>Ordinal regression estimates the location of boundaries between categories along the latent variable. It also tries to predict the expected value of the latent variable for each observation, based on the combination of the dependent variables. For example, in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-10">12.10</a> the boundaries are at 31 and 34 bigness, and you have an expected bigness of around 32 for a vocal-tract length of 14.1. We can think of it this way: Given a VTL of 14.1 we know how big people will sound in general (32), but these things are fuzzy and necessarily noisy. So actually, there is a distribution of expected perceived/apparent ‘bigness’ given our predictors. Given the actual value of the latent variable for a trial (including the random component), the ordinal response is based on the value of the latent variable with respect to the estimated category boundaries. So, for a fixed expected value of 32, sometimes the distribution might ‘generate’ a bigness value of 28, resulting in a response of small. For another observation with the same expected value, the value of the latent variable may be 36, meaning the speaker will be identified as large.</p>
<div id="c12-cumulative-density" class="section level3 hasAnchor" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Cumulative distribution functions<a href="multinomial-and-ordinal-regression.html#c12-cumulative-density" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to understand ordinal regression, we need to talk about cumulative distribution functions first. We haven’t explicitly talked about cumulative distribution functions yet, though we did see one in chapter 10 when it was referred to as the <em>inverse logit</em> function (more on this in a moment). To this point we’ve been focused on what are called <em>probability density functions</em> (PDF), functions that assign a density to different values of a variable. Examples of PDFs are given in the top row of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a>. <strong>Cumulative distribution functions</strong> (CDF) tell you the probability that a variable will be <em>less than or equal to</em> some value. Since a PDF has an area under the curve equal to 1 (representing the total probability of the variable), a CDF tells you how much of that area (out of 1) is to the left of the value, and has a range from 0 to 1.</p>
<p>For example, in the left column of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a> we see a standard normal distribution with a mean of 0 and a standard deviation of one. We know that the normal distribution is symmetrical about its mean so that half the area under the curve of this density must be below 0. If we look at the bottom row of the first column, we can see the CDF corresponding to the PDF in the top row. If we look at the value of this function at x=0, we see that it is exactly equal to 0.5. In other words, the cumulative function tells us that the standard normal has exactly half its mass at values less than x=0, i.e. the probability of observing a value of x that is less than or equal to 0, the mean, is exactly 0.5.</p>
<div class="figure"><span style="display:block;" id="fig:F12-11"></span>
<img src="_main_files/figure-html/F12-11-1.jpeg" alt="(top row) Probability density functions for a standard normal and standard logistic distributions. (bottom row) Cumulative distribution functions for densities immediately above. Numbers indicate x and y values at different points along each curve." width="4800" />
<p class="caption">
Figure 12.11: (top row) Probability density functions for a standard normal and standard logistic distributions. (bottom row) Cumulative distribution functions for densities immediately above. Numbers indicate x and y values at different points along each curve.
</p>
</div>
<p>We can use the <code>pnorm</code> function (discussed in chapter 2) to get the value of the normal CDF for any value of x. Below we calculate this for x=0, x= -2, and x=2. See that in each case, the output of the function corresponds to the y axis value of the CDF at that x axis location (bottom left, figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a>).</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="multinomial-and-ordinal-regression.html#cb479-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cumulative distribution at x = -2, 0, 2</span></span>
<span id="cb479-2"><a href="multinomial-and-ordinal-regression.html#cb479-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span> (<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>),<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb479-3"><a href="multinomial-and-ordinal-regression.html#cb479-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.02275 0.50000 0.97725</span></span></code></pre></div>
<p>The CDF is the <strong>integral</strong> of the PDF, and the PDF is the <strong>derivative</strong> of the CDF. We’ve already explained the <em>integral</em> part of this: As you move left to right, the value of the CDF equals the cumulative (i.e. added up total) area under the curve (i.e. probability) in the PDF to that point. To say that the PDF is the derivative of the CDF means that the value of the PDF reflects the <em>rate of change</em> (i.e. the slope) in the CDF for different values of x. Imagine you were in a tiny car driving along the standard normal CDF in figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a>, left to right. As you drive along around -4 the ‘terrain’ is flat and the value of the density is near 0. As you near -2 the slope of the CDF begins to increase, as does the value of the PDF. The value of the PDF is largest at x=0, telling us that the slope of the CDF is greatest at that point. As we drive past x=0 in the CDF, we see a gradual decrease in the slope until we more or less reach ‘flat ground’ again at just past x=2. This is reflected by the gradual decrease in the value of the PDF between x=0 and x=2.</p>
<p>In the middle column of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a> we see a logistic distribution. The <strong>logistic distribution</strong> is a two-parameter distribution similar in shape to the normal distribution but with ‘fatter/heavier’ tails (like the t distribution). The logistic distribution has a location parameter (<span class="math inline">\(\mu\)</span>, equal to the mean) that determines its location along the number line, and a scale parameter (<span class="math inline">\(s\)</span>, equal to the standard deviation times <span class="math inline">\(\sqrt{3}/\pi\)</span>) that is positively related to the ‘width’ of the density function. In the bottom row of the middle column, we see the CDF of the logistic density. It turns out that the inverse logit function we use as the link function for logistic regression is simply the CDF of a logistic distribution with a mean of 0 and a scale of 1 (i.e. a <em>standard</em> logistic distribution). This is also why you might commonly see the inverse logit function referred to as the logistic function. We avoided using this name for the function to avoid confusion.</p>
<p>In the middle column of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a> we see that we can use the same approach we used for our normal distribution to calculate values of the CDF of our logistic distribution. Below, we use the <code>plogis</code> function to calculate the probability of observing a value greater than or equal to 2 form that distribution.</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="multinomial-and-ordinal-regression.html#cb480-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plogis</span> (<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb480-2"><a href="multinomial-and-ordinal-regression.html#cb480-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8808</span></span></code></pre></div>
<p>In the right column of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a> we’ve placed two vertical lines at x= -1 and x=2. We use the code below to calculate the area under the curve to the left of each line.</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="multinomial-and-ordinal-regression.html#cb481-1" aria-hidden="true" tabindex="-1"></a><span class="co"># area left of first line</span></span>
<span id="cb481-2"><a href="multinomial-and-ordinal-regression.html#cb481-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plogis</span> (<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb481-3"><a href="multinomial-and-ordinal-regression.html#cb481-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.2689</span></span>
<span id="cb481-4"><a href="multinomial-and-ordinal-regression.html#cb481-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb481-5"><a href="multinomial-and-ordinal-regression.html#cb481-5" aria-hidden="true" tabindex="-1"></a><span class="co"># area left of second line</span></span>
<span id="cb481-6"><a href="multinomial-and-ordinal-regression.html#cb481-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plogis</span> (<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb481-7"><a href="multinomial-and-ordinal-regression.html#cb481-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8808</span></span></code></pre></div>
<p>At this point, the connection between CDFs and ordinal regression may be clear: We can use a set of boundaries and the CDF of a probability distribution to calculate the probability of observing different values of the latent variable, within certain intervals. For example, in the bottom right plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-11">12.11</a>, if 0.27 falls to the left of the first line and 0.88 falls to the left of the second line, then 0.61 must fall between the first and second lines. Thus, we have effectively divided the distribution into three parts and can discuss the probability that the observation of the variable will fall within each part.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="multinomial-and-ordinal-regression.html#cb482-1" aria-hidden="true" tabindex="-1"></a><span class="co"># area between first and second line</span></span>
<span id="cb482-2"><a href="multinomial-and-ordinal-regression.html#cb482-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plogis</span> (<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)<span class="sc">-</span><span class="fu">plogis</span> (<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb482-3"><a href="multinomial-and-ordinal-regression.html#cb482-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.6119</span></span></code></pre></div>
<p>In figure <a href="multinomial-and-ordinal-regression.html#fig:F12-12">12.12</a>, the boundaries and distributions from figure <a href="multinomial-and-ordinal-regression.html#fig:F12-10">12.10</a> have been rotated clockwise 90 degrees so that our latent variable (the arbitrarily named ‘bigness’) varies along the x axis. The distributions presented in the figure have means of 28, 32, 36, and scales of 1. The probabilities in each subsection of each plot represent the predicted probability of observing size group responses of 1, 2, and 3 for each distribution. These probabilities were calculated based on the area under the curve of the distributions falling between different intervals of the latent variable space.</p>
<div class="figure"><span style="display:block;" id="fig:F12-12"></span>
<img src="_main_files/figure-html/F12-12-1.jpeg" alt="Error distributions from figure \@ref(fig:F12-10), presented in more detail. Vertical lines indicate the expected value for each distribution. Numbers indicate the area under the curve of the density function inside of each category's section of the 'bigness' dimension. These values correspond to the expected probability of observing each of the size group responses for each expected value of the latent variable." width="4800" />
<p class="caption">
Figure 12.12: Error distributions from figure <a href="multinomial-and-ordinal-regression.html#fig:F12-10">12.10</a>, presented in more detail. Vertical lines indicate the expected value for each distribution. Numbers indicate the area under the curve of the density function inside of each category’s section of the ‘bigness’ dimension. These values correspond to the expected probability of observing each of the size group responses for each expected value of the latent variable.
</p>
</div>
<p>For example, below we calculate the probability of observing each size group based on the distribution in the middle plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-12">12.12</a>, a latent variable distribution with a mean of 32 and a scale of 1. First, we find the probability of observing a bigness value less than 31 (the first boundary). This tells us the probability of observing a response of size group 1 for this expected value of bigness (P(SG=1)=0.26). Then, we again find the probability of a bigness value less than 31 (the first boundary), and subtract this from the probability of a bigness value less than 34 (the second boundary). This operation gives us the probability of observing a bigness value between the first and second boundaries, i.e. the probability of observing a medium size group response (P(SG=2) = 0.88 - 0.26). Finally, to find the probability of a large size group response we find the probability of a bigness value less than 34 and subtract this from 1. Since there are no more boundaries above the largest response category, the entire probability above the highest threshold must correspond to the final category.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="multinomial-and-ordinal-regression.html#cb483-1" aria-hidden="true" tabindex="-1"></a><span class="co"># probability of observing category 1</span></span>
<span id="cb483-2"><a href="multinomial-and-ordinal-regression.html#cb483-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plogis</span> (<span class="dv">31</span>,<span class="dv">32</span>,<span class="dv">1</span>)</span>
<span id="cb483-3"><a href="multinomial-and-ordinal-regression.html#cb483-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.2689</span></span>
<span id="cb483-4"><a href="multinomial-and-ordinal-regression.html#cb483-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb483-5"><a href="multinomial-and-ordinal-regression.html#cb483-5" aria-hidden="true" tabindex="-1"></a><span class="co"># probability of observing category 2</span></span>
<span id="cb483-6"><a href="multinomial-and-ordinal-regression.html#cb483-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plogis</span> (<span class="dv">34</span>,<span class="dv">32</span>,<span class="dv">1</span>) <span class="sc">-</span> <span class="fu">plogis</span> (<span class="dv">31</span>,<span class="dv">32</span>,<span class="dv">1</span>)</span>
<span id="cb483-7"><a href="multinomial-and-ordinal-regression.html#cb483-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.6119</span></span>
<span id="cb483-8"><a href="multinomial-and-ordinal-regression.html#cb483-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb483-9"><a href="multinomial-and-ordinal-regression.html#cb483-9" aria-hidden="true" tabindex="-1"></a><span class="co"># probability of observing category 3</span></span>
<span id="cb483-10"><a href="multinomial-and-ordinal-regression.html#cb483-10" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">plogis</span> (<span class="dv">34</span>,<span class="dv">32</span>,<span class="dv">1</span>)</span>
<span id="cb483-11"><a href="multinomial-and-ordinal-regression.html#cb483-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1192</span></span></code></pre></div>
<p>This conceptualization of our model leads to both <strong>hard classification</strong> and <strong>soft classification</strong>. An ordinal regression model will predict, for every observation, an expected value of the dependent variable based on the values of the predictors. This value can then be used to find response probabilities for each category. A <em>hard</em> classification classifies each observation into a predicted category, 1, 2, or 3, based on the value of the highest predicted probability across the categories. A <em>soft</em> classification provides the probability that an observation is classified into one or more categories. For example, in the middle plot of figure <a href="multinomial-and-ordinal-regression.html#fig:F12-12">12.12</a>, it may be useful to say that the most probable classification is of a ‘medium’ speaker. However, it may be just as, or more, useful to indicate that the probability of this response was only 0.61, making the observation of a small response (P=0.26) not unlikely.</p>
</div>
<div id="data-and-research-questions-10" class="section level3 hasAnchor" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Data and research questions<a href="multinomial-and-ordinal-regression.html#data-and-research-questions-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We load our packages and data below. We also add a new variable, <code>SG</code> (size group), which will act as our dependent variable. This variable represents children with a 1, adult women with a 2, and adult males with a 3. We don’t need to do anything special to make this variable ‘ordinal’, but we are using three consecutive integers to represent the categories to make things simpler. After this, we process our quantitative predictors in the same way as in the previous chapters.</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="multinomial-and-ordinal-regression.html#cb484-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb484-2"><a href="multinomial-and-ordinal-regression.html#cb484-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb484-3"><a href="multinomial-and-ordinal-regression.html#cb484-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb484-4"><a href="multinomial-and-ordinal-regression.html#cb484-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb484-5"><a href="multinomial-and-ordinal-regression.html#cb484-5" aria-hidden="true" tabindex="-1"></a><span class="co"># new dependent variable: Size Group</span></span>
<span id="cb484-6"><a href="multinomial-and-ordinal-regression.html#cb484-6" aria-hidden="true" tabindex="-1"></a>SG <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb484-7"><a href="multinomial-and-ordinal-regression.html#cb484-7" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;g&#39;</span>] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb484-8"><a href="multinomial-and-ordinal-regression.html#cb484-8" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;b&#39;</span>] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb484-9"><a href="multinomial-and-ordinal-regression.html#cb484-9" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;w&#39;</span>] <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb484-10"><a href="multinomial-and-ordinal-regression.html#cb484-10" aria-hidden="true" tabindex="-1"></a>SG[exp_data<span class="sc">$</span>C<span class="sc">==</span><span class="st">&#39;m&#39;</span>] <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb484-11"><a href="multinomial-and-ordinal-regression.html#cb484-11" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>SG <span class="ot">=</span> SG</span>
<span id="cb484-12"><a href="multinomial-and-ordinal-regression.html#cb484-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb484-13"><a href="multinomial-and-ordinal-regression.html#cb484-13" aria-hidden="true" tabindex="-1"></a><span class="co"># preparation of quantitative predictors as in previous chapters</span></span>
<span id="cb484-14"><a href="multinomial-and-ordinal-regression.html#cb484-14" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl_original <span class="ot">=</span> exp_data<span class="sc">$</span>vtl</span>
<span id="cb484-15"><a href="multinomial-and-ordinal-regression.html#cb484-15" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>vtl <span class="ot">=</span> exp_data<span class="sc">$</span>vtl <span class="sc">-</span> <span class="fu">mean</span> (exp_data<span class="sc">$</span>vtl)</span>
<span id="cb484-16"><a href="multinomial-and-ordinal-regression.html#cb484-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb484-17"><a href="multinomial-and-ordinal-regression.html#cb484-17" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>f0_original <span class="ot">=</span> exp_data<span class="sc">$</span>f0 </span>
<span id="cb484-18"><a href="multinomial-and-ordinal-regression.html#cb484-18" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>f0 <span class="ot">=</span> exp_data<span class="sc">$</span>f0 <span class="sc">-</span> <span class="fu">mean</span>(exp_data<span class="sc">$</span>f0)</span>
<span id="cb484-19"><a href="multinomial-and-ordinal-regression.html#cb484-19" aria-hidden="true" tabindex="-1"></a>exp_data<span class="sc">$</span>f0 <span class="ot">=</span> exp_data<span class="sc">$</span>f0 <span class="sc">/</span> <span class="dv">100</span></span></code></pre></div>
<p>We’re going to keep our research questions relatively simple this time, as this is mostly a demonstration of an ordinal regression analysis and not a ‘real’ ordinal variable:</p>
<p>(Q1) Can we predict size group responses for a speaker given their f0 and VTL?</p>
</div>
<div id="description-of-the-model-11" class="section level3 hasAnchor" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Description of the model<a href="multinomial-and-ordinal-regression.html#description-of-the-model-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To fit a model predicting apparent size group from speaker f0 and VTL, we can use the formula below:</p>
<p><code>SG ~ vtl+f0 + (vtl+f0|L) + (1|S)</code></p>
<p>This model formula is very much like ones we’ve used several times in earlier chapters. The formula says “predict size group using a plane based on voice f0 and VTL, allow for listener-specific planes, and speaker-specific adjustments to the height of the plane”. However, the relationship it represents between our predictors and our dependent variables is substantially different to what we’ve seen before. Let’s begin with the relationship between the independent variables and the expected value of the latent variable (i.e., the mean of our logistic distribution), presented in equation <a href="multinomial-and-ordinal-regression.html#eq:12-11">(12.16)</a>:</p>
<p><span class="math display" id="eq:12-11">\[
\begin{equation}
\begin{split}
\mu_{[i]} = a_{[i]} + b_{[i]} \cdot \mathrm{vtl}_{[i]} + c_{[i]} \cdot \mathrm{f0}_{[i]} \end{split}
\tag{12.16}
\end{equation}
\]</span></p>
<p>We model this latent variable as being distributed according to a logistic distribution with a mean of <span class="math inline">\(\mu\)</span> and a scale parameter equal to 1, as in equation <a href="multinomial-and-ordinal-regression.html#eq:12-12">(12.17)</a>. Actually, for reasons that we won’t get into, for ordinal regression <code>brm</code> uses a parameter called the <strong>discrimination</strong> parameter, which is the inverse of the scale. However, since we are using a scale of 1 we can ignore this for now (since 1/1=1).</p>
<p><span class="math display" id="eq:12-12">\[
\begin{equation}
\begin{split}
z_{[i]} \sim \mathrm{Logistic} (\mu_{[i]}, 1)
\end{split}
\tag{12.17}
\end{equation}
\]</span>
Our observed (dependent) ordinal category is then predicted based on the value of the latent variable (<span class="math inline">\(z_{[i]}\)</span>) for that trial, relative to the model boundaries, called <strong>thresholds</strong>, or <strong>cutoffs</strong> (<span class="math inline">\(\theta_{[1]},\theta_{[2]}\)</span>). We see this in <a href="multinomial-and-ordinal-regression.html#eq:12-13">(12.18)</a> below.</p>
<p><span class="math display" id="eq:12-13">\[
\begin{equation}
\begin{split}
SG_{[i]} =
\begin{cases}
1 \; \mathrm{if} \; z_{[i]} \leq \theta_1 \\
2 \; \mathrm{if} \; \theta_{[1]} &lt; z_{[i]} \leq \theta_{[2]} \\
3 \; \mathrm{if} \; \theta_{[2]} \leq z_{[i]} \\
\end{cases}       
\end{split}
\tag{12.18}
\end{equation}
\]</span></p>
<p>The above says: “SG is equal to 1 if the latent variable is below the first threshold, 2 if it’s between the first and second thresholds, and 3 if it is above the second threshold. Here is a full description of our model:</p>
<p><span class="math display" id="eq:12-14">\[
\begin{equation}
\begin{split}
SG_{[i]} =
\begin{cases}
1 \; \mathrm{if} \; z_{[i]} \leq \theta_{[1]} \\
2 \; \mathrm{if} \; \theta_{[1]} &lt; z_{[i]} \leq \theta_{[2]} \\
3 \; \mathrm{if} \; \theta_{[2]} \leq z_{[i]} \\
\end{cases}
\\\\
z_{[i]} \sim \mathrm{Logistic} (\mu_{[i]}, 1)\\
\\
\mu_{[i]} = a_{[i]} + b_{[i]} \cdot \mathrm{vtl}_{[i]} + c_{[i]} \cdot \mathrm{f0}_{[i]}  \\
a_{[i]} =  L_{[\mathsf{L}_{[i]}]} \\
b_{[i]} =  VTL + VTL \colon L_{[\mathsf{L}_{[i]}]} \\
c_{[i]} =  F0 + F0 \colon L_{[\mathsf{L}_{[i]}]} \\
\\
\textrm{Priors:} \\
S_{[\bullet]} \sim \mathrm{Normal}(0,\sigma_{S})
\\
\begin{bmatrix} L_{[\bullet]} \\ VTL \colon L_{[\bullet]} \\ F0 \colon L_{[\bullet]} \end{bmatrix}  
\sim \mathrm{MVNormal} \left(\, \begin{bmatrix} 0\\ 0 \\ 0 \\ \end{bmatrix}, \mathrm{\Sigma} \right) \\ \\
\theta_{[1]}, \theta_{[2]} \sim \mathrm{t}(3, 0, 3) \\
VTL, F0 \sim \mathrm{t}(3, 0, 3) \\
\sigma_{L}, \sigma_{VTL \colon L}, \sigma_{F0 \colon L}, \sigma_{S} \sim \mathrm{t}(3, 0, 3) \\ R \sim \mathrm{LKJCorr} (2)
\end{split}
\tag{12.19}
\end{equation}
\]</span></p>
<p>And here is a plain English, verbal description of our model:</p>
<blockquote>
<p>We are modelling an ordinal variable, size group (<span class="math inline">\(SG\)</span>), with possible outcomes of small (1), medium (2), and large (3). The value expected for any given trial is based on the value of a latent variable (<span class="math inline">\(z\)</span>) in relation to two thresholds (<span class="math inline">\(\theta_{[1]},\theta_{[2]}\)</span>), which were estimated from the data. Our latent variable is modeled as coming from a logistic distribution with a scale of 1 and a mean that varies from trial to trial. The mean of these distributions varies based on the combination of ‘main’ (e.g., <span class="math inline">\(VTL\)</span>) and listener-dependent (e.g. <span class="math inline">\(VTL \colon L\)</span>) effects for vocal tract length and f0, and a listener-dependent intercept. The speaker intercept (<span class="math inline">\(S\)</span>) terms were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The listener random effects were drawn from a multivariate normal distribution with means of zero and a correlation matrix (R) estimated from the data. All other effects were treated as ‘fixed’ and drawn from prior distributions appropriate for their expected range of values.</p>
</blockquote>
<p>Notice that the model in <a href="multinomial-and-ordinal-regression.html#eq:12-14">(12.19)</a> does not contain a term representing the overall model intercept. This is because the important thing for our model is the distance between the expected value of the latent variable and the thresholds, and not the value of the latent variable itself. For example, imagine an ‘intercept-only’ model (chapter 4) that had intercept = 3 and thresholds of 1 and 5. The behavior of this model would be exactly analogous to a model with an intercept of 13 and thresholds of 11 and 15. In fact, there are an infinite number of models that maintain this structure, one for each possible value of the intercept. Under these conditions finding the ‘best’ model is an impossible task (i.e. the model is not <em>identifiable</em>, see section <a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability">8.8</a>). To resolve this issue, the overall intercept in these models is usually set to zero and not modeled, and instead only the <span class="math inline">\(J-1\)</span> thresholds are modeled (for <span class="math inline">\(J\)</span> categories).</p>
<p>Although ordinal models do not usually estimate the overall intercept, they <em>can</em> estimate listener-dependent (and speaker-dependent) intercepts, and our model in equation does contain these. Since the boundaries are shared by all groups, using group-dependent intercepts can be thought of as sliding the thresholds up and down the number line by different amounts for each level of a grouping factor (e.g. listener), or as moving the predicted values by the same amount. This sort of adjustment <em>can</em> have a meaningful effect on predictions, unlike changes in the overall model intercept outlined above.</p>
</div>
<div id="fitting-and-interpreting-the-model-10" class="section level3 hasAnchor" number="12.3.4">
<h3><span class="header-section-number">12.3.4</span> Fitting and interpreting the model<a href="multinomial-and-ordinal-regression.html#fitting-and-interpreting-the-model-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To fit an ordinal logistic model, you set <code>family=cumulative</code> when fitting your model. The logistic distribution is the default error distribution for these sorts of models, but other distributions such as the normal distribution can be used.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="multinomial-and-ordinal-regression.html#cb485-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb485-2"><a href="multinomial-and-ordinal-regression.html#cb485-2" aria-hidden="true" tabindex="-1"></a>model_ordinal <span class="ot">=</span> </span>
<span id="cb485-3"><a href="multinomial-and-ordinal-regression.html#cb485-3" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (SG <span class="sc">~</span> vtl<span class="sc">+</span>f0 <span class="sc">+</span> (vtl<span class="sc">+</span>f0<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data=</span>exp_data, </span>
<span id="cb485-4"><a href="multinomial-and-ordinal-regression.html#cb485-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">family=</span><span class="st">&quot;cumulative&quot;</span>, <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, <span class="at">warmup=</span><span class="dv">1000</span>, </span>
<span id="cb485-5"><a href="multinomial-and-ordinal-regression.html#cb485-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>,</span>
<span id="cb485-6"><a href="multinomial-and-ordinal-regression.html#cb485-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb485-7"><a href="multinomial-and-ordinal-regression.html#cb485-7" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb485-8"><a href="multinomial-and-ordinal-regression.html#cb485-8" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb485-9"><a href="multinomial-and-ordinal-regression.html#cb485-9" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>)))</span></code></pre></div>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="multinomial-and-ordinal-regression.html#cb486-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb486-2"><a href="multinomial-and-ordinal-regression.html#cb486-2" aria-hidden="true" tabindex="-1"></a>model_ordinal <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&quot;12_model_ordinal.RDS&quot;</span>)</span></code></pre></div>
<p>We can inspect the short summary to see the information it contains:</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="multinomial-and-ordinal-regression.html#cb487-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">short_summary</span>(model_ordinal)</span>
<span id="cb487-2"><a href="multinomial-and-ordinal-regression.html#cb487-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula:  SG ~ vtl + f0 + (vtl + f0 | L) + (1 | S)</span></span>
<span id="cb487-3"><a href="multinomial-and-ordinal-regression.html#cb487-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb487-4"><a href="multinomial-and-ordinal-regression.html#cb487-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Group-Level Effects:</span></span>
<span id="cb487-5"><a href="multinomial-and-ordinal-regression.html#cb487-5" aria-hidden="true" tabindex="-1"></a><span class="do">## ~L (Number of levels: 15)</span></span>
<span id="cb487-6"><a href="multinomial-and-ordinal-regression.html#cb487-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                    Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb487-7"><a href="multinomial-and-ordinal-regression.html#cb487-7" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)          0.54      0.15     0.30     0.89</span></span>
<span id="cb487-8"><a href="multinomial-and-ordinal-regression.html#cb487-8" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(vtl)                0.45      0.20     0.07     0.89</span></span>
<span id="cb487-9"><a href="multinomial-and-ordinal-regression.html#cb487-9" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(f0)                 0.67      0.40     0.04     1.55</span></span>
<span id="cb487-10"><a href="multinomial-and-ordinal-regression.html#cb487-10" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,vtl)     0.07      0.33    -0.58     0.67</span></span>
<span id="cb487-11"><a href="multinomial-and-ordinal-regression.html#cb487-11" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,f0)      0.14      0.36    -0.58     0.79</span></span>
<span id="cb487-12"><a href="multinomial-and-ordinal-regression.html#cb487-12" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(vtl,f0)           -0.11      0.37    -0.78     0.62</span></span>
<span id="cb487-13"><a href="multinomial-and-ordinal-regression.html#cb487-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb487-14"><a href="multinomial-and-ordinal-regression.html#cb487-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ~S (Number of levels: 139)</span></span>
<span id="cb487-15"><a href="multinomial-and-ordinal-regression.html#cb487-15" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb487-16"><a href="multinomial-and-ordinal-regression.html#cb487-16" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)     1.27      0.14     1.02     1.57</span></span>
<span id="cb487-17"><a href="multinomial-and-ordinal-regression.html#cb487-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb487-18"><a href="multinomial-and-ordinal-regression.html#cb487-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects:</span></span>
<span id="cb487-19"><a href="multinomial-and-ordinal-regression.html#cb487-19" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb487-20"><a href="multinomial-and-ordinal-regression.html#cb487-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept[1]    -1.86      0.22    -2.32    -1.44</span></span>
<span id="cb487-21"><a href="multinomial-and-ordinal-regression.html#cb487-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept[2]     2.58      0.24     2.13     3.07</span></span>
<span id="cb487-22"><a href="multinomial-and-ordinal-regression.html#cb487-22" aria-hidden="true" tabindex="-1"></a><span class="do">## vtl              3.16      0.29     2.63     3.76</span></span>
<span id="cb487-23"><a href="multinomial-and-ordinal-regression.html#cb487-23" aria-hidden="true" tabindex="-1"></a><span class="do">## f0              -1.78      0.60    -2.98    -0.58</span></span>
<span id="cb487-24"><a href="multinomial-and-ordinal-regression.html#cb487-24" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb487-25"><a href="multinomial-and-ordinal-regression.html#cb487-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Family Specific Parameters:</span></span>
<span id="cb487-26"><a href="multinomial-and-ordinal-regression.html#cb487-26" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb487-27"><a href="multinomial-and-ordinal-regression.html#cb487-27" aria-hidden="true" tabindex="-1"></a><span class="do">## disc        1         0        1        1</span></span></code></pre></div>
<p>In the population-level effects, we see the two estimated thresholds <code>Intercept[1]</code> and <code>Intercept[2]</code> representing <span class="math inline">\(\theta_{[1]}\)</span> and <span class="math inline">\(\theta_{[2]}\)</span> above. We also see our ‘main’ effects for VTL and f0 in this section. Our results indicate that VTL is positively related to a larger size group response, while f0 is negatively related to this (no surprises there). In the group-level effects section, note that we get listener-dependent intercepts, VTL, and f0 effects. The intercepts correspond to the <span class="math inline">\(L\)</span> terms in the model described in <a href="multinomial-and-ordinal-regression.html#eq:12-14">(12.19)</a>, and do not refer to listener-dependent thresholds/cutoffs. Below we make ‘fixed effect’ predictions using our model. These predictions omit the random effects structure to let us see how well our fixed effects can predict our data. In addition, we set <code>scale="linear"</code> so that we get predictions of the latent variable rather than the probability of category membership.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="multinomial-and-ordinal-regression.html#cb488-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make latent variable predictions</span></span>
<span id="cb488-2"><a href="multinomial-and-ordinal-regression.html#cb488-2" aria-hidden="true" tabindex="-1"></a>predictions_latent <span class="ot">=</span> <span class="fu">fitted</span> (model_ordinal,<span class="at">scale=</span><span class="st">&quot;linear&quot;</span>, <span class="at">re_formula =</span> <span class="cn">NA</span>)</span>
<span id="cb488-3"><a href="multinomial-and-ordinal-regression.html#cb488-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb488-4"><a href="multinomial-and-ordinal-regression.html#cb488-4" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect first 6 predictions</span></span>
<span id="cb488-5"><a href="multinomial-and-ordinal-regression.html#cb488-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (predictions_latent)</span>
<span id="cb488-6"><a href="multinomial-and-ordinal-regression.html#cb488-6" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error   Q2.5  Q97.5</span></span>
<span id="cb488-7"><a href="multinomial-and-ordinal-regression.html#cb488-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   -5.000    0.3558 -5.742 -4.330</span></span>
<span id="cb488-8"><a href="multinomial-and-ordinal-regression.html#cb488-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   -4.545    0.3958 -5.330 -3.783</span></span>
<span id="cb488-9"><a href="multinomial-and-ordinal-regression.html#cb488-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   -5.864    0.4669 -6.817 -4.994</span></span>
<span id="cb488-10"><a href="multinomial-and-ordinal-regression.html#cb488-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   -5.645    0.3486 -6.376 -4.978</span></span>
<span id="cb488-11"><a href="multinomial-and-ordinal-regression.html#cb488-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]   -4.728    0.2945 -5.340 -4.171</span></span>
<span id="cb488-12"><a href="multinomial-and-ordinal-regression.html#cb488-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   -7.115    0.5044 -8.168 -6.163</span></span></code></pre></div>
<p>Below, we predict our latent variable using our posterior mean fixed effects. We do this just to show that it does, in fact, result in the expected predictions. Below, we get the posterior mean model fixed effects and use these to come up with an expected value for our latent variable for each observation (<code>mu</code>).</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="multinomial-and-ordinal-regression.html#cb489-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get fixed effects</span></span>
<span id="cb489-2"><a href="multinomial-and-ordinal-regression.html#cb489-2" aria-hidden="true" tabindex="-1"></a>fixed_effects <span class="ot">=</span> brms<span class="sc">::</span><span class="fu">fixef</span>(model_ordinal)</span>
<span id="cb489-3"><a href="multinomial-and-ordinal-regression.html#cb489-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb489-4"><a href="multinomial-and-ordinal-regression.html#cb489-4" aria-hidden="true" tabindex="-1"></a>threshold_1 <span class="ot">=</span> fixed_effects[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb489-5"><a href="multinomial-and-ordinal-regression.html#cb489-5" aria-hidden="true" tabindex="-1"></a>threshold_2 <span class="ot">=</span> fixed_effects[<span class="dv">2</span>,<span class="dv">1</span>]</span>
<span id="cb489-6"><a href="multinomial-and-ordinal-regression.html#cb489-6" aria-hidden="true" tabindex="-1"></a>vtl_slope <span class="ot">=</span> fixed_effects[<span class="dv">3</span>,<span class="dv">1</span>]</span>
<span id="cb489-7"><a href="multinomial-and-ordinal-regression.html#cb489-7" aria-hidden="true" tabindex="-1"></a>f0_slope <span class="ot">=</span> fixed_effects[<span class="dv">4</span>,<span class="dv">1</span>]</span>
<span id="cb489-8"><a href="multinomial-and-ordinal-regression.html#cb489-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb489-9"><a href="multinomial-and-ordinal-regression.html#cb489-9" aria-hidden="true" tabindex="-1"></a><span class="co"># get expected values</span></span>
<span id="cb489-10"><a href="multinomial-and-ordinal-regression.html#cb489-10" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> exp_data<span class="sc">$</span>vtl <span class="sc">*</span> vtl_slope <span class="sc">+</span> exp_data<span class="sc">$</span>f0  <span class="sc">*</span> f0_slope</span></code></pre></div>
<p>We can see that the prediction above results in basically the same expected values for our latent variable as obtained using the <code>fitted</code> function. Of course, the <code>fitted</code> function finds these estimates the right way, computing a prediction for each posterior sample rather than only using the posterior mean.</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="multinomial-and-ordinal-regression.html#cb490-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (predictions_latent[,<span class="dv">1</span>], mu)</span>
<span id="cb490-2"><a href="multinomial-and-ordinal-regression.html#cb490-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1 </span></span>
<span id="cb490-3"><a href="multinomial-and-ordinal-regression.html#cb490-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb490-4"><a href="multinomial-and-ordinal-regression.html#cb490-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span> (predictions_latent[,<span class="dv">1</span>] <span class="sc">-</span> mu)</span>
<span id="cb490-5"><a href="multinomial-and-ordinal-regression.html#cb490-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3.571143e-16</span></span></code></pre></div>
<p>We can also ask for predictions as probabilities, rather than latent variable values, by not specifying the scale:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="multinomial-and-ordinal-regression.html#cb491-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict probability of category membership</span></span>
<span id="cb491-2"><a href="multinomial-and-ordinal-regression.html#cb491-2" aria-hidden="true" tabindex="-1"></a>predictions_latent <span class="ot">=</span> <span class="fu">fitted</span> (model_ordinal, <span class="at">re_formula =</span> <span class="cn">NA</span>)</span></code></pre></div>
<p>In this case our prediction results in a three dimensional matrix where response category varies along the third dimension (very much like our multinomial predictions). For example, below we see the probability of observing the first response category (small), for the first six observations.</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="multinomial-and-ordinal-regression.html#cb492-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see first six for the first category</span></span>
<span id="cb492-2"><a href="multinomial-and-ordinal-regression.html#cb492-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (predictions_latent[,,<span class="dv">1</span>])</span>
<span id="cb492-3"><a href="multinomial-and-ordinal-regression.html#cb492-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error   Q2.5  Q97.5</span></span>
<span id="cb492-4"><a href="multinomial-and-ordinal-regression.html#cb492-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   0.9557  0.016006 0.9184 0.9803</span></span>
<span id="cb492-5"><a href="multinomial-and-ordinal-regression.html#cb492-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   0.9314  0.026561 0.8714 0.9714</span></span>
<span id="cb492-6"><a href="multinomial-and-ordinal-regression.html#cb492-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   0.9799  0.009882 0.9566 0.9933</span></span>
<span id="cb492-7"><a href="multinomial-and-ordinal-regression.html#cb492-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   0.9763  0.008675 0.9561 0.9896</span></span>
<span id="cb492-8"><a href="multinomial-and-ordinal-regression.html#cb492-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]   0.9435  0.017720 0.9035 0.9723</span></span>
<span id="cb492-9"><a href="multinomial-and-ordinal-regression.html#cb492-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   0.9941  0.003051 0.9866 0.9981</span></span></code></pre></div>
<p>If we consider only the first element of the second dimension (columns), we can get the expected value for each category, for the first six observations.</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="multinomial-and-ordinal-regression.html#cb493-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see first six for each category</span></span>
<span id="cb493-2"><a href="multinomial-and-ordinal-regression.html#cb493-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (predictions_latent[,<span class="dv">1</span>,])</span>
<span id="cb493-3"><a href="multinomial-and-ordinal-regression.html#cb493-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      P(Y = 1) P(Y = 2)  P(Y = 3)</span></span>
<span id="cb493-4"><a href="multinomial-and-ordinal-regression.html#cb493-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   0.9557 0.043713 5.615e-04</span></span>
<span id="cb493-5"><a href="multinomial-and-ordinal-regression.html#cb493-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   0.9314 0.067657 8.960e-04</span></span>
<span id="cb493-6"><a href="multinomial-and-ordinal-regression.html#cb493-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]   0.9799 0.019885 2.480e-04</span></span>
<span id="cb493-7"><a href="multinomial-and-ordinal-regression.html#cb493-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]   0.9763 0.023454 2.943e-04</span></span>
<span id="cb493-8"><a href="multinomial-and-ordinal-regression.html#cb493-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]   0.9435 0.055775 7.227e-04</span></span>
<span id="cb493-9"><a href="multinomial-and-ordinal-regression.html#cb493-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]   0.9941 0.005824 7.232e-05</span></span></code></pre></div>
<p>We can use the expected value for the latent variable calculated manually above (<code>mu</code>), combined with the model thresholds, to calculate the expected probability that the observation will belong to each category (rounded to 5 decimal places).</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="multinomial-and-ordinal-regression.html#cb494-1" aria-hidden="true" tabindex="-1"></a>predictions_manual <span class="ot">=</span> </span>
<span id="cb494-2"><a href="multinomial-and-ordinal-regression.html#cb494-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span> (<span class="fu">round</span> (<span class="fu">plogis</span> (threshold_1, mu),<span class="dv">5</span>),</span>
<span id="cb494-3"><a href="multinomial-and-ordinal-regression.html#cb494-3" aria-hidden="true" tabindex="-1"></a>         <span class="fu">round</span> ((<span class="fu">plogis</span> (threshold_2, mu)<span class="sc">-</span><span class="fu">plogis</span> (threshold_1, mu)),<span class="dv">5</span>),</span>
<span id="cb494-4"><a href="multinomial-and-ordinal-regression.html#cb494-4" aria-hidden="true" tabindex="-1"></a>         <span class="fu">round</span> (<span class="dv">1</span><span class="sc">-</span><span class="fu">plogis</span> (threshold_2, mu), <span class="dv">5</span>))</span>
<span id="cb494-5"><a href="multinomial-and-ordinal-regression.html#cb494-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb494-6"><a href="multinomial-and-ordinal-regression.html#cb494-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (predictions_manual)</span>
<span id="cb494-7"><a href="multinomial-and-ordinal-regression.html#cb494-7" aria-hidden="true" tabindex="-1"></a><span class="do">##         [,1]    [,2]    [,3]</span></span>
<span id="cb494-8"><a href="multinomial-and-ordinal-regression.html#cb494-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] 0.95834 0.04115 0.00051</span></span>
<span id="cb494-9"><a href="multinomial-and-ordinal-regression.html#cb494-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] 0.93592 0.06328 0.00080</span></span>
<span id="cb494-10"><a href="multinomial-and-ordinal-regression.html#cb494-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,] 0.98201 0.01778 0.00021</span></span>
<span id="cb494-11"><a href="multinomial-and-ordinal-regression.html#cb494-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,] 0.97772 0.02202 0.00027</span></span>
<span id="cb494-12"><a href="multinomial-and-ordinal-regression.html#cb494-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,] 0.94604 0.05329 0.00067</span></span>
<span id="cb494-13"><a href="multinomial-and-ordinal-regression.html#cb494-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,] 0.99478 0.00515 0.00006</span></span></code></pre></div>
<p>Again, we can see that this approach results in basically the same values as those calculated with the <code>fitted</code> function.</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="multinomial-and-ordinal-regression.html#cb495-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (predictions_latent[,<span class="dv">1</span>,<span class="dv">1</span>],predictions_manual[,<span class="dv">1</span>])</span>
<span id="cb495-2"><a href="multinomial-and-ordinal-regression.html#cb495-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span>
<span id="cb495-3"><a href="multinomial-and-ordinal-regression.html#cb495-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb495-4"><a href="multinomial-and-ordinal-regression.html#cb495-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (predictions_latent[,<span class="dv">1</span>,<span class="dv">2</span>],predictions_manual[,<span class="dv">2</span>])</span>
<span id="cb495-5"><a href="multinomial-and-ordinal-regression.html#cb495-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span>
<span id="cb495-6"><a href="multinomial-and-ordinal-regression.html#cb495-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb495-7"><a href="multinomial-and-ordinal-regression.html#cb495-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (predictions_latent[,<span class="dv">1</span>,<span class="dv">3</span>],predictions_manual[,<span class="dv">3</span>])</span>
<span id="cb495-8"><a href="multinomial-and-ordinal-regression.html#cb495-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span></code></pre></div>
<p>Below we find the maximum value across each row of our predictions in <code>predictions_manual</code>, representing the <em>hard</em>, categorical prediction for each observation.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="multinomial-and-ordinal-regression.html#cb496-1" aria-hidden="true" tabindex="-1"></a>SG_hat <span class="ot">=</span> <span class="fu">apply</span> (predictions_latent[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span>
<span id="cb496-2"><a href="multinomial-and-ordinal-regression.html#cb496-2" aria-hidden="true" tabindex="-1"></a>SG_hat_manual <span class="ot">=</span> <span class="fu">apply</span> (predictions_manual,<span class="dv">1</span>,which.max)</span></code></pre></div>
<p>We can see that these two approaches lead to the same outcomes and that in either case we can correctly predict about 83% of our observations using only our fixed effects.</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="multinomial-and-ordinal-regression.html#cb497-1" aria-hidden="true" tabindex="-1"></a><span class="co"># manual and automatically-calculated predictions</span></span>
<span id="cb497-2"><a href="multinomial-and-ordinal-regression.html#cb497-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG_hat <span class="sc">==</span> SG_hat_manual)</span>
<span id="cb497-3"><a href="multinomial-and-ordinal-regression.html#cb497-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span>
<span id="cb497-4"><a href="multinomial-and-ordinal-regression.html#cb497-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb497-5"><a href="multinomial-and-ordinal-regression.html#cb497-5" aria-hidden="true" tabindex="-1"></a><span class="co"># predictive accuracy for actual size group responses </span></span>
<span id="cb497-6"><a href="multinomial-and-ordinal-regression.html#cb497-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG_hat <span class="sc">==</span> exp_data<span class="sc">$</span>SG)</span>
<span id="cb497-7"><a href="multinomial-and-ordinal-regression.html#cb497-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8264</span></span>
<span id="cb497-8"><a href="multinomial-and-ordinal-regression.html#cb497-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb497-9"><a href="multinomial-and-ordinal-regression.html#cb497-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG_hat_manual <span class="sc">==</span> exp_data<span class="sc">$</span>SG)</span>
<span id="cb497-10"><a href="multinomial-and-ordinal-regression.html#cb497-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8264</span></span></code></pre></div>
<p>Our focus has been on manually replicating some of the helper functions included in <code>brms</code> in order to highlight the relationship between our model parameters and expected outcomes. For more complex models, the specific effects of parameters or combinations of parameters can still be investigated using the techniques discussed in earlier chapters. For example, imagine an ordinal model involving a single quantitative predictor and a single categorical predictor with four levels. The geometry of the lines formed to predict expected values in such a model would be the same as that of the models discussed in chapter 9, and many of the strategies discussed in that chapter could be directly applied to such an analysis.</p>
</div>
<div id="listener-specific-discrimination-terms" class="section level3 hasAnchor" number="12.3.5">
<h3><span class="header-section-number">12.3.5</span> Listener-specific discrimination terms<a href="multinomial-and-ordinal-regression.html#listener-specific-discrimination-terms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before wrapping up our discussion of ordinal models, we want to talk briefly about the <code>disc</code> (discrimination) parameter. As mentioned above, this parameter is equal to the inverse of the scale parameter of the logistic distribution. In our previous ordinal model, the <code>disc</code> parameter was fixed at a value of 1 for all observations, However, this parameter can be predicted just as the error term (<code>sigma</code>, <span class="math inline">\(\sigma\)</span>) of our normal or t distributed errors was predicted in chapter 8. Since this value cannot be negative our model will predict the logarithm of <code>disc</code> just as it did for <code>sigma</code>.</p>
<p>Below we fit a model with listener-specific discrimination parameters. We’re not going to discuss this in detail, however, the additional model structure is extremely similar to that presented in chapter 8 (in particular section <a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2">8.6.1</a>), and should be clear based on the information provided in that chapter.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="multinomial-and-ordinal-regression.html#cb498-1" aria-hidden="true" tabindex="-1"></a>model_formula <span class="ot">=</span> <span class="fu">bf</span>(SG <span class="sc">~</span> vtl<span class="sc">+</span>f0 <span class="sc">+</span> (vtl<span class="sc">+</span>f0<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S),</span>
<span id="cb498-2"><a href="multinomial-and-ordinal-regression.html#cb498-2" aria-hidden="true" tabindex="-1"></a>                   disc <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>L))</span>
<span id="cb498-3"><a href="multinomial-and-ordinal-regression.html#cb498-3" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">=</span> </span>
<span id="cb498-4"><a href="multinomial-and-ordinal-regression.html#cb498-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb498-5"><a href="multinomial-and-ordinal-regression.html#cb498-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb498-6"><a href="multinomial-and-ordinal-regression.html#cb498-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb498-7"><a href="multinomial-and-ordinal-regression.html#cb498-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;disc&quot;</span>),</span>
<span id="cb498-8"><a href="multinomial-and-ordinal-regression.html#cb498-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>,<span class="at">dpar=</span><span class="st">&quot;disc&quot;</span>),</span>
<span id="cb498-9"><a href="multinomial-and-ordinal-regression.html#cb498-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>))</span>
<span id="cb498-10"><a href="multinomial-and-ordinal-regression.html#cb498-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb498-11"><a href="multinomial-and-ordinal-regression.html#cb498-11" aria-hidden="true" tabindex="-1"></a>model_ordinal_disc <span class="ot">=</span> </span>
<span id="cb498-12"><a href="multinomial-and-ordinal-regression.html#cb498-12" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (model_formula, <span class="at">data=</span>exp_data, <span class="at">family=</span><span class="st">&quot;cumulative&quot;</span>, <span class="at">chains=</span><span class="dv">4</span>, </span>
<span id="cb498-13"><a href="multinomial-and-ordinal-regression.html#cb498-13" aria-hidden="true" tabindex="-1"></a>             <span class="at">cores=</span><span class="dv">4</span>, <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>, <span class="at">prior =</span> priors,</span>
<span id="cb498-14"><a href="multinomial-and-ordinal-regression.html#cb498-14" aria-hidden="true" tabindex="-1"></a>             <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.99</span>))</span></code></pre></div>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="multinomial-and-ordinal-regression.html#cb499-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb499-2"><a href="multinomial-and-ordinal-regression.html#cb499-2" aria-hidden="true" tabindex="-1"></a>model_ordinal_disc <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&quot;12_model_ordinal_disc.RDS&quot;</span>)</span></code></pre></div>
<p>We can see that our model summary now includes a fixed effect for the <code>disc_Intercept</code> and the standard deviation of the listener-dependent discrimination terms (<code>sd(disc_Intercept)</code>) in the listener random effects section.</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="multinomial-and-ordinal-regression.html#cb500-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">short_summary</span> (model_ordinal_disc)</span>
<span id="cb500-2"><a href="multinomial-and-ordinal-regression.html#cb500-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula:  SG ~ vtl + f0 + (vtl + f0 | L) + (1 | S)</span></span>
<span id="cb500-3"><a href="multinomial-and-ordinal-regression.html#cb500-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb500-4"><a href="multinomial-and-ordinal-regression.html#cb500-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Group-Level Effects:</span></span>
<span id="cb500-5"><a href="multinomial-and-ordinal-regression.html#cb500-5" aria-hidden="true" tabindex="-1"></a><span class="do">## ~L (Number of levels: 15)</span></span>
<span id="cb500-6"><a href="multinomial-and-ordinal-regression.html#cb500-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                    Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb500-7"><a href="multinomial-and-ordinal-regression.html#cb500-7" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)          0.95      0.42     0.36     1.99</span></span>
<span id="cb500-8"><a href="multinomial-and-ordinal-regression.html#cb500-8" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(vtl)                0.62      0.43     0.03     1.68</span></span>
<span id="cb500-9"><a href="multinomial-and-ordinal-regression.html#cb500-9" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(f0)                 1.07      0.77     0.05     2.99</span></span>
<span id="cb500-10"><a href="multinomial-and-ordinal-regression.html#cb500-10" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(disc_Intercept)     0.42      0.11     0.25     0.67</span></span>
<span id="cb500-11"><a href="multinomial-and-ordinal-regression.html#cb500-11" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,vtl)    -0.19      0.36    -0.81     0.56</span></span>
<span id="cb500-12"><a href="multinomial-and-ordinal-regression.html#cb500-12" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,f0)      0.21      0.37    -0.58     0.82</span></span>
<span id="cb500-13"><a href="multinomial-and-ordinal-regression.html#cb500-13" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(vtl,f0)           -0.03      0.39    -0.76     0.72</span></span>
<span id="cb500-14"><a href="multinomial-and-ordinal-regression.html#cb500-14" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb500-15"><a href="multinomial-and-ordinal-regression.html#cb500-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ~S (Number of levels: 139)</span></span>
<span id="cb500-16"><a href="multinomial-and-ordinal-regression.html#cb500-16" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb500-17"><a href="multinomial-and-ordinal-regression.html#cb500-17" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)     2.43      0.85     1.11     4.33</span></span>
<span id="cb500-18"><a href="multinomial-and-ordinal-regression.html#cb500-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb500-19"><a href="multinomial-and-ordinal-regression.html#cb500-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects:</span></span>
<span id="cb500-20"><a href="multinomial-and-ordinal-regression.html#cb500-20" aria-hidden="true" tabindex="-1"></a><span class="do">##                Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb500-21"><a href="multinomial-and-ordinal-regression.html#cb500-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept[1]      -3.57      1.24    -6.36    -1.61</span></span>
<span id="cb500-22"><a href="multinomial-and-ordinal-regression.html#cb500-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept[2]       4.97      1.72     2.30     8.83</span></span>
<span id="cb500-23"><a href="multinomial-and-ordinal-regression.html#cb500-23" aria-hidden="true" tabindex="-1"></a><span class="do">## disc_Intercept    -0.52      0.35    -1.17     0.21</span></span>
<span id="cb500-24"><a href="multinomial-and-ordinal-regression.html#cb500-24" aria-hidden="true" tabindex="-1"></a><span class="do">## vtl                6.11      2.13     2.78    10.93</span></span>
<span id="cb500-25"><a href="multinomial-and-ordinal-regression.html#cb500-25" aria-hidden="true" tabindex="-1"></a><span class="do">## f0                -3.31      1.51    -6.95    -0.95</span></span></code></pre></div>
<p>We can get the listener-dependent disc terms using the <code>short_hypothesis</code> function. Since these values are the logarithm of the discrimination, we need to exponentiate and then invert the parameter estimate to get the scale. We can do this inside the hypothesis function so that we summarize after manipulating (rather than exponentiating and inverting our summary).</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="multinomial-and-ordinal-regression.html#cb501-1" aria-hidden="true" tabindex="-1"></a>listener_scale <span class="ot">=</span> </span>
<span id="cb501-2"><a href="multinomial-and-ordinal-regression.html#cb501-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">short_hypothesis</span> (model_ordinal_disc, </span>
<span id="cb501-3"><a href="multinomial-and-ordinal-regression.html#cb501-3" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;1/exp(disc_Intercept)=0&quot;</span>, <span class="at">group=</span><span class="st">&quot;L&quot;</span>,<span class="at">scope=</span><span class="st">&quot;coef&quot;</span>)</span>
<span id="cb501-4"><a href="multinomial-and-ordinal-regression.html#cb501-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-5"><a href="multinomial-and-ordinal-regression.html#cb501-5" aria-hidden="true" tabindex="-1"></a><span class="co"># omit hypothesis column so this fits in the book</span></span>
<span id="cb501-6"><a href="multinomial-and-ordinal-regression.html#cb501-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(listener_scale)[,<span class="sc">-</span><span class="dv">5</span>]</span>
<span id="cb501-7"><a href="multinomial-and-ordinal-regression.html#cb501-7" aria-hidden="true" tabindex="-1"></a><span class="do">##    Estimate Est.Error   Q2.5 Q97.5 group</span></span>
<span id="cb501-8"><a href="multinomial-and-ordinal-regression.html#cb501-8" aria-hidden="true" tabindex="-1"></a><span class="do">## H1    2.947    1.0599 1.3115 5.406     1</span></span>
<span id="cb501-9"><a href="multinomial-and-ordinal-regression.html#cb501-9" aria-hidden="true" tabindex="-1"></a><span class="do">## H2    2.157    0.7857 0.9580 4.006     2</span></span>
<span id="cb501-10"><a href="multinomial-and-ordinal-regression.html#cb501-10" aria-hidden="true" tabindex="-1"></a><span class="do">## H3    1.423    0.5277 0.6223 2.619     3</span></span>
<span id="cb501-11"><a href="multinomial-and-ordinal-regression.html#cb501-11" aria-hidden="true" tabindex="-1"></a><span class="do">## H4    2.730    1.0239 1.1936 5.062     4</span></span>
<span id="cb501-12"><a href="multinomial-and-ordinal-regression.html#cb501-12" aria-hidden="true" tabindex="-1"></a><span class="do">## H5    2.180    0.7832 0.9581 4.014     5</span></span>
<span id="cb501-13"><a href="multinomial-and-ordinal-regression.html#cb501-13" aria-hidden="true" tabindex="-1"></a><span class="do">## H6    2.229    0.7954 0.9919 4.038     6</span></span></code></pre></div>
<p>Below we compare the distributions implied by these <code>disc</code> parameters for a value of 0 for the latent variable. We can see that these distributions imply substantially different behaviors across listeners, given the same predicted value for the latent variable.</p>
<div class="figure"><span style="display:block;" id="fig:F12-13"></span>
<img src="_main_files/figure-html/F12-13-1.jpeg" alt="Listener-specific distributions of the latent variable given an expected value of zero. Numbers indicate the expected probability of observing each size group for this expected value, for each listener. Plot y axis ranges are determined relative to the height of each density and are not all equal." width="4800" />
<p class="caption">
Figure 12.13: Listener-specific distributions of the latent variable given an expected value of zero. Numbers indicate the expected probability of observing each size group for this expected value, for each listener. Plot y axis ranges are determined relative to the height of each density and are not all equal.
</p>
</div>
</div>
<div id="answering-our-research-questions-3" class="section level3 hasAnchor" number="12.3.6">
<h3><span class="header-section-number">12.3.6</span> Answering our research questions<a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our research question was very simple:</p>
<p>(Q1) Can we predict size group responses for a speaker given their f0 and VTL?</p>
<p>Yes we can, and with reasonable accuracy (83%) based only on two acoustic predictors. Below we find the predicted category for each trial based on our first model, including the random effects, and for the second ordinal model we fit that included listener-specific <code>disc</code> parameters.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="multinomial-and-ordinal-regression.html#cb502-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions with random effects</span></span>
<span id="cb502-2"><a href="multinomial-and-ordinal-regression.html#cb502-2" aria-hidden="true" tabindex="-1"></a>predictions_latent_full <span class="ot">=</span> <span class="fu">fitted</span> (model_ordinal)</span>
<span id="cb502-3"><a href="multinomial-and-ordinal-regression.html#cb502-3" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions with listener dependent disc</span></span>
<span id="cb502-4"><a href="multinomial-and-ordinal-regression.html#cb502-4" aria-hidden="true" tabindex="-1"></a>predictions_latent_disc <span class="ot">=</span> <span class="fu">fitted</span> (model_ordinal_disc)</span>
<span id="cb502-5"><a href="multinomial-and-ordinal-regression.html#cb502-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb502-6"><a href="multinomial-and-ordinal-regression.html#cb502-6" aria-hidden="true" tabindex="-1"></a><span class="co"># find highest predicted probability</span></span>
<span id="cb502-7"><a href="multinomial-and-ordinal-regression.html#cb502-7" aria-hidden="true" tabindex="-1"></a>SG_hat_full <span class="ot">=</span> <span class="fu">apply</span> (predictions_latent_full[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span>
<span id="cb502-8"><a href="multinomial-and-ordinal-regression.html#cb502-8" aria-hidden="true" tabindex="-1"></a>SG_hat_disc <span class="ot">=</span> <span class="fu">apply</span> (predictions_latent_disc[,<span class="dv">1</span>,],<span class="dv">1</span>,which.max)</span></code></pre></div>
<p>Below, we calculate the probability of a correct size group prediction for the fixed effects predictions, the predictions made by the full model with a single <code>disc</code> parameter, and the model with a listener-dependent <code>disc</code> parameter. We can see that the random effects noticeably improve prediction whereas the inclusion of listener-specific <code>disc</code> has a much smaller effect.</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="multinomial-and-ordinal-regression.html#cb503-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fixed effects</span></span>
<span id="cb503-2"><a href="multinomial-and-ordinal-regression.html#cb503-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG_hat <span class="sc">==</span> exp_data<span class="sc">$</span>SG)</span>
<span id="cb503-3"><a href="multinomial-and-ordinal-regression.html#cb503-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8264</span></span>
<span id="cb503-4"><a href="multinomial-and-ordinal-regression.html#cb503-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-5"><a href="multinomial-and-ordinal-regression.html#cb503-5" aria-hidden="true" tabindex="-1"></a><span class="co"># full model, single disc</span></span>
<span id="cb503-6"><a href="multinomial-and-ordinal-regression.html#cb503-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG_hat_full <span class="sc">==</span> exp_data<span class="sc">$</span>SG)</span>
<span id="cb503-7"><a href="multinomial-and-ordinal-regression.html#cb503-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8887</span></span>
<span id="cb503-8"><a href="multinomial-and-ordinal-regression.html#cb503-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-9"><a href="multinomial-and-ordinal-regression.html#cb503-9" aria-hidden="true" tabindex="-1"></a><span class="co"># full model, listener-dependent disc</span></span>
<span id="cb503-10"><a href="multinomial-and-ordinal-regression.html#cb503-10" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (SG_hat_disc <span class="sc">==</span> exp_data<span class="sc">$</span>SG)</span>
<span id="cb503-11"><a href="multinomial-and-ordinal-regression.html#cb503-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8911</span></span></code></pre></div>
<p>In any case, it seems clear that ordinal variables related to apparent talker size/height are likely to be very predictable based on speaker VTL and f0. However, we’re not going to dig into these results very much since this dependent variable was sort of ‘made up’, and our intention was mostly to provide practical examples of ordinal regression analyses that take advantage of the information in repeated measures data.</p>
</div>
</div>
<div id="exercises-11" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Exercises<a href="multinomial-and-ordinal-regression.html#exercises-11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The analyses in the main body of the text all involve only the unmodified ‘actual’ resonance level (in <code>exp_data</code>). Responses for the stimuli with the simulate ‘big’ resonance are reserved for exercises throughout. You can get the ‘big’ resonance in the <code>exp_ex</code> data frame, or all data in the <code>exp_data_all</code> data frame.</p>
<p>Fit and interpret one or more of the suggested models:</p>
<ol style="list-style-type: decimal">
<li><p>Easy: Analyze the (pre-fit) model that’s exactly like <code>model_multinomial</code>, except using the data in <code>exp_ex</code> (<code>bmmb::get_model("12_model_multinomial_ex.RDS")</code>).</p></li>
<li><p>Easy: Analyze the (pre-fit) model that’s exactly like <code>model_ordinal</code>, except using the data in <code>exp_ex</code> (<code>bmmb::get_model("12_model_ordinal_ex.RDS")</code>).</p></li>
<li><p>Medium: Model <code>SG</code> using a multinomial model and compare results, predictions, coefficient estimates, etc. with <code>model_multinomial</code>.</p></li>
<li><p>Hard: Expand on either of the models fit in this chapter, either by adding duration as a quantitative predictor, or by including resonance and its interaction with the other predictors.</p></li>
</ol>
<p>In any case, describe the model, present and explain the results, and include some figures.</p>
</div>
<div id="references-9" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> References<a href="multinomial-and-ordinal-regression.html#references-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agresti, A. (2018). An introduction to categorical data analysis. John Wiley &amp; Sons.</p>
<p>Agresti, A. (2003). Categorical data analysis. John Wiley &amp; Sons.</p>
<p>Kruschke, J. (2014). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
