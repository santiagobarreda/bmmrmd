<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Variation in parameters (‘random effects’) and model comparison | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</title>
  <meta name="description" content="Bayesian Models for Repeated Measures" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Variation in parameters (‘random effects’) and model comparison | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Bayesian Models for Repeated Measures" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Variation in parameters (‘random effects’) and model comparison | Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R" />
  
  <meta name="twitter:description" content="Bayesian Models for Repeated Measures" />
  

<meta name="author" content="Santiago Bareda and Noah Silbert" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="comparing-two-groups-of-observations-factors-and-contrasts.html"/>
<link rel="next" href="comparing-many-groups-interactions-and-posterior-predictive-checks.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Repeated Measures data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bayesian-multilevel-models-and-repeated-measures-data"><i class="fa fa-check"></i>Bayesian Multilevel models and repeated measures data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-missing-from-this-book"><i class="fa fa-check"></i>What’s missing from this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#statistics-as-procedural-knowledge"><i class="fa fa-check"></i>Statistics as Procedural knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practice-vs-brain-power"><i class="fa fa-check"></i>Practice vs brain power</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#supplemental-resources"><i class="fa fa-check"></i>Supplemental Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#our-target-audience"><i class="fa fa-check"></i>Our target audience</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-self-starter"><i class="fa fa-check"></i>The self-starter</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-convert"><i class="fa fa-check"></i>The convert</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-instructor"><i class="fa fa-check"></i>The instructor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-you-need-installed-to-use-this-book"><i class="fa fa-check"></i>What you need installed to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-go-bayesian"><i class="fa fa-check"></i>Why go Bayesian?</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-brms"><i class="fa fa-check"></i>Why brms?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#it-takes-a-village-of-books"><i class="fa fa-check"></i>It takes a village (of books)</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html"><i class="fa fa-check"></i><b>1</b> Introduction: Experiments and Variables</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#chapter-pre-cap"><i class="fa fa-check"></i><b>1.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-and-effects"><i class="fa fa-check"></i><b>1.2</b> Experiments and effects</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-and-inference"><i class="fa fa-check"></i><b>1.2.1</b> Experiments and inference</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp"><i class="fa fa-check"></i><b>1.3</b> Our experiment</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-intro"><i class="fa fa-check"></i><b>1.3.1</b> Our experiment: Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-methods"><i class="fa fa-check"></i><b>1.3.2</b> Our experimental methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-research-questions"><i class="fa fa-check"></i><b>1.3.3</b> Our research questions</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-exp-data"><i class="fa fa-check"></i><b>1.3.4</b> Our experimental data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-variables"><i class="fa fa-check"></i><b>1.4</b> Variables</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-pops-and-samps"><i class="fa fa-check"></i><b>1.4.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-dep-and-indep"><i class="fa fa-check"></i><b>1.4.2</b> Dependent and Independent Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-categorical"><i class="fa fa-check"></i><b>1.4.3</b> Categorical variables and ‘factors’</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-quantitative"><i class="fa fa-check"></i><b>1.4.4</b> Quantitative variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-logical"><i class="fa fa-check"></i><b>1.4.5</b> Logical variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting"><i class="fa fa-check"></i><b>1.5</b> Inspecting our data</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-categorical"><i class="fa fa-check"></i><b>1.5.1</b> Inspecting categorical variables</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-quantitative"><i class="fa fa-check"></i><b>1.5.2</b> Inspecting quantitative variables</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#c1-inspecting-together"><i class="fa fa-check"></i><b>1.5.3</b> Exploring continuous and categorical variables together</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
<li class="chapter" data-level="1.7" data-path="introduction-experiments-and-variables.html"><a href="introduction-experiments-and-variables.html#references"><i class="fa fa-check"></i><b>1.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html"><i class="fa fa-check"></i><b>2</b> Probabilities, likelihood, and inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#chapter-pre-cap-1"><i class="fa fa-check"></i><b>2.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="2.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-data"><i class="fa fa-check"></i><b>2.2</b> Data and research questions</a></li>
<li class="chapter" data-level="2.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-empirical-prob"><i class="fa fa-check"></i><b>2.3</b> Empirical Probabilities</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-conditional"><i class="fa fa-check"></i><b>2.3.1</b> Conditional and marginal probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-joint"><i class="fa fa-check"></i><b>2.3.2</b> Joint probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-theoretical"><i class="fa fa-check"></i><b>2.4</b> Probability distributions</a></li>
<li class="chapter" data-level="2.5" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-normal"><i class="fa fa-check"></i><b>2.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-sample-mean"><i class="fa fa-check"></i><b>2.5.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.5.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-sample-variance"><i class="fa fa-check"></i><b>2.5.2</b> The sample variance (or standard deviation)</a></li>
<li class="chapter" data-level="2.5.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-normal-density"><i class="fa fa-check"></i><b>2.5.3</b> The normal density</a></li>
<li class="chapter" data-level="2.5.4" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-standard-normal"><i class="fa fa-check"></i><b>2.5.4</b> The standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-models-and-inference"><i class="fa fa-check"></i><b>2.6</b> Models and inference</a></li>
<li class="chapter" data-level="2.7" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-likelihoods"><i class="fa fa-check"></i><b>2.7</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods"><i class="fa fa-check"></i><b>2.7.1</b> Characteristics of likelihoods</a></li>
<li class="chapter" data-level="2.7.2" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-logarithms"><i class="fa fa-check"></i><b>2.7.2</b> A brief aside on logarithms</a></li>
<li class="chapter" data-level="2.7.3" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods-2"><i class="fa fa-check"></i><b>2.7.3</b> Characteristics of likelihoods, continued</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#c2-inference-and-likelihood"><i class="fa fa-check"></i><b>2.8</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.9" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#exercises-1"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
<li class="chapter" data-level="2.10" data-path="probabilities-likelihood-and-inference.html"><a href="probabilities-likelihood-and-inference.html#references-1"><i class="fa fa-check"></i><b>2.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html"><i class="fa fa-check"></i><b>3</b> Fitting Bayesian regression models with <em>brms</em></a>
<ul>
<li class="chapter" data-level="3.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#chapter-pre-cap-2"><i class="fa fa-check"></i><b>3.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="3.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-what-is-reg"><i class="fa fa-check"></i><b>3.2</b> What are regression models?</a></li>
<li class="chapter" data-level="3.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-whats-bayes"><i class="fa fa-check"></i><b>3.3</b> What’s ‘Bayesian’ about these models?</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-priors"><i class="fa fa-check"></i><b>3.3.1</b> Prior probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-posterior"><i class="fa fa-check"></i><b>3.3.2</b> Posterior distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-characteristics-posteriors"><i class="fa fa-check"></i><b>3.3.3</b> Posterior distributions and shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-sampling"><i class="fa fa-check"></i><b>3.4</b> Sampling from the posterior using <em>Stan</em> and <em>brms</em></a></li>
<li class="chapter" data-level="3.5" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-estimating"><i class="fa fa-check"></i><b>3.5</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-data-qs-1"><i class="fa fa-check"></i><b>3.5.1</b> Data and Research Questions</a></li>
<li class="chapter" data-level="3.5.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-description-1"><i class="fa fa-check"></i><b>3.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.5.3" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-errors-and-residuals"><i class="fa fa-check"></i><b>3.5.3</b> Errors and residuals</a></li>
<li class="chapter" data-level="3.5.4" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-model-formula"><i class="fa fa-check"></i><b>3.5.4</b> The model formula</a></li>
<li class="chapter" data-level="3.5.5" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-calling-brm"><i class="fa fa-check"></i><b>3.5.5</b> Fitting the model: Calling the <em>brm</em> function</a></li>
<li class="chapter" data-level="3.5.6" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-interpreting-print"><i class="fa fa-check"></i><b>3.5.6</b> Interpreting the model: The print statement</a></li>
<li class="chapter" data-level="3.5.7" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-seeing-samples"><i class="fa fa-check"></i><b>3.5.7</b> Seeing the samples</a></li>
<li class="chapter" data-level="3.5.8" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-getting-residuals"><i class="fa fa-check"></i><b>3.5.8</b> Getting the residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-checking-convergence"><i class="fa fa-check"></i><b>3.6</b> Checking model convergence</a></li>
<li class="chapter" data-level="3.7" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-specifying-priors"><i class="fa fa-check"></i><b>3.7</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="3.8" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-log-posterior"><i class="fa fa-check"></i><b>3.8</b> The log prior and log posterior densities</a></li>
<li class="chapter" data-level="3.9" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-answering-qs"><i class="fa fa-check"></i><b>3.9</b> Answering our research questions</a></li>
<li class="chapter" data-level="3.10" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-frequentist"><i class="fa fa-check"></i><b>3.10</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-vs-ttest"><i class="fa fa-check"></i><b>3.10.1</b> One-sample t-test vs. intercept-only Bayesian models</a></li>
<li class="chapter" data-level="3.10.2" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#c3-vs-ols"><i class="fa fa-check"></i><b>3.10.2</b> Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="fitting-bayesian-regression-models-with-brms.html"><a href="fitting-bayesian-regression-models-with-brms.html#exercises-2"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><i class="fa fa-check"></i><b>4</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#chapter-pre-cap-3"><i class="fa fa-check"></i><b>4.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-multilevel"><i class="fa fa-check"></i><b>4.2</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-levels"><i class="fa fa-check"></i><b>4.2.1</b> Multilevel models and ‘levels’ of variation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-many-levels"><i class="fa fa-check"></i><b>4.3</b> Representing predictors with many levels</a></li>
<li class="chapter" data-level="4.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-strategies"><i class="fa fa-check"></i><b>4.4</b> Strategies for estimating factors with many levels</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-complete-pooling"><i class="fa fa-check"></i><b>4.4.1</b> Complete pooling</a></li>
<li class="chapter" data-level="4.4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-no-pooling"><i class="fa fa-check"></i><b>4.4.2</b> No pooling</a></li>
<li class="chapter" data-level="4.4.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-partial-pooling"><i class="fa fa-check"></i><b>4.4.3</b> (Adaptive) Partial pooling</a></li>
<li class="chapter" data-level="4.4.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#hyperpriors"><i class="fa fa-check"></i><b>4.4.4</b> Hyperpriors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-estimating1"><i class="fa fa-check"></i><b>4.5</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-data-and-qs-1"><i class="fa fa-check"></i><b>4.5.1</b> Data and Research questions</a></li>
<li class="chapter" data-level="4.5.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model"><i class="fa fa-check"></i><b>4.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="4.5.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-fitting-1"><i class="fa fa-check"></i><b>4.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="4.5.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#interpreting-the-model"><i class="fa fa-check"></i><b>4.5.4</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-random-effects"><i class="fa fa-check"></i><b>4.6</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-inspecting-random-effects"><i class="fa fa-check"></i><b>4.6.1</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-simulating"><i class="fa fa-check"></i><b>4.7</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="4.8" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-second-random-effect"><i class="fa fa-check"></i><b>4.8</b> Adding a second random effect</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-updating-model"><i class="fa fa-check"></i><b>4.8.1</b> Updating the model description</a></li>
<li class="chapter" data-level="4.8.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-and-interpreting-the-model"><i class="fa fa-check"></i><b>4.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-investigating-shrinkage"><i class="fa fa-check"></i><b>4.9</b> Investigating ‘shrinkage’</a></li>
<li class="chapter" data-level="4.10" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-answering-question"><i class="fa fa-check"></i><b>4.10</b> Answering our research questions</a></li>
<li class="chapter" data-level="4.11" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-frequentist"><i class="fa fa-check"></i><b>4.11</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-vs-lmer"><i class="fa fa-check"></i><b>4.11.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#exercises-3"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
<li class="chapter" data-level="4.13" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#references-2"><i class="fa fa-check"></i><b>4.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html"><i class="fa fa-check"></i><b>5</b> Comparing two groups of observations: Factors and contrasts</a>
<ul>
<li class="chapter" data-level="5.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#chapter-pre-cap-4"><i class="fa fa-check"></i><b>5.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="5.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#comparing-two-groups"><i class="fa fa-check"></i><b>5.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="5.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#distribution-of-repeated-measures-across-factor-levels"><i class="fa fa-check"></i><b>5.3</b> Distribution of repeated measures across factor levels</a></li>
<li class="chapter" data-level="5.4" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-data-and-qs"><i class="fa fa-check"></i><b>5.4</b> Data and research questions</a></li>
<li class="chapter" data-level="5.5" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-two-means"><i class="fa fa-check"></i><b>5.5</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-the-model"><i class="fa fa-check"></i><b>5.5.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>5.5.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-contrasts"><i class="fa fa-check"></i><b>5.6</b> Contrasts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-treatment-coding"><i class="fa fa-check"></i><b>5.6.1</b> Treatment coding</a></li>
<li class="chapter" data-level="5.6.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-sum-coding"><i class="fa fa-check"></i><b>5.6.2</b> Sum coding</a></li>
<li class="chapter" data-level="5.6.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-comparison-sum-treatment"><i class="fa fa-check"></i><b>5.6.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-refittin-sum"><i class="fa fa-check"></i><b>5.7</b> Sum coding and the decomposition of variation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-description-1"><i class="fa fa-check"></i><b>5.7.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.7.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-the-model-1"><i class="fa fa-check"></i><b>5.7.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.7.3" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>5.7.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-working-with-posteriors"><i class="fa fa-check"></i><b>5.8</b> Inspecting and manipulating the posterior samples</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-using-hypothesis"><i class="fa fa-check"></i><b>5.8.1</b> Using the <em>hypothesis</em> function</a></li>
<li class="chapter" data-level="5.8.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-manipulating-random-effects"><i class="fa fa-check"></i><b>5.8.2</b> Working with the random effects</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-robustness"><i class="fa fa-check"></i><b>5.9</b> Making our models more robust: The (non-standardized) t distribution</a></li>
<li class="chapter" data-level="5.10" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#re-fitting-with-t-distributed-errors."><i class="fa fa-check"></i><b>5.10</b> Re-fitting with t-distributed errors</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#description-of-the-model-1"><i class="fa fa-check"></i><b>5.10.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.10.2" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fitting-and-interpreting-the-model-1"><i class="fa fa-check"></i><b>5.10.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-simulating"><i class="fa fa-check"></i><b>5.11</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="5.12" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-answering-qs"><i class="fa fa-check"></i><b>5.12</b> Answering our research questions</a></li>
<li class="chapter" data-level="5.13" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-frequentist"><i class="fa fa-check"></i><b>5.13</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#bayesian-multilevel-models-vs.-lmer"><i class="fa fa-check"></i><b>5.13.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="comparing-two-groups-of-observations-factors-and-contrasts.html"><a href="comparing-two-groups-of-observations-factors-and-contrasts.html#exercises-4"><i class="fa fa-check"></i><b>5.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html"><i class="fa fa-check"></i><b>6</b> Variation in parameters (‘random effects’) and model comparison</a>
<ul>
<li class="chapter" data-level="6.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#chapter-pre-cap-5"><i class="fa fa-check"></i><b>6.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="6.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-data-and-qs"><i class="fa fa-check"></i><b>6.2</b> Data and research questions</a></li>
<li class="chapter" data-level="6.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-variation-sources"><i class="fa fa-check"></i><b>6.3</b> Variation in parameters across sources of data</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#description-of-our-model"><i class="fa fa-check"></i><b>6.3.1</b> Description of our model</a></li>
<li class="chapter" data-level="6.3.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-correlations"><i class="fa fa-check"></i><b>6.3.2</b> Correlations between random parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-random-and-mvn"><i class="fa fa-check"></i><b>6.3.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-mvn-priors"><i class="fa fa-check"></i><b>6.3.4</b> Specifying priors for a multivariate normal distribution</a></li>
<li class="chapter" data-level="6.3.5" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#updating-our-model-description"><i class="fa fa-check"></i><b>6.3.5</b> Updating our model description</a></li>
<li class="chapter" data-level="6.3.6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-fitting"><i class="fa fa-check"></i><b>6.3.6</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-model-comparison"><i class="fa fa-check"></i><b>6.4</b> Model Comparison</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-in-and-out-prediction"><i class="fa fa-check"></i><b>6.4.1</b> In-sample and out-of-sample prediction</a></li>
<li class="chapter" data-level="6.4.2" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-adjust"><i class="fa fa-check"></i><b>6.4.2</b> Out-of-sample prediction: Adjusting predictive accuracy</a></li>
<li class="chapter" data-level="6.4.3" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-crossval"><i class="fa fa-check"></i><b>6.4.3</b> Out-of-sample prediction: Cross validation</a></li>
<li class="chapter" data-level="6.4.4" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#selecting-a-model"><i class="fa fa-check"></i><b>6.4.4</b> Selecting a model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-answering"><i class="fa fa-check"></i><b>6.5</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.6" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-frequentist"><i class="fa fa-check"></i><b>6.6</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-vs-lmer"><i class="fa fa-check"></i><b>6.6.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="variation-in-parameters-random-effects-and-model-comparison.html"><a href="variation-in-parameters-random-effects-and-model-comparison.html#references-3"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><i class="fa fa-check"></i><b>7</b> Comparing many groups, interactions, and posterior predictive checks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#chapter-pre-cap-6"><i class="fa fa-check"></i><b>7.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="7.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions"><i class="fa fa-check"></i><b>7.2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.2.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-description-1"><i class="fa fa-check"></i><b>7.2.2</b> Description of our model</a></li>
<li class="chapter" data-level="7.2.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-2"><i class="fa fa-check"></i><b>7.2.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-multiple-factors-simultaneously"><i class="fa fa-check"></i><b>7.3</b> Investigating multiple factors simultaneously</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>7.3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.3.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-the-model-2"><i class="fa fa-check"></i><b>7.3.2</b> Description of the model</a></li>
<li class="chapter" data-level="7.3.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-3"><i class="fa fa-check"></i><b>7.3.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-posterior-prediction"><i class="fa fa-check"></i><b>7.4</b> Posterior prediction: Using our models to predict new data</a></li>
<li class="chapter" data-level="7.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-interactions-and-plots"><i class="fa fa-check"></i><b>7.5</b> Interactions and interaction plots</a></li>
<li class="chapter" data-level="7.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>7.6</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>7.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="7.6.2" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#model-formulas"><i class="fa fa-check"></i><b>7.6.2</b> Model formulas</a></li>
<li class="chapter" data-level="7.6.3" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#description-of-our-model-1"><i class="fa fa-check"></i><b>7.6.3</b> Description of our model</a></li>
<li class="chapter" data-level="7.6.4" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#fitting-and-interpreting-the-model-4"><i class="fa fa-check"></i><b>7.6.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="7.6.5" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-calc-means"><i class="fa fa-check"></i><b>7.6.5</b> Caulculating group means in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.6" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#calculating-simple-effects-in-the-presence-of-interactions"><i class="fa fa-check"></i><b>7.6.6</b> Calculating simple effects in the presence of interactions</a></li>
<li class="chapter" data-level="7.6.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#assessing-model-fit-bayesian-r2"><i class="fa fa-check"></i><b>7.6.7</b> Assessing model fit: Bayesian <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-answering"><i class="fa fa-check"></i><b>7.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="7.8" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.8</b> Factors with more than two levels</a></li>
<li class="chapter" data-level="7.9" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#c7-frequentist"><i class="fa fa-check"></i><b>7.9</b> ‘Traditionalists’ corner</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#bayesian-multilevel-models-vs.-lmer-1"><i class="fa fa-check"></i><b>7.9.1</b> Bayesian multilevel models vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#exercises-6"><i class="fa fa-check"></i><b>7.10</b> Exercises</a></li>
<li class="chapter" data-level="7.11" data-path="comparing-many-groups-interactions-and-posterior-predictive-checks.html"><a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html#references-4"><i class="fa fa-check"></i><b>7.11</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html"><i class="fa fa-check"></i><b>8</b> Varying variances, more about priors, and prior predictive checks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#chapter-pre-cap-7"><i class="fa fa-check"></i><b>8.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>8.2</b> Data and Research questions</a></li>
<li class="chapter" data-level="8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-about-priors"><i class="fa fa-check"></i><b>8.3</b> More about priors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-prior-prediction"><i class="fa fa-check"></i><b>8.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.3.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#more-specific-priors"><i class="fa fa-check"></i><b>8.3.2</b> More specific priors</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#heteroskedasticity-and-distributional-or-mixture-models"><i class="fa fa-check"></i><b>8.4</b> Heteroskedasticity and distributional (or mixture) models</a></li>
<li class="chapter" data-level="8.5" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-simple-model-error-varies-according-to-a-single-fixed-effect"><i class="fa fa-check"></i><b>8.5</b> A ‘simple’ model: Error varies according to a single fixed effect</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#description-of-our-model-2"><i class="fa fa-check"></i><b>8.5.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.5.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#prior-predictive-checks"><i class="fa fa-check"></i><b>8.5.2</b> Prior predictive checks</a></li>
<li class="chapter" data-level="8.5.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-5"><i class="fa fa-check"></i><b>8.5.3</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#a-complex-model-error-varies-according-to-fixed-and-random-effects"><i class="fa fa-check"></i><b>8.6</b> A ‘complex’ model: Error varies according to fixed and random effects</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-description-2"><i class="fa fa-check"></i><b>8.6.1</b> Description of our model</a></li>
<li class="chapter" data-level="8.6.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#fitting-and-interpreting-the-model-6"><i class="fa fa-check"></i><b>8.6.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#answering-our-research-questions"><i class="fa fa-check"></i><b>8.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="8.8" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#c8-identifiability"><i class="fa fa-check"></i><b>8.8</b> Building identifiable and supportable models</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#collinearity"><i class="fa fa-check"></i><b>8.8.1</b> Collinearity</a></li>
<li class="chapter" data-level="8.8.2" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#predictable-values-of-categorical-predictors"><i class="fa fa-check"></i><b>8.8.2</b> Predictable values of categorical predictors</a></li>
<li class="chapter" data-level="8.8.3" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#saturated-and-nearly-saturated-models"><i class="fa fa-check"></i><b>8.8.3</b> Saturated, and ‘nearly-saturated’, models</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#exercises-7"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
<li class="chapter" data-level="8.10" data-path="varying-variances-more-about-priors-and-prior-predictive-checks.html"><a href="varying-variances-more-about-priors-and-prior-predictive-checks.html#references-5"><i class="fa fa-check"></i><b>8.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html"><i class="fa fa-check"></i><b>9</b> Quantitative predictors and their interactions with factors</a>
<ul>
<li class="chapter" data-level="9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#chapter-pre-cap-8"><i class="fa fa-check"></i><b>9.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>9.2</b> Data and research questions</a></li>
<li class="chapter" data-level="9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#modeling-variation-along-lines"><i class="fa fa-check"></i><b>9.3</b> Modeling variation along lines</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-3"><i class="fa fa-check"></i><b>9.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.3.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-centering"><i class="fa fa-check"></i><b>9.3.2</b> Centering quantitative predictors</a></li>
<li class="chapter" data-level="9.3.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-an-interpreting-the-model"><i class="fa fa-check"></i><b>9.3.3</b> Fitting an interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>9.4</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-4"><i class="fa fa-check"></i><b>9.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.4.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-7"><i class="fa fa-check"></i><b>9.4.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.4.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-shared-non-zero-slopes"><i class="fa fa-check"></i><b>9.4.3</b> Interpreting group effects in the presence of shared (non-zero) slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>9.5</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-5"><i class="fa fa-check"></i><b>9.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.5.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-3"><i class="fa fa-check"></i><b>9.5.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.5.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#interpreting-group-effects-in-the-presence-of-varying-slopes"><i class="fa fa-check"></i><b>9.5.3</b> Interpreting group effects in the presence of varying slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-interim-discussion"><i class="fa fa-check"></i><b>9.6</b> Answering our research questions: Interim discussion</a></li>
<li class="chapter" data-level="9.7" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#data-and-research-questions-updated"><i class="fa fa-check"></i><b>9.7</b> Data and research questions: Updated</a></li>
<li class="chapter" data-level="9.8" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-intercepts-and-slopes-for-each-level-of-a-grouping-factor-i.e.-random-slopes"><i class="fa fa-check"></i><b>9.8</b> Models with intercepts and slopes for each level of a grouping factor (i.e. ‘random slopes’)</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-6"><i class="fa fa-check"></i><b>9.8.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.8.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#fitting-and-interpreting-the-model-8"><i class="fa fa-check"></i><b>9.8.2</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#models-with-multiple-predictors-for-each-level-of-a-grouping-factor"><i class="fa fa-check"></i><b>9.9</b> Models with multiple predictors for each level of a grouping factor</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#description-of-the-model-7"><i class="fa fa-check"></i><b>9.9.1</b> Description of the model</a></li>
<li class="chapter" data-level="9.9.2" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#c9-fitting-5"><i class="fa fa-check"></i><b>9.9.2</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="9.9.3" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#model-selection"><i class="fa fa-check"></i><b>9.9.3</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#answering-our-research-questions-updated"><i class="fa fa-check"></i><b>9.10</b> Answering our research questions: Updated</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#a-word-on-causality"><i class="fa fa-check"></i><b>9.10.1</b> A word on causality</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#exercises-8"><i class="fa fa-check"></i><b>9.11</b> Exercises</a></li>
<li class="chapter" data-level="9.12" data-path="quantitative-predictors-and-their-interactions-with-factors.html"><a href="quantitative-predictors-and-their-interactions-with-factors.html#references-6"><i class="fa fa-check"></i><b>9.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html"><i class="fa fa-check"></i><b>10</b> Logistic regression and signal detection theory models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#chapter-pre-cap-9"><i class="fa fa-check"></i><b>10.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-dichotomous"><i class="fa fa-check"></i><b>10.2</b> Dichotomous variables and data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#generalizing-our-linear-models"><i class="fa fa-check"></i><b>10.3</b> Generalizing our linear models</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logits"><i class="fa fa-check"></i><b>10.4.1</b> Logits</a></li>
<li class="chapter" data-level="10.4.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-inverse-logit"><i class="fa fa-check"></i><b>10.4.2</b> The inverse logit link function</a></li>
<li class="chapter" data-level="10.4.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#building-intuitions-about-logits-and-the-inverse-logit-function"><i class="fa fa-check"></i><b>10.4.3</b> Building intuitions about logits and the inverse logit function</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#logistic-regression-with-one-quantitative-predictor"><i class="fa fa-check"></i><b>10.5</b> Logistic regression with one quantitative predictor</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>10.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.5.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-8"><i class="fa fa-check"></i><b>10.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.5.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-0"><i class="fa fa-check"></i><b>10.5.3</b> Fitting the model</a></li>
<li class="chapter" data-level="10.5.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-fitting-1"><i class="fa fa-check"></i><b>10.5.4</b> Interpreting the model</a></li>
<li class="chapter" data-level="10.5.5" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#c10-classification"><i class="fa fa-check"></i><b>10.5.5</b> Using logistic models to understand classification</a></li>
<li class="chapter" data-level="10.5.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-question"><i class="fa fa-check"></i><b>10.5.6</b> Answering our research question</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#measuring-sensitivity-and-bias"><i class="fa fa-check"></i><b>10.6</b> Measuring sensitivity and bias</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#data-and-research-questions-6"><i class="fa fa-check"></i><b>10.6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="10.6.2" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#description-of-the-model-9"><i class="fa fa-check"></i><b>10.6.2</b> Description of the model</a></li>
<li class="chapter" data-level="10.6.3" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#fitting-and-interpreting-the-model-9"><i class="fa fa-check"></i><b>10.6.3</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="10.6.4" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>10.6.4</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#exercises-9"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
<li class="chapter" data-level="10.8" data-path="logistic-regression-and-signal-detection-theory-models.html"><a href="logistic-regression-and-signal-detection-theory-models.html#references-7"><i class="fa fa-check"></i><b>10.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><i class="fa fa-check"></i><b>11</b> Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#chapter-pre-cap-10"><i class="fa fa-check"></i><b>11.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="11.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#models-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.2</b> Models with multiple quantitative predictors</a></li>
<li class="chapter" data-level="11.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#interactions-between-quantitative-predictors"><i class="fa fa-check"></i><b>11.3</b> Interactions between quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#centering-quantitative-predictors-when-including-interactions"><i class="fa fa-check"></i><b>11.3.1</b> Centering quantitative predictors when including interactions</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-7"><i class="fa fa-check"></i><b>11.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="11.3.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-description-1"><i class="fa fa-check"></i><b>11.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="11.3.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-the-model-2"><i class="fa fa-check"></i><b>11.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="11.3.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#advantages-of-bayesian-multilevel-models-for-large-models"><i class="fa fa-check"></i><b>11.3.5</b> Advantages of Bayesian multilevel models for large models</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c11-BANOVA"><i class="fa fa-check"></i><b>11.4</b> Bayesian Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#getting-the-standard-deviations-from-our-models-manually"><i class="fa fa-check"></i><b>11.4.1</b> Getting the standard deviations from our models ‘manually’</a></li>
<li class="chapter" data-level="11.4.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#using-the-banova-function"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>banova</code> function</a></li>
<li class="chapter" data-level="11.4.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-comparing-the-reduced-model"><i class="fa fa-check"></i><b>11.4.3</b> Fitting and comparing the reduced model</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#a-logistic-regression-model-with-multiple-quantitative-predictors"><i class="fa fa-check"></i><b>11.5</b> A logistic regression model with multiple quantitative predictors</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#data-and-research-questions-8"><i class="fa fa-check"></i><b>11.5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="11.5.2" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#description-of-the-model-10"><i class="fa fa-check"></i><b>11.5.2</b> Description of the model</a></li>
<li class="chapter" data-level="11.5.3" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#fitting-and-the-model-and-applying-a-bayesian-anova"><i class="fa fa-check"></i><b>11.5.3</b> Fitting and the model and applying a Bayesian ANOVA</a></li>
<li class="chapter" data-level="11.5.4" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#c12-2d-categorization"><i class="fa fa-check"></i><b>11.5.4</b> Categorization in two dimensions</a></li>
<li class="chapter" data-level="11.5.5" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#model-selection-and-misspecification"><i class="fa fa-check"></i><b>11.5.5</b> Model selection and misspecification</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#exercises-10"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
<li class="chapter" data-level="11.7" data-path="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html"><a href="multiple-quantitative-predictors-dealing-with-large-models-and-bayesian-anova.html#references-8"><i class="fa fa-check"></i><b>11.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial and Ordinal regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#chapter-pre-cap-11"><i class="fa fa-check"></i><b>12.1</b> Chapter pre-cap</a></li>
<li class="chapter" data-level="12.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#multinomial-logits-and-the-softmax-function"><i class="fa fa-check"></i><b>12.2.1</b> Multinomial logits and the softmax function</a></li>
<li class="chapter" data-level="12.2.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#comparison-to-logistic-regression"><i class="fa fa-check"></i><b>12.2.2</b> Comparison to logistic regression</a></li>
<li class="chapter" data-level="12.2.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-9"><i class="fa fa-check"></i><b>12.2.3</b> Data and research questions</a></li>
<li class="chapter" data-level="12.2.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-our-model-3"><i class="fa fa-check"></i><b>12.2.4</b> Description of our model</a></li>
<li class="chapter" data-level="12.2.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-the-model-3"><i class="fa fa-check"></i><b>12.2.5</b> Fitting the model</a></li>
<li class="chapter" data-level="12.2.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>12.2.6</b> Interpreting the model</a></li>
<li class="chapter" data-level="12.2.7" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-multinomial-territorial-maps"><i class="fa fa-check"></i><b>12.2.7</b> Multinomial models and territorial maps</a></li>
<li class="chapter" data-level="12.2.8" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#refitting-the-model-without-speaker-random-effects"><i class="fa fa-check"></i><b>12.2.8</b> Refitting the model without speaker random effects</a></li>
<li class="chapter" data-level="12.2.9" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-2"><i class="fa fa-check"></i><b>12.2.9</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Ordinal (logistic) regression</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#c12-cumulative-density"><i class="fa fa-check"></i><b>12.3.1</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="12.3.2" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#data-and-research-questions-10"><i class="fa fa-check"></i><b>12.3.2</b> Data and research questions</a></li>
<li class="chapter" data-level="12.3.3" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#description-of-the-model-11"><i class="fa fa-check"></i><b>12.3.3</b> Description of the model</a></li>
<li class="chapter" data-level="12.3.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#fitting-and-interpreting-the-model-10"><i class="fa fa-check"></i><b>12.3.4</b> Fitting and interpreting the model</a></li>
<li class="chapter" data-level="12.3.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#listener-specific-discrimination-terms"><i class="fa fa-check"></i><b>12.3.5</b> Listener-specific discrimination terms</a></li>
<li class="chapter" data-level="12.3.6" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#answering-our-research-questions-3"><i class="fa fa-check"></i><b>12.3.6</b> Answering our research questions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#exercises-11"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
<li class="chapter" data-level="12.5" data-path="multinomial-and-ordinal-regression.html"><a href="multinomial-and-ordinal-regression.html#references-9"><i class="fa fa-check"></i><b>12.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><i class="fa fa-check"></i><b>13</b> Writing up experiments: An investigation of the perception of apparent speaker characteristics from speech acoustics</a>
<ul>
<li class="chapter" data-level="13.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#fundamental-frequency-and-voice-pitch"><i class="fa fa-check"></i><b>13.1.1</b> Fundamental frequency and voice pitch</a></li>
<li class="chapter" data-level="13.1.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-fundamental-frequency-between-speakers"><i class="fa fa-check"></i><b>13.1.2</b> Variation in fundamental frequency between speakers</a></li>
<li class="chapter" data-level="13.1.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#voice-resonance-and-vocal-tract-length"><i class="fa fa-check"></i><b>13.1.3</b> Voice resonance and vocal-tract length</a></li>
<li class="chapter" data-level="13.1.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-estimating-vtl"><i class="fa fa-check"></i><b>13.1.4</b> Estimating vocal-tracts length from speech</a></li>
<li class="chapter" data-level="13.1.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#variation-in-vocal-tract-length-between-speakers"><i class="fa fa-check"></i><b>13.1.5</b> Variation in vocal-tract length between speakers</a></li>
<li class="chapter" data-level="13.1.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-perception-of-chars"><i class="fa fa-check"></i><b>13.1.6</b> Perception of age, gender and size</a></li>
<li class="chapter" data-level="13.1.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#category-dependent-behavior"><i class="fa fa-check"></i><b>13.1.7</b> Category-dependent behavior</a></li>
<li class="chapter" data-level="13.1.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-current-experiment"><i class="fa fa-check"></i><b>13.1.8</b> The current experiment</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#methods"><i class="fa fa-check"></i><b>13.2</b> Methods</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#participants"><i class="fa fa-check"></i><b>13.2.1</b> Participants</a></li>
<li class="chapter" data-level="13.2.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#c13-stimuli"><i class="fa fa-check"></i><b>13.2.2</b> Stimuli</a></li>
<li class="chapter" data-level="13.2.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#procedure"><i class="fa fa-check"></i><b>13.2.3</b> Procedure</a></li>
<li class="chapter" data-level="13.2.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#data-screening"><i class="fa fa-check"></i><b>13.2.4</b> Data screening</a></li>
<li class="chapter" data-level="13.2.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#loading-the-data-and-packages"><i class="fa fa-check"></i><b>13.2.5</b> Loading the data and packages</a></li>
<li class="chapter" data-level="13.2.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-height"><i class="fa fa-check"></i><b>13.2.6</b> Statistical Analysis: Apparent height</a></li>
<li class="chapter" data-level="13.2.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#statistical-analysis-apparent-gender"><i class="fa fa-check"></i><b>13.2.7</b> Statistical Analysis: Apparent gender</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-height-judgments"><i class="fa fa-check"></i><b>13.3</b> Results: Apparent height judgments</a></li>
<li class="chapter" data-level="13.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-height"><i class="fa fa-check"></i><b>13.4</b> Discussion: Apparent height</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#age-dependent-use-of-vtl-cues-on-apparent-height"><i class="fa fa-check"></i><b>13.4.1</b> Age-dependent use of VTL cues on apparent height</a></li>
<li class="chapter" data-level="13.4.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#the-effect-for-apparent-gender-on-apparent-height"><i class="fa fa-check"></i><b>13.4.2</b> The effect for apparent gender on apparent height</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-height-judgments"><i class="fa fa-check"></i><b>13.5</b> Conclusion: Apparent height judgments</a></li>
<li class="chapter" data-level="13.6" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#results-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.6</b> Results: Apparent gender judgments</a></li>
<li class="chapter" data-level="13.7" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#discussion-apparent-gender-judgments"><i class="fa fa-check"></i><b>13.7</b> Discussion: Apparent gender judgments</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#effect-of-apparent-age-on-the-perception-of-femaleness"><i class="fa fa-check"></i><b>13.7.1</b> Effect of apparent age on the perception of femaleness</a></li>
<li class="chapter" data-level="13.7.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#between-listener-variation-in-gender-perception"><i class="fa fa-check"></i><b>13.7.2</b> Between-listener variation in gender perception</a></li>
<li class="chapter" data-level="13.7.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#beyond-gross-acoustic-cues-in-gender-perception"><i class="fa fa-check"></i><b>13.7.3</b> Beyond gross acoustic cues in gender perception</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#conclusion-apparent-gender"><i class="fa fa-check"></i><b>13.8</b> Conclusion: Apparent gender</a></li>
<li class="chapter" data-level="13.9" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#next-steps"><i class="fa fa-check"></i><b>13.9</b> Next steps</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#research-design-variable-selection-etc."><i class="fa fa-check"></i><b>13.9.1</b> Research design, variable selection, etc.</a></li>
<li class="chapter" data-level="13.9.2" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#non-linear-models"><i class="fa fa-check"></i><b>13.9.2</b> Non-linear models</a></li>
<li class="chapter" data-level="13.9.3" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#other-data-distributions"><i class="fa fa-check"></i><b>13.9.3</b> Other data distributions</a></li>
<li class="chapter" data-level="13.9.4" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#multivariate-analyses"><i class="fa fa-check"></i><b>13.9.4</b> Multivariate analyses</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html"><a href="writing-up-experiments-an-investigation-of-the-perception-of-apparent-speaker-characteristics-from-speech-acoustics.html#references-10"><i class="fa fa-check"></i><b>13.10</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/santiagobarreda/bmmrmd" target="blank">Book GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variation-in-parameters-random-effects-and-model-comparison" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Variation in parameters (‘random effects’) and model comparison<a href="variation-in-parameters-random-effects-and-model-comparison.html#variation-in-parameters-random-effects-and-model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In chapter 4 we introduced ‘random effects’. These were intercept terms that were specific to ‘sources’ of data (e.g., listener, speaker), and fit using adaptive partial pooling. We used speaker and listener-dependent intercepts because we know that each listener, and each speaker, in our data can be associated with unpredictably different average apparent heights. In chapter 5 we built models that compared two groups of measurements by including a single predictor that distinguished the groups. However, our models assumed that the difference between the groups, the effect of the single predictor, was fixed across all listeners. In this chapter we will build models that allow for listener-specific variation in fixed effects predictors in addition to the intercept. Then, we will discuss how to evaluate whether the increasing complexity of our model is making our model ‘better’, and whether it’s justified by our data.</p>
<div id="chapter-pre-cap-5" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Chapter pre-cap<a href="variation-in-parameters-random-effects-and-model-comparison.html#chapter-pre-cap-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we introduce interactions as conditional effects, and marginal (average) effects are introduced as ‘main’ effects. The concepts of crossing and nesting are presented and the importance of crossing for the estimation of interactions is discussed. We then outline models with multiple group-dependent parameters (‘random effects’), and the interpretation of these as interactions. We introduce the multivariate normal distribution, in addition to the concepts of multidimensionality and correlation. The fact that random effects are drawn from multivariate normal distributions is discussed, as are related topics for multilevel models (e.g. the specification of priors for correlation matrices). After that, the chapter focuses on model comparison. We contrast in sample and out-of-sample prediction, and discuss the bias-variance tradeoff. Finally, we present the widely applicable information criterion (WAIC) and leave-one-out cross validation, and discuss model selection.</p>
</div>
<div id="c6-data-and-qs" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Data and research questions<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-data-and-qs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’re going to use the same data from the last chapter, again excluding adult males and focusing only on the apparent height of adult females, girls and boys. We’re going to build models that expand on those of last chapter to ask the same questions:</p>
<p>(Q1) How tall do speakers perceived as adult females sound?</p>
<p>(Q2) How tall do speakers perceived as children sound?</p>
<p>(Q3) What is the difference in apparent height associated with the perception of adultness?</p>
<p>Below we load the data and exclude all apparent and veridical adult males.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages and data</span></span>
<span id="cb183-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (bmmb)</span>
<span id="cb183-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb183-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span> (exp_data)</span>
<span id="cb183-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-6" aria-hidden="true" tabindex="-1"></a><span class="co"># exclude actual men and apparent men</span></span>
<span id="cb183-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb183-7" aria-hidden="true" tabindex="-1"></a>notmen <span class="ot">=</span> exp_data[exp_data<span class="sc">$</span>C_v<span class="sc">!=</span><span class="st">&#39;m&#39;</span> <span class="sc">&amp;</span> exp_data<span class="sc">$</span>C<span class="sc">!=</span><span class="st">&#39;m&#39;</span>,]</span></code></pre></div>
<p>The relevant variables in our data frame are:</p>
<ul>
<li><code>L</code>: An integer from 1-15 indicating which <em>listener</em> responded to the trial.</li>
<li><code>height</code>: A floating-point number representing the <em>height</em> (in centimeters) reported for the speaker on each trial.</li>
<li><code>S</code>: An integer from 1-139 indicating which <em>speaker</em> produced the trial stimulus.</li>
<li><code>A</code>: The <em>apparent age</em> of the speaker indicated by the listener, <code>a</code> (adult) or <code>c</code> (child).</li>
</ul>
<p>We recreate figure <a href="comparing-two-groups-of-observations-factors-and-contrasts.html#fig:F5-9">5.9</a> as figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-1">6.1</a> below to show the distribution of apparent height by listener, and across apparent ages. In the last chapter we focused on ‘the difference’ between height judgments for apparent adults and apparent children as a singular thing. This is a bit like focusing only on the difference between the boxes in the left plot of figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-1">6.1</a>. However, if we instead focus on the right plot in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-1">6.1</a> we see that there is not a <em>single</em> difference in apparent heights between apparent children and adults, but rather a <em>set</em> of listener-specific differences. The model we will introduce in this chapter will be able to represent this information.</p>
<div class="figure"><span style="display:block;" id="fig:F6-1"></span>
<img src="_main_files/figure-html/F6-1-1.jpeg" alt="(left) Distribution of apparent heights according to apparent age group. (right) Same as left plot but presented individually for each listener. In each case, the first box of each color (the upper box) indicates responses for apparent adults. The horizontal lines running through the figures represent the grand mean (black), the adult mean (blue), and the child mean (green)." width="4800" />
<p class="caption">
Figure 6.1: (left) Distribution of apparent heights according to apparent age group. (right) Same as left plot but presented individually for each listener. In each case, the first box of each color (the upper box) indicates responses for apparent adults. The horizontal lines running through the figures represent the grand mean (black), the adult mean (blue), and the child mean (green).
</p>
</div>
</div>
<div id="c6-variation-sources" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Variation in parameters across sources of data<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-variation-sources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The models we fit in chapter 5 included a predictor representing the apparent age of the speaker. However, this was only a <code>population-level parameter</code> (i.e. a ‘fixed’ effect) meaning that it had the same value for all listeners. Another way to think about this is that our model included only the <em>marginal</em> effect for apparent age. This is the effect of apparent age on average across all listeners, <em>independent</em> of listener. This sort of effect is often referred to as a <strong>main effect</strong>. Someone might ask “what is the average apparent height difference between apparent children and adult females?” and you might say “about 20 cm”. Which listener exactly does this statement apply to? To all of them, this is the average <em>overall</em> effect.</p>
<p>In contrast, we may want to think about the effect of apparent age <em>conditional</em> on listener. For example, imagine that this effect varies conditionally based on the listener such that it is large for some listeners and small for others. In this case if someone asks “what is the apparent height difference between apparent children and adult females?” you may have to answer “well it depends on the listener”. When the effect of one predictor varies based on the value of another predictor, these predictors are said to <strong>interact</strong> or to have an <strong>interaction</strong>. The parameters in your model that help you capture these conditional effects are referred to as interactions or <strong>interaction effects</strong>. The ‘fixed’ effect for <code>A1</code> we included in our models in the last chapter really represents the marginal (overall) effect for the predictor. To investigate the values of this predictor <em>conditional</em> on listener, we need to include the listener by <code>A1</code> <em>interaction</em> in our model. We denote interaction terms in our models using a colon (<code>:</code>) where <code>A:B</code> can be read ‘A given B’ or ‘A conditional on B’.</p>
<p>We can find the average apparent height reported by each listener for each apparent age (<code>a</code> adult, or <code>c</code> child) using the code below:</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span> ( <span class="fu">tapply</span> (notmen<span class="sc">$</span>height, notmen[,<span class="fu">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;L&quot;</span>)], mean) )</span>
<span id="cb184-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb184-2" aria-hidden="true" tabindex="-1"></a><span class="do">##    L</span></span>
<span id="cb184-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb184-3" aria-hidden="true" tabindex="-1"></a><span class="do">## A     1   2   3   4   5   6   7   8   9  10  11  12  13  14  15</span></span>
<span id="cb184-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb184-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   a 167 167 164 161 162 166 163 165 166 165 165 168 168 168 165</span></span>
<span id="cb184-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb184-5" aria-hidden="true" tabindex="-1"></a><span class="do">##   c 162 142 132 154 141 148 149 140 140 138 155 148 134 146 148</span></span></code></pre></div>
<p>The average of each column is the average apparent height reported by each listener across both apparent ages. The average across each row is the average apparent height reported for each apparent age, across all listeners. Each individual cell represents the average apparent height reported by each listener for each apparent age, and the mean of all cells represents the overall grand mean.</p>
<p>In the left plot of figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-2">6.2</a> we present the average height reported by each listener. The variation seen along the line is a result of non-zero listener effects: In the absence of listener effects the line would be flat and horizontal at the grand mean. So, it is the listener effect (<code>L</code>) that gives the line its specific shape. In the middle plot we see the average apparent height reported by each listener for each apparent age. Each line has an unpredictably different shape, and since the shape reflects the listener effect, this means that listener (<code>L</code>) has an unpredictable effect across the levels of apparent age (<code>A</code>). This suggests that our data has a meaningful interaction between listener and apparent age, <code>L:A</code>, which can be read as ‘the effect of listener given the level of apparent age’.</p>
<div class="figure"><span style="display:block;" id="fig:F6-2"></span>
<img src="_main_files/figure-html/F6-2-1.jpeg" alt="(left) Average height reported by each listener overall. The horizontal line represents the grand mean. (middle) Age-dependent listener effects for apparent children (lower line) and apparent adults (upper line). The horizontal lines indicate the grand mean (solid), and the average means for apparent adults (upper dotted) and apparent children (lower dotted). (right) Listener-dependent age effects, the difference between the adult and child means reported by each listener (i.e., the difference between the lines in the middle plot). The horizontal line represents the average age effect across listeners." width="4800" />
<p class="caption">
Figure 6.2: (left) Average height reported by each listener overall. The horizontal line represents the grand mean. (middle) Age-dependent listener effects for apparent children (lower line) and apparent adults (upper line). The horizontal lines indicate the grand mean (solid), and the average means for apparent adults (upper dotted) and apparent children (lower dotted). (right) Listener-dependent age effects, the difference between the adult and child means reported by each listener (i.e., the difference between the lines in the middle plot). The horizontal line represents the average age effect across listeners.
</p>
</div>
<p>In the middle plot of figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-2">6.2</a>, the broken lines represent the mean apparent heights for apparent adults and apparent children. The average (main) effect for apparent age is equal to the difference between these broken lines and the solid line representing the grand mean (or half the distance between the broken lines). This effect has an average magnitude of 10.1 cm, which we can see in the right plot of the figure. However, each listener has an unpredictable difference between their points on each line, and a correspondingly unpredictable effect for apparent age in the right plot in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-2">6.2</a>. Since the distance between the lines determines the effect for apparent age, this represents listener-dependent variation in <code>A</code> and a meaningful interaction between apparent age and listener (<code>A:L</code>) in our data.</p>
<p>Actually, the <code>L:A</code> interaction <em>entails</em> an <code>A:L</code> interaction (and vice versa): In the process of adding an <code>A:L</code> interaction in your data you also necessarily add an <code>L:A</code> interaction to your data. Imagine the middle plot in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-2">6.2</a> contained two flat parallel lines, separated by some amount (the effect for <code>A</code>). In order to add a listener dependent age effect (<code>A:L</code>) to these lines, we would need to make the separation between the lines unpredictably different for each listener. However, in doing so we <em>necessarily</em> also make the shapes of the two lines unpredictably different. This is the only way to make the separation between the lines different for each listener. Since the shape of the line is the listener effect, by giving each line a different shape we also necessarily add an age-dependent effect for listener to our data. As a result of this, the <code>A:L</code> interaction is <em>the same thing</em> as the <code>L:A</code> interaction and the notational difference between them is only superficial.</p>
<p>Although we can think of interactions either way, conceptually it may be useful to think of them primarily from one perspective or another. In our case, we will focus on the listener-dependent effect for apparent age, presented in the right plot of figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-2">6.2</a>. The value of each point along the line in the right plot reflects the spacing of the two lines in the middle plot of the same figure. In the complete and total absence of an interaction between listener and age, the orange line in the figure would be horizontal at 10.1 (the marginal difference). In the presence of an interaction, our marginal and listener-dependent effects may diverge in a seemingly random manner, as seen in the figure.</p>
<p>Before moving on, we need to discuss the concepts of <strong>crossing</strong> and <strong>nesting</strong>, and how these relate to interactions. When two factors are <strong>crossed</strong>, this means that all levels of one factor appear at all levels of the other factor. When levels of one factor appear exclusively at specific levels of the other factor they are said to be <em>nested</em>. To estimate the interaction between two factors, the factors should be crossed. Since our ‘random effects’ are just interactions, this means that to include a random effect for a predictor given some grouping factor (like listener), it needs to be crossed with that factor.</p>
<p>For example, consider a situation where (monolingual) listeners are divided into groups according to their native languages. We might imagine a model like this that tries to estimate height based on the native language (<code>NatLang</code>) of the listeners to see if there are reliable cross-language differences in listener’s opinions:</p>
<p><code>height ~ NatLang + (1|L)</code></p>
<p>In such an experiment, the listener factor will be nested within first language because each listener only speaks one language. As a result, in such a situation a model <em>can</em> estimate the ‘fixed’, marginal effect for native language on apparent height. However, this model can <em>not</em> estimate the listener-dependent effect for first language, i.e. the first language ‘random effect’ for listeners <em>or</em> the listener by native language interaction. This means that neither of the following models are possible:</p>
<p><code>height ~ NatLang + (NatLang|L)</code></p>
<p><code>height ~ NatLang + L + NatLang:L</code></p>
<p>The reason for this is very straightforward. If we do not observe the effect of first language for each level of listener, we are not in a position to say anything about the <em>listener-dependent</em> effect of first language (i.e. the conditional effect of native language for every individual listener). For example, if the first listener is an English speaker, how can you possibly estimate the conditional effect for when that listener is a Mandarin speaker? What would that even mean?</p>
<p>In terms of our data, this means that to estimate listener-dependent effects for apparent age we need to make sure listener is <em>crossed</em> with apparent age. We can do this by, for example, cross tabulating our observations of apparent age with listener (<code>table(notmen$A, notmen$L)</code>) and making sure that we have more than zero observations for every combination of listener and apparent age (which we do). As a result, it’s possible to estimate the interaction between these in our model. In contrast, speakers are not crossed with apparent age. If we tabulate apparent age by speakers (<code>table(notmen$A, notmen$S)</code>), we see a lot of cells with zero observations. This is because some speakers were just not very confusable: Some children were never identified as adults and some adults were never identified as children. As a result, our model may run into problems if it tries to estimate the interaction between speaker and apparent age, or the random effect for apparent age for each speaker.</p>
<div id="description-of-our-model" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Description of our model<a href="variation-in-parameters-random-effects-and-model-comparison.html#description-of-our-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before including interactions in our model, let’s take a step back and consider the model formula we used last chapter. Below, we explicitly include the leading one (indicating the intercept) to make the following discussion simpler.</p>
<p><code>height ~ 1 + A + (1|L) + (1|S)</code></p>
<p>This formula told our model to draw the parameter related to <code>A</code> from a fixed prior distribution and to draw the <code>L</code> and <code>S</code> terms from distributions whose standard deviations were estimated from the data (i.e. using adaptive partial pooling). We did this by adding <code>L</code> and <code>S</code> to the right of a pipe inside parentheses (like this <code>(1|L)</code>), and putting a <code>1</code> to the left of the pipe. Recall that the <code>|</code> symbol means ‘given’, so that the notation <code>(1|L)</code> says ‘estimate an intercept given listener’, or ‘estimate a different intercept effect for each level of the factor listener’.</p>
<p>We can imagine an analogous model formula that uses only ‘fixed effects’, meaning it uses no pooling to estimate all of the listener and speaker effects. The formula for such a model would look like this:</p>
<p><code>height ~ 1 + A + L + S</code></p>
<p>As discussed above, we seem to have listener-dependent age effects in our model, suggesting that we want to include the interaction of apparent age and listener in our model. Interactions between combinations of fixed effects can be denoted using <code>:</code>. For example the formula below says “include the main effect for A and L, and the interaction between A and L”. The <code>:</code> symbol can also be read as “given”, which helps to highlight that these help represent conditional effects. So <code>A:L</code> can be read out loud as “A given L” or “the effect of A given the value of L”.</p>
<p><code>height ~ 1 + A + L + A:L + S</code></p>
<p>As a shortcut, we can use the symbol <code>*</code> which means “include the main effects for these predictors and all interactions between them.”</p>
<p><code>height ~ 1 + A * L + S</code></p>
<p>The previous two formulas include the marginal effects of age and listener and the interaction between them. However, they estimate all these without adaptive pooling. We know that it is advisable to estimate factors with large numbers of levels using adaptive pooling, regardless of how ‘random’ the effect may be (see section <a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#c4-strategies">4.4</a>). This applies to the marginal effects of predictors such as listener, but also to the interactions between these ‘random’ effects and our ‘fixed’ effects. In order to estimate listener-dependent age effects in our model using adaptive pooling, we include these inside the parentheses belonging to the listener predictor, like so:</p>
<p><code>height ~ 1 + A + (1 + A|L) + (1|S)</code></p>
<p>The formula above tells <code>brms</code>: “Height varies as a function of an intercept and an age effect, a listener-specific intercept and age effect, and a speaker specific intercept”. It also tells <code>brms</code> to estimate the listener intercepts, the listener age effects, and the speaker intercepts as random effects, i.e. using adaptive pooling. Notice the parallel structure between our models with and without random effects:</p>
<p><code>height ~ 1 + A +  L +  A:L     +  S</code></p>
<p><code>height ~ 1 + A + (1 +   A  |L) + (1 |S)</code></p>
<p>These models have equivalent parameters, which have been horizontally aligned in the two formulas above. The <code>L</code> term in the top model is represented by the <code>1</code> in <code>(1 + A|L)</code> in the bottom model (the listener dependent intercept effect), and the <code>A:L</code> term in the top model is represented by the <code>A</code> in <code>(1 + A|L)</code> in the bottom model (the listener dependent age effect). The main difference between <code>L</code> and <code>A:L</code> on one hand and <code>(1 + A|L)</code> on the other, is that the latter notation tells your model to estimate these listener dependent predictors using adaptive pooling. When our formula has non intercept predictors, we can omit the <code>1</code> and write it as seen below:</p>
<p><code>height ~ A + (A|L) + (1|S)</code></p>
<p>Our model formula might specify a model as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a>. Compared to the t distributed model we fit at the end of chapter 5, the only changes have been the addition of a new term, <span class="math inline">\(A \colon L_{[\mathsf{L}_{[i]}]}\)</span>, and its associated priors <span class="math inline">\(A \colon L_{[\bullet]} \sim \mathrm{t}(3, 0,\sigma_{A \colon L})\)</span>, and <span class="math inline">\(\sigma_{A \colon L} \sim \mathrm{t}(3,0,12)\)</span>. Note that although the <span class="math inline">\(A\)</span> predictor does not need a subscript, the <span class="math inline">\(A \colon L_{[\mathsf{L}_{[i]}]}\)</span> term (the by-listener random effect for age) does. This is because although we only have a single age predictor, the interaction term estimates a different one of these for each listener, which requires the estimation of 15 such parameters in our model.</p>
<p><span class="math display" id="eq:6-1">\[
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{t}(\nu, \mu_{[i]},\sigma) \\
\mu_{[i]} = \mathrm{Intercept} + A + L_{[\mathsf{L}_{[i]}]} + A \colon L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\ \\
\mathrm{Priors:} \\
L_{[\bullet]} \sim \mathrm{N}(0,\sigma_L) \\
A \colon L_{[\bullet]} \sim \mathrm{N}(0,\sigma_{A \colon L}) \\
S_{[\bullet]} \sim \mathrm{N}(0,\sigma_S) \\
\\
\mathrm{Intercept} \sim \mathrm{t}(3, 156,12) \\
A \sim \mathrm{t}(3, 0,12) \\
\sigma_L, \sigma_S, \sigma_{A \colon L} \sim \mathrm{t}(3, 0,12) \\
\nu \sim \mathrm{gamma}(2, 0.1) \\
\sigma \sim \mathrm{t}(3, 0,12) \\
\end{split}
\tag{6.1}
\end{equation}
\]</span></p>
<p>For the sake of comparison, <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1a">(6.2)</a> presents the model structure associated with the following formula:</p>
<p><code>height ~ 1 + A + L + A:L + S</code></p>
<p>We can see that <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a> and <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1a">(6.2)</a> are nearly identical. The only differences between the two models is whether the standard deviations of the prior distributions of the listener and speaker related parameters (e.g. <span class="math inline">\(L_{[\bullet]}\)</span>) are estimated from the data (as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a>), or specified a priori ( as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1a">(6.2)</a>).</p>
<p><span class="math display" id="eq:6-1a">\[
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{t}(\nu, \mu_{[i]},\sigma) \\
\mu_{[i]} = \mathrm{Intercept} + A + L_{[\mathsf{L}_{[i]}]} + A \colon L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\ \\
\mathrm{Priors:} \\
L_{[\bullet]} \sim \mathrm{N}(0,12) \\
A \colon L_{[\bullet]} \sim \mathrm{N}(0,12) \\
S_{[\bullet]} \sim \mathrm{N}(0,12) \\
\\
\mathrm{Intercept} \sim \mathrm{t}(3, 156,12) \\
A \sim \mathrm{t}(3, 0,12) \\
\nu \sim \mathrm{gamma}(2, 0.1) \\
\sigma \sim \mathrm{t}(3, 0,12) \\
\end{split}
\tag{6.2}
\end{equation}
\]</span></p>
<p>The model in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a> is preferable over the one in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1a">(6.2)</a> because it will use information about the distribution of <span class="math inline">\(L\)</span>, <span class="math inline">\(A \colon L\)</span>, and <span class="math inline">\(S\)</span> to estimate the associated effects. However, one potential problem is that the model in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a> draws each of the listener-dependent effects (<span class="math inline">\(L\)</span>, <span class="math inline">\(A \colon L\)</span>) from independent distributions as if these were totally unrelated, and they may not be.</p>
</div>
<div id="c6-correlations" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Correlations between random parameters<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-correlations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-3">6.3</a> we plot the listener effects, and the listener-dependent age effects originally presented in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-2">6.2</a>. However, this time they have both been sorted based on the value of the listener effects. We can see clearly that as the listener effects increase, the age effects decrease. In the rightmost plot we present a scatter plot of these two variables, which reinforces the fact that these variables have a negative relationship: Smaller values of apparent height tend to be associated with larger apparent age effects. This is important because, if this is true, it suggests that we are unlikely to observe listeners who reported tall speakers on average <em>and</em> had large age effects on height (or vice versa). This is because as seen in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-3">6.3</a>, listeners who reported tall speakers on average tended to exhibit small age effects.</p>
<div class="figure"><span style="display:block;" id="fig:F6-3"></span>
<img src="_main_files/figure-html/F6-3-1.jpeg" alt="(left) Average reported height by each listener, sorted by magnitude. (middle) The magnitude of the age effect for each listener, sorted by that listener's effect in the left plot. (right) Listener average reported height plotted against listener age effects." width="4800" />
<p class="caption">
Figure 6.3: (left) Average reported height by each listener, sorted by magnitude. (middle) The magnitude of the age effect for each listener, sorted by that listener’s effect in the left plot. (right) Listener average reported height plotted against listener age effects.
</p>
</div>
<p>The <strong>linear correlation</strong> between two variables (often referred to simply as the <strong>correlation</strong>) is a measure of the <strong>linear relationship</strong> between the variables. An informal (but accurate) way to think of a linear relationship is that when you make a scatter plot using two variables (as in the right plot of figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-3">6.3</a>), the organization of the points should resemble a line if there is a strong linear relationship between them. The more the dots form a single perfect line, the closer the magnitude of the correlation gets to 1. Whether the value of a perfect correlation is 1 or -1 depends on if the line is sloped up (1) or down (-1) left to right. A correlation of 0 means there is no linear relationship between the variables. So, if two variables have a correlation of 0 you would be hard-pressed to draw any kind of <em>line</em> that captured the relationship between the variables (though some other curve might work). Figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-3">6.3</a> suggests that listener means and age effects are <em>negatively correlated</em>.</p>
<p>There are many ways to measure correlation, but the most common way is by using <strong>Pearson’s correlation coefficient</strong>, defined in equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-2">(6.3)</a> for variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We use the little hat, like this <span class="math inline">\(\hat{r}\)</span>, because this is an estimate of the correlation between our variables. Equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-2">(6.3)</a> introduces a term we have not seen yet, <span class="math inline">\(\sigma_{xy}\)</span>, corresponding to the <em>covariance</em> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (often denoted by <span class="math inline">\(\mathrm{cov}(x,y)\)</span>). We can see below that the correlation between two variables is the covariance divided by the product of their individual standard deviations (<span class="math inline">\(\hat{\sigma}_x\)</span> and <span class="math inline">\(\hat{\sigma}_y\)</span>).</p>
<p><span class="math display" id="eq:6-2">\[
\begin{equation}
\hat{r} = \frac {\hat{\sigma}_{x,y}}
          {\hat{\sigma}_x \hat{\sigma}_y}
\tag{6.3}
\end{equation}
\]</span></p>
<p>The variance of a variable is the expected value of squared deviations from the mean. The <strong>covariance</strong> of two variables is the expected value of the product of their deviations of each from their respective means. The covariance of two variables can be estimated by calculating the sum of the product of deviations from the mean and dividing by the number of observations minus one, as seen in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-2a">(6.4)</a>.</p>
<p><span class="math display" id="eq:6-2a">\[
\begin{equation}
\hat{\sigma}_{x,y} = \frac {\Sigma(x_{[i]}-\bar{x})(y_{[i]}-\bar{y})}
          {(n-1)}
\tag{6.4}
\end{equation}
\]</span></p>
<p>Replacing <span class="math inline">\(\hat{\sigma}_{xy}\)</span> in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-2">(6.3)</a> with <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-2a">(6.4)</a> yields <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-3">(6.5)</a>, an estimate of the correlation (<span class="math inline">\(r\)</span>) between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p><span class="math display" id="eq:6-3">\[
\begin{equation}
\hat{r} = \frac {\Sigma(x_{[i]}-\bar{x})(y_{[i]}-\bar{y})}
          {(n-1) \cdot \hat{\sigma}_x \hat{\sigma}_y}
\tag{6.5}
\end{equation}
\]</span></p>
<p>Using equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-3">(6.5)</a>, we can calculate Pearson’s correlation coefficient for two variables. We can do this for any two vectors in R using the <code>cor</code> function. Below we find the average apparent height reported by each listener, and then half the difference in apparent height reported by each listener for each apparent age (i.e. the effect for apparent age). As seen below, this tells us that the correlation between the listener means and apparent age effects is -0.85, representing a strong negative correlation between these two variables.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find means for each listener for each apparent age</span></span>
<span id="cb185-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-2" aria-hidden="true" tabindex="-1"></a>listener_age_means <span class="ot">=</span> <span class="fu">tapply</span> (notmen<span class="sc">$</span>height, notmen[,<span class="fu">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;L&quot;</span>)], mean)</span>
<span id="cb185-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-4" aria-hidden="true" tabindex="-1"></a><span class="co"># average of each column = listener means</span></span>
<span id="cb185-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-5" aria-hidden="true" tabindex="-1"></a>listener_effects <span class="ot">=</span> <span class="fu">colMeans</span> (listener_age_means)</span>
<span id="cb185-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-7" aria-hidden="true" tabindex="-1"></a><span class="co"># half the difference across rows = listener age effects</span></span>
<span id="cb185-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-8" aria-hidden="true" tabindex="-1"></a>listener_age_effects <span class="ot">=</span> (listener_age_means[<span class="dv">1</span>,] <span class="sc">-</span> listener_age_means[<span class="dv">2</span>,]) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb185-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-10" aria-hidden="true" tabindex="-1"></a><span class="co"># correlation between listener means and age effects</span></span>
<span id="cb185-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (listener_effects, listener_age_effects)</span>
<span id="cb185-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb185-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -0.8691</span></span></code></pre></div>
<p>We can develop some intuitions regarding correlations by considering the very simple vectors seen below. The elements of the second vector are exactly equal to twice every element of the first vector: They are perfectly predictable one from the other. As a result, these two vectors have a correlation of 1.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb186-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb186-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb186-2" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb186-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb186-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (x1, y1)</span>
<span id="cb186-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb186-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span></code></pre></div>
<p>Below we see the opposite situation. The second vector is still twice the first vector, but now every sign differs across the two vectors. These are still perfectly predictable, just backwards. For example, if a gambler were wrong 100% of the time, anyone who did the opposite would win every bet. Below, we see that these vectors have a correlation of -1.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb187-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>,  <span class="dv">1</span>,  <span class="dv">1</span>)</span>
<span id="cb187-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb187-2" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">=</span> <span class="fu">c</span>( <span class="dv">2</span>,  <span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>)</span>
<span id="cb187-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb187-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (x1, y1)</span>
<span id="cb187-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb187-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -1</span></span></code></pre></div>
<p>In the next example we see that the signs of each element of the second vector are totally <em>unpredictable</em> based on the corresponding element in the first vector. In half the cases the signs match and in half the cases they do not. This results in a correlation of 0 between the vectors.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb188-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>,  <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb188-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb188-2" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,  <span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb188-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (x1, y1)</span>
<span id="cb188-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb188-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0</span></span></code></pre></div>
<p>Finally, we see a situation where the vectors <em>almost</em> match. In the example below three of the four elements match in sign, resulting in a positive correlation between 0 and 1.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb189-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>,  <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb189-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb189-2" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb189-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb189-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span> (x1, y1)</span>
<span id="cb189-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb189-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.5774</span></span></code></pre></div>
</div>
<div id="c6-random-and-mvn" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Random effects and the multivariate normal distribution<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-random-and-mvn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>multivariate normal</strong> distribution is a straightforward and very useful generalization of the normal distribution. A multivariate normal variable is a set of variables that are each normally distributed, <em>and</em> that result in another normally-distributed variable when linearly combined. Each individual variable that makes up the multivariate normal represents a <strong>dimension</strong> of the multivariate normal. So, if we have a three-dimensional normal distribution with dimensions <span class="math inline">\(x_1, x_2\)</span>, and <span class="math inline">\(x_3\)</span>, this implies that: 1) Each of <span class="math inline">\(x_1, x_2\)</span>, and <span class="math inline">\(x_3\)</span> are normally distributed, and 2) If we define a new variable <span class="math inline">\(x_4\)</span> such that <span class="math inline">\(x_4 = a \cdot x_1 + b \cdot x_2 + c \cdot x_3\)</span>, where <span class="math inline">\(a, b,\)</span> and <span class="math inline">\(c\)</span> can take any scalar value, then <span class="math inline">\(x_4\)</span> will be normally distributed.</p>
<p>When our models feature multiple predictors for a factor whose levels are estimated with adaptive pooling (e.g., speaker, listener), we model the coefficients with a multivariate normal distribution. In doing so we estimate the standard deviation of each dimension <em>and</em> the correlation between all pairs of dimensions. The easiest way to see why the correlation between dimensions matters is by drawing bivariate (2-dimensional) normal variables with different correlations and plotting the results.</p>
<p>The dimensions of the multivariate normal can have arbitrarily different means and standard deviations, but their marginal distributions will always be a (univariate) normal. In figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-4">6.4</a> we plot randomly-generated samples from three different bivariate normal distributions differing in the correlation between their dimensions, one variable presented in each row. The variables all have means and standard deviations that reflect the distribution of listener means (first column), and the listener-dependent age effects (second column). So, the first dimension has a mean and standard deviation equal to 154 and 4.8 cm, and the second dimension has a mean and standard deviation equal to 10.1 and 4.2 cm respectively.</p>
<div class="figure"><span style="display:block;" id="fig:F6-4"></span>
<img src="_main_files/figure-html/F6-4-1.jpeg" alt="Marginal distributions of 10,000 bivariate normal draws of simulated listener intercepts (left column) and listener-dependent A1 coefficients (middle column) from distributions with means and standard deviations based on our listener data. The right column presents both variables together. The correlation of the variables is 0 (top), 0.5 (middle) and -0.85 (bottom)." width="3600" />
<p class="caption">
Figure 6.4: Marginal distributions of 10,000 bivariate normal draws of simulated listener intercepts (left column) and listener-dependent A1 coefficients (middle column) from distributions with means and standard deviations based on our listener data. The right column presents both variables together. The correlation of the variables is 0 (top), 0.5 (middle) and -0.85 (bottom).
</p>
</div>
<p>We can use the simulated variables presented in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-4">6.4</a> to imagine possible relationships between listener means and age effects across some large sample of listeners. In the absence of any correlation between variables (top row), the distribution will resemble a circle in 2 dimensions (if the standard deviations are equal). When there is a positive correlation between the two dimensions (middle row), the joint distribution looks like an ellipse tilted up (moving from left to right). When there is a negative correlation (bottom row), the ellipse is tilted down moving left to right. Note however, that the marginal distributions of the variables (illustrated in the left and middle columns) don’t change as the correlation changes. The correlation is a reflection of the <em>joint</em> variation in the two variables and will not be evident in the marginal distributions of each variable.</p>
<p>Understanding the correlations between your ‘random effect’ parameters is important because it helps you understand which combinations of parameter values are more or less likely. When our dimensions are uncorrelated it is difficult (if not impossible) to make useful predictions from one dimension to the other. For example, in the top row we see that a mean height of 140 cm is about equally likely to be paired with apparent age affects (<code>A1</code>) of 0 or 20 cm. As a result, knowing that the speaker mean is 140 cm does not provide you much information about whether the age effect for that listener is likely to be large or small, positive or negative.</p>
<p>However, when the dimensions <em>are</em> correlated we can use this to make better predictions using our data. For example, in the bottom row we see a strong negative correlation between our dimensions. As a result, an average height of 140 cm is very likely when age effects are 20 cm, but <em>extremely</em> unlikely to be seen with an age effect of 0 cm. So, in this case knowing the speaker mean would actually help you narrow down the range of plausible listener effects. For this reason, when more than one random effect is estimated for each grouping factor, these are usually treated as a <em>multivariate normal</em> variable rather than as a set of independent normal distributions (as in our model in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a>).</p>
</div>
<div id="c6-mvn-priors" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Specifying priors for a multivariate normal distribution<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-mvn-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The shape of the multivariate normal distribution (i.e. how much it looks like a circle vs a tilted ellipse in the bivariate case) is determined by a covariance matrix, often represented by a capital sigma (<span class="math inline">\(\mathrm{\Sigma}\)</span>). A covariance matrix will be a square <span class="math inline">\(n \cdot n\)</span> matrix for a variable with <span class="math inline">\(n\)</span> dimensions. In equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-3a">(6.6)</a> we see an example of a covariance matrix for variables x, y, and z. The elements on the main diagonal of the covariance matrix represent the variances of each dimension, while the off-diagonal elements represent the co-variances of the variables (where <span class="math inline">\(\sigma_{x,y}=\sigma_{y,x}\)</span>).</p>
<p><span class="math display" id="eq:6-3a">\[
\begin{equation}
\begin{split}
\Sigma = \begin{bmatrix} \sigma_{x}^2 &amp; \sigma_{x,y} &amp; \sigma_{x,z} \\
\sigma_{y,x} &amp; \sigma_{y}^2 &amp; \sigma_{y,z} \\
\sigma_{z,x} &amp; \sigma_{z,y} &amp; \sigma_{z}^2 \end{bmatrix} \\
\end{split}
\tag{6.6}
\end{equation}
\]</span></p>
<p>Consider two random effects, a random by subject intercept <span class="math inline">\(L\)</span>, and a random by-listener apparent age effect called <span class="math inline">\(A \colon L\)</span>. If we assumed these came from a bivariate normal distribution the covariance matrix associated with this distribution might be:</p>
<p><span class="math display" id="eq:6-3b">\[
\begin{equation}
\begin{split}
\Sigma = \begin{bmatrix} \sigma_{L}^2 &amp; \sigma_{L,A \colon L} \\ \sigma_{A \colon L, L} &amp; \sigma_{A \colon L}^2 \\ \end{bmatrix} \\
\end{split}
\tag{6.7}
\end{equation}
\]</span></p>
<p>When we dealt with unidimensional normal distributions for our previous random effects, we specified priors for the (unidimensional) standard deviations using t (or normal) distributions. The specification of priors for our covariance matrix is only slightly more complicated. Rather than specify priors for <span class="math inline">\(\mathrm{\Sigma}\)</span> directly, <code>brms</code> (and <em>Stan</em>) builds up <span class="math inline">\(\mathrm{\Sigma}\)</span> from some simpler components that we specify. The covariance matrix for our random effects is created by multiplying the standard deviations of our individual dimensions by a correlation matrix (<span class="math inline">\(R\)</span>) specifying the correlations between each dimension. The operation is presented in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-4">(6.8)</a>.</p>
<p><span class="math display" id="eq:6-4">\[
\begin{equation}
\begin{split}
\mathrm{\Sigma} = \begin{bmatrix} \sigma_{L}^2 &amp; \sigma_{L,A \colon L} \\ \sigma_{A \colon L, L} &amp; \sigma_{A \colon L}^2 \\ \end{bmatrix} = \begin{bmatrix} \sigma_{L} &amp; 0 \\ 0 &amp; \sigma_{A \colon L} \\ \end{bmatrix}
\cdot R \cdot
\begin{bmatrix} \sigma_{L} &amp; 0 \\ 0 &amp; \sigma_{A \colon L} \\ \end{bmatrix} \\
\end{split}
\tag{6.8}
\end{equation}
\]</span></p>
<p>The non-zero values in the outside matrices are the (marginal, univariate) standard deviations of the random intercepts (<span class="math inline">\(\sigma_{L}\)</span>) and age effects (<span class="math inline">\(\sigma_{A \colon L}\)</span>). The correlation matrix <span class="math inline">\(R\)</span> contains information about the correlation between the dimensions of the variable (e.g., <span class="math inline">\(r_{L , A \colon L}\)</span>). The correlation matrix <span class="math inline">\(R\)</span> will look like <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-5">(6.9)</a> for a two-dimensional variable. In a correlation matrix each row gives you information about the correlation of one dimension with the others. For example, the first row represents the correlation of the first dimension to itself (first column) and the second dimension (second column). Correlation matrices contain only values of 1 on the main diagonal since the correlation of something with itself is one. Since the correlation of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> equals the correlation of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, correlation matrices are symmetric. This means that in the two dimensional case <span class="math inline">\(r_{1,2}=r_{2,1}\)</span>, or, more generally, <span class="math inline">\(r_{i, j} = r_{j, i}\)</span>.</p>
<p><span class="math display" id="eq:6-5">\[
\begin{equation}
\begin{split}
R = \begin{bmatrix} 1 &amp; r_{L,A \colon L} \\ r_{A \colon L,L} &amp; 1 \\ \end{bmatrix} \\ \\
\end{split}
\tag{6.9}
\end{equation}
\]</span></p>
<p>When we have multiple random effects that we are modeling as a multivariate normal, we need to specify priors for the standard deviations of each individual dimension and for the correlations between each pair of dimensions (i.e., for the correlation matrix), but not for the covariance matrix <span class="math inline">\(\mathrm{\Sigma}\)</span> directly. We provide priors for the standard deviations of the individual dimensions in the same way as we do for ‘unidimensional’ random effects. We specify priors for correlation matrices using the <span class="math inline">\(\mathrm{LKJCorr}\)</span> distribution in <code>brms</code>.</p>
<p><span class="math display" id="eq:6-6">\[
\begin{equation}
\begin{split}
R \sim \mathrm{LKJCorr} (\eta)
\end{split}
\tag{6.10}
\end{equation}
\]</span></p>
<p>This distribution has a single parameter (eta, <span class="math inline">\(\eta\)</span>) that determines how peaked the distribution is around 0 (seen in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-4b">6.5</a>). Basically, higher numbers make it harder to find larger correlations (and therefore yield more conservative estimates).</p>
<div class="figure"><span style="display:block;" id="fig:F6-4b"></span>
<img src="_main_files/figure-html/F6-4b-1.jpeg" alt="(left) Density of different correlation parameters for a two-dimensional correlation matrix, according to LKJ distributions with varying eta parameters. (right) Log densities of the densities in the left plot." width="4800" />
<p class="caption">
Figure 6.5: (left) Density of different correlation parameters for a two-dimensional correlation matrix, according to LKJ distributions with varying eta parameters. (right) Log densities of the densities in the left plot.
</p>
</div>
<p>The above was a full explanation of what information the model needs and why it needs it. You don’t need to <em>understand</em> any of the above to use random effects correctly. The important take away is that whenever you are estimating any random effects above and beyond a random intercept, you need to:</p>
<ol style="list-style-type: decimal">
<li><p>Specify priors for the standard deviation of each dimension.</p></li>
<li><p>Specify a prior for the correlation matrix for the multivariate normal used for the random parameters.</p></li>
</ol>
<p>And <code>brm</code> (and <em>Stan</em>) will do the rest.</p>
</div>
<div id="updating-our-model-description" class="section level3 hasAnchor" number="6.3.5">
<h3><span class="header-section-number">6.3.5</span> Updating our model description<a href="variation-in-parameters-random-effects-and-model-comparison.html#updating-our-model-description" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve discussed using multivariate normal distributions for our random effects, we can update our model formula as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-7">(6.11)</a>. Only two things have changed with respect to the description in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-1">(6.1)</a>. First, the listener random effects are drawn from a multivariate normal distribution (<span class="math inline">\(\mathrm{MVNormal}\)</span>) rather than as independent univariate normal variables. Second, we specify a prior for our correlation matrix (<span class="math inline">\(\mathrm{LKJCorr}\)</span>). Notice that we do not directly specify a prior for the multivariate normal covariance matrix <span class="math inline">\(\mathrm{\Sigma}\)</span>. This is because, as noted above, <em>Stan</em> constructs <span class="math inline">\(\mathrm{\Sigma}\)</span> by multiplying the standard deviation for each dimension (<span class="math inline">\(\sigma_{A \colon L}\)</span> and <span class="math inline">\(\sigma_{L}\)</span>) by the correlation matrix (<span class="math inline">\(R\)</span>) as in equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-4">(6.8)</a>. We include this in the final line of the model for completeness, but will be excluding it from future model specifications. This is because this element is not specified by the user, and becomes increasingly bulky as our models become more complicated.</p>
<p><span class="math display" id="eq:6-7">\[
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{t}(\nu,\mu_{[i]},\sigma) \\
\mu_{[i]} = \mathrm{Intercept} + A  + L_{[\mathsf{L}_{[i]}]} + A \colon L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\ \\
\mathrm{Priors:} \\
S_{[\bullet]} \sim \mathrm{N}(0,\sigma_S) \\
\begin{bmatrix} L_{[\bullet]} \\ A \colon L_{[\bullet]} \end{bmatrix} \sim \mathrm{MVNormal} \left( \begin{bmatrix} 0 \\ 0 \\ \end{bmatrix}, \mathrm{\Sigma} \right) \\
\\
\mathrm{Intercept} \sim \mathrm{t}(3,156,12) \\
A \sim \mathrm{t}(3,0,12) \\
\sigma, \sigma_L, \sigma_{A \colon L}, \sigma_S \sim \mathrm{t}(3,0,12) \\
\nu \sim \mathrm{gamma}(2, 0.1) \\
R \sim \mathrm{LKJCorr} (2) \\\\
\mathrm{\Sigma} = \begin{bmatrix} \sigma_{L} &amp; 0 \\ 0 &amp; \sigma_{A \colon L} \\ \end{bmatrix}
\cdot R \cdot
\begin{bmatrix} \sigma_{L} &amp; 0 \\ 0 &amp; \sigma_{A \colon L} \\ \end{bmatrix} \\
\end{split}
\tag{6.11}
\end{equation}
\]</span></p>
<p>Here’s a plain English description of the model specification in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-7">(6.11)</a>:</p>
<blockquote>
<p>We are modeling apparent height as coming from a t distribution with unknown nu (<span class="math inline">\(\nu\)</span>), mean (<span class="math inline">\(\mu\)</span>), and scale (<span class="math inline">\(\sigma\)</span>) parameters. The expected value for any given trial (<span class="math inline">\(\mu\)</span>) is modeled as the sum of an intercept, an effect for apparent age (<span class="math inline">\(A\)</span>), a listener effect (<span class="math inline">\(L\)</span>), a listener dependent effect for apparent age (<span class="math inline">\(A \colon L\)</span>), and a speaker effect (<span class="math inline">\(S\)</span>). The speaker effects were drawn from a univariate normal distribution with a standard deviation (<span class="math inline">\(\sigma_S\)</span>) estimated from the data. The two listener effects were drawn from a bivariate normal distribution with standard deviations (<span class="math inline">\(\sigma_L, \sigma_{A \colon L}\)</span>) and a correlation matrix (<span class="math inline">\(R\)</span>) that was estimated from the data. The remainder of the ‘fixed’ effects and the bivariate correlation were given prior distributions appropriate for their expected range of values.</p>
</blockquote>
</div>
<div id="c6-fitting" class="section level3 hasAnchor" number="6.3.6">
<h3><span class="header-section-number">6.3.6</span> Fitting and interpreting the model<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-fitting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below we fit a model where age coefficients vary across listeners. Notice that the only change in the formula is the inclusion of the <code>A</code> predictor on the left-hand-side of the pipe in <code>(A|L)</code>. We now include a prior for a new class of parameter <code>cor</code> which applies to the correlation matrices for our multivariate normal variables. In addition, we specify the priors <em>outside</em> the <code>brm</code> function call, and pass this to the function.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb190-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-2" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">=</span> <span class="fu">c</span>(brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,156, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb190-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-3" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,0, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb190-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-4" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,0, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb190-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-5" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>), </span>
<span id="cb190-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-6" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;gamma(2, 0.1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;nu&quot;</span>),</span>
<span id="cb190-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-7" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,0, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sigma&quot;</span>))</span>
<span id="cb190-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-9" aria-hidden="true" tabindex="-1"></a>model_re_t <span class="ot">=</span>  </span>
<span id="cb190-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-10" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> A <span class="sc">+</span> (A<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data =</span> notmen, <span class="at">chains =</span> <span class="dv">4</span>, </span>
<span id="cb190-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>, </span>
<span id="cb190-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb190-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">prior =</span> priors, <span class="at">family =</span> <span class="st">&quot;student&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or download it from the GitHub page:</span></span>
<span id="cb191-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb191-2" aria-hidden="true" tabindex="-1"></a>model_re_t <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;6_model_re_t.RDS&#39;</span>)</span></code></pre></div>
<p>We can check out the summary of our model:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb192-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">short_summary</span> (model_re_t)</span></code></pre></div>
<p>And see that although many parameter values may have changed with respect to <code>model_sum_coding_t</code> from chapter 5, the only new information is in the <code>L</code> group-level effects:</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Group-Level Effects:</span></span>
<span id="cb193-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb193-2" aria-hidden="true" tabindex="-1"></a><span class="do">## ~L (Number of levels: 15)</span></span>
<span id="cb193-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Estimate Est.Error l-95% CI u-95% CI</span></span>
<span id="cb193-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb193-4" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)         4.34      0.84     3.05     6.29</span></span>
<span id="cb193-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb193-5" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(A1)                4.26      0.85     2.96     6.25</span></span>
<span id="cb193-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb193-6" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,A1)    -0.80      0.12    -0.95    -0.51</span></span></code></pre></div>
<p>This section of the model summary now includes an estimate of the standard deviation of the by-listener effect for age (<code>sd(A1)</code>, i.e. the standard deviation of the listener by age interaction, <span class="math inline">\(\sigma_{A \colon L}\)</span> in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-7">(6.11)</a>), and the correlation between our listener-dependent intercept and age-effect terms (<code>cor(Intercept,A1)</code>). We can see that the estimate of this correlation (-0.80) is very similar to the ‘simple’ estimate we found above (-0.85).</p>
<p>You can get information about the variance, correlation, and covariance parameters using the <code>VarCorr</code> function in the <code>brms</code> package. As with the <code>fixef</code> and <code>ranef</code> functions, this function returns summaries of these samples by default but you can set <code>summary=FALSE</code> to get the individual samples for these parameters. We’re not going to talk about the structure of this output, but we encourage you to investigate it using the <code>str</code> function.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb194-1" aria-hidden="true" tabindex="-1"></a>varcorr_information <span class="ot">=</span> brms<span class="sc">::</span><span class="fu">VarCorr</span> (model_re_t)</span>
<span id="cb194-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb194-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span> (varcorr_information)</span></code></pre></div>
<p>You can also use the <code>get_corrs</code> and <code>get_sds</code> functions in the <code>bmmb</code> package to get summaries of your model standard deviations (including the error term):</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb195-1" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">get_sds</span> (model_re_t)</span>
<span id="cb195-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb195-2" aria-hidden="true" tabindex="-1"></a><span class="do">##       Estimate Est.Error  Q2.5 Q97.5 group</span></span>
<span id="cb195-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb195-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Int.L    4.338    0.8356 3.050 6.289     L</span></span>
<span id="cb195-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb195-4" aria-hidden="true" tabindex="-1"></a><span class="do">## A1       4.258    0.8463 2.964 6.248     L</span></span>
<span id="cb195-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb195-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Int.S    3.057    0.3594 2.398 3.808     S</span></span>
<span id="cb195-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb195-6" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma    5.354    0.2057 4.958 5.770 sigma</span></span></code></pre></div>
<p>And correlations:</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="co"># specify that we want the correlations for L</span></span>
<span id="cb196-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb196-2" aria-hidden="true" tabindex="-1"></a>bmmb<span class="sc">::</span><span class="fu">getcorrs</span> (model_re_t, <span class="at">factor=</span><span class="st">&quot;L&quot;</span>)</span>
<span id="cb196-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb196-3" aria-hidden="true" tabindex="-1"></a><span class="do">##          Estimate Est.Error    Q2.5   Q97.5</span></span>
<span id="cb196-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb196-4" aria-hidden="true" tabindex="-1"></a><span class="do">## A1, Int.  -0.8018    0.1151 -0.9491 -0.5087</span></span></code></pre></div>
<p>Below we load the sum-coded, t distributed model we fit in the last chapter. This model contained an identical fixed-effects structure to <code>model_re_t</code>, but does not include listener-dependent age effects (and associated parameters).</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb197-1" aria-hidden="true" tabindex="-1"></a>model_sum_coding_t <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;5_model_sum_coding_t.RDS&#39;</span>)</span></code></pre></div>
<p>Figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-5">6.6</a> presents a comparison of the two models, for selected model parameters. Despite clear similarities between the models, there are some changes to the credible intervals around the parameters that are shared across both models (presented in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-5">6.6</a>). It can be risky to make up explanations for things after the fact (<strong>post hoc</strong>), but we can try to think about why these changes may have occurred. The change in the interval around the intercept is very small, and likely reflects our (slightly) better overall understanding of the data. In contrast, the interval around <code>A1</code> has grown substantially. This is likely because we are now acknowledging between-listener variation in the <code>A1</code> parameter, which <code>model_sum_coding_t</code> ignored (or treated as zero). <code>model_re_t</code> predicts a standard deviation of 4.3 cm in the <code>A1</code> effect between listeners, and our data includes judgments from only 15 unique listeners. This limits how precise our <code>A1</code> estimate can/should be.</p>
<div class="figure"><span style="display:block;" id="fig:F6-5"></span>
<img src="_main_files/figure-html/F6-5-1.jpeg" alt="A comparison of estimates of the same parameters across our models with and without random effects (RE) for apparent age." width="4800" />
<p class="caption">
Figure 6.6: A comparison of estimates of the same parameters across our models with and without random effects (RE) for apparent age.
</p>
</div>
<p>The standard deviation of our listener intercepts (<span class="math inline">\(\sigma_L\)</span>, ‘sigma_L’ in the figure) appears to decrease slightly at the inclusion of random effects for age. This is likely because what previously seemed like variation in listener averages may have actually been variation in listener age effects. The separation of this into two separate predictors may lead to the magnitude of variation in one being diminished. Finally, the decrease in the data-level error (<span class="math inline">\(\sigma\)</span>, ‘sigma’ in the figure) is due to the inclusion of the listener random effects for <code>A1</code>, and the fact that including listener-dependent values for <code>A1</code> helped explain our variable. The error is just what the model can’t explain, so as our models explain more, the error will tend to get smaller.</p>
</div>
</div>
<div id="c6-model-comparison" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Model Comparison<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian models allow you to fit rather complicated ‘bespoke’ models to your data. A potential problem with this is that at some point your model may be ‘good enough’ such that, although further tweaks could be made, they are no longer contributing any practical benefit to your model. In fact, you can build a model that is so good at predicting your data that it can actually become bad at understanding the subject of your investigation more generally.</p>
<p>An analogy can be made between model fitting and clothing made by a tailor for an individual. You can go to a tailor and get a shirt made that exactly fits your body proportions. Imagine that tailoring the perfect shirt for a human body usually involved adjusting ten ‘shirt parameters’ (e.g. torso circumference, torso to sleeve ratio, etc.) to ensure a perfect fit for an individual. Despite this, when you go to a store to buy a shirt, there is usually one ‘parameter’, size, and it often comes in a small number of variants (e.g. small, medium, large). Since the shirts vary only in a small number of ways, they are very unlikely to fit any individual perfectly.</p>
<p>Why do stores intentionally sell clothing that doesn’t fit people as well as they could? Because the store is interested in selling clothes that fit everyone <em>pretty well</em>, rather than selling clothes that fit everyone perfectly. If stores sold shirts that fit everyone perfectly, each shirt model would only be useful to a very small number of people and would be useless for a large number of people. In contrast, by having a small number of shirts that fit the most people ‘pretty well’, the store can carry a small number of models and apply them successfully to customers in general.</p>
<p>This problem is sometimes referred to as the <strong>bias-variance tradeoff</strong>. In this case <em>bias</em> refers to the model’s ability to represent the ‘true’ relationship underlying the data, and variance refers to the degree to which the model is stable across data sets. Models that fit data very well (i.e. they have a low bias) often change substantially when fit to new data sets (i.e. they have a high variance). This is analogous to the fact that the shape of two shirts perfectly tailored (low bias) for two different people will potentially be very different from each other (high variance). On the other hand, models that do not fit the data <em>too</em> well (higher bias) often also have more consistent properties across new data sets (lower variance). This is analogous to the fact that shirts that vary in terms of small, medium, and large don’t fit many people perfectly, but manage to fit everyone pretty well while varying only slightly.</p>
<p>The bias-variance tradeoff can be understood in terms of the prediction of in-sample and out-of-sample data. Your <strong>in-sample</strong> data is the data you used to fit your model, and the <strong>out-of-sample</strong> data is other data that you do not have access to. A statistical model is <strong>overfit</strong> when it corresponds too closely to your in-sample data to the detriment of its ability to explain out-of-sample data. Recall that our models are usually used to carry out inductive reasoning: Going from a limited number of observations to the general case. In light of this, overfitting a model is exactly contrary to the goals of inductive inference. A model that does a poor job of generalizing to new observations is not useful to carry out induction. Further, a model that cannot explain out-of-sample data will not hold up to replication, which should be a concern for any researcher. This is because a researcher carrying out a replication of your work will necessarily be working with data that, from your perspective, is <em>out-of-sample</em> data.</p>
<p>In this section we will offer a high-level conceptual understanding of Bayesian model comparison, along with an explanation of how to carry this out. For more information on the subject we recommend reading chapter 7 of the excellent <em>Statistical Rethinking</em> (McElreath, 2020). Much of the information provided in this section is provided in more detail in Gelman et al. (2014), and Vehtari et al. (2017).</p>
<div id="c6-in-and-out-prediction" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> In-sample and out-of-sample prediction<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-in-and-out-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to compare models in a quantitative way, we need some value that can be calculated for different models that captures how ‘good’ the model is. For Bayesian models, we begin with what is called the <strong>log pointwise predictive density</strong>, abbreviated <span class="math inline">\(\mathrm{lpd}\)</span> (or <span class="math inline">\(\mathrm{lppd}\)</span>). The <span class="math inline">\(\mathrm{lpd}\)</span> can be calculated in several ways, but equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-8">(6.12)</a> presents an example of the general case. The <span class="math inline">\(\mathrm{lpd}\)</span> is the sum of the logarithm of the density for each of your data points. The symbol <span class="math inline">\(\theta\)</span> represents posterior estimates of all of the estimated parameters necessary to determine the probability density over each data point. This can be thought of as the general case of the log-likelihood outlined for the normal distribution in section <a href="probabilities-likelihood-and-inference.html#c2-chars-of-likelihoods">2.7.1</a>.</p>
<p><span class="math display" id="eq:6-8">\[
\begin{equation}
\widehat{\mathrm{lpd}} = \sum_{i=1}^{N} \mathrm{log} (p(y_{[i]} | \theta))
\tag{6.12}
\end{equation}
\]</span></p>
<p>The definition of <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> bears a strong resemblance to the log posterior density, discussed in section <a href="fitting-bayesian-regression-models-with-brms.html#c3-log-posterior">3.8</a>. Since we are adding logarithmic values, we know we are multiplying the underlying probabilities. If the data points are independent given our parameter values, then <span class="math inline">\(\mathrm{lpd}\)</span> represents the joint density of observing the data given the model structure and the parameter estimates (<span class="math inline">\(\theta\)</span>). Although prior probabilities are not included in this calculation they do factor into the estimation of <span class="math inline">\(\theta\)</span>, and as a result do have an effect on values of <span class="math inline">\(\mathrm{lpd}\)</span>.</p>
<p>Because <span class="math inline">\(\mathrm{lpd}\)</span> is a logarithmic value, large negative values are closer to zero on the original scale. So, values of <span class="math inline">\(\mathrm{lpd}\)</span> closer to (or above) zero represent models that are more likely given the data. As a result, we might think that we should simply select the model with the highest <span class="math inline">\(\mathrm{lpd}\)</span> as our ‘best’ model. However, <span class="math inline">\(\mathrm{lpd}\)</span> does not tell us directly about expected out-of-sample prediction, which is what we often care about.</p>
<p>We can think about out-of-sample prediction in terms of the <strong>expected log pointwise predictive density</strong>, abbreviated <span class="math inline">\(\mathrm{elpd}\)</span> (or <span class="math inline">\(\mathrm{elppd}\)</span>), presented in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-9">(6.13)</a>. Unlike the <span class="math inline">\(\mathrm{lpd}\)</span>, the <span class="math inline">\(\mathrm{elpd}\)</span> is defined in terms of hypothetical out-of-sample data (<span class="math inline">\(\tilde{y}\)</span>) instead of the in-sample data (<span class="math inline">\(y\)</span>). The <span class="math inline">\(\mathbb{E}\)</span> below represents the expected value function (discussed in section <a href="probabilities-likelihood-and-inference.html#c2-sample-mean">2.5.1</a>). So, the <span class="math inline">\(\mathrm{elpd}\)</span> represents the expected value of <span class="math inline">\(\mathrm{lpd}\)</span> for out-of-sample data.</p>
<p><span class="math display" id="eq:6-9">\[
\begin{equation}
\mathrm{elpd} = \sum_{i=1}^{N} \mathbb{E}(\mathrm{log} (p(\tilde{y}_i | \theta)))
\tag{6.13}
\end{equation}
\]</span></p>
<p>Of course, you can’t actually know the value of <span class="math inline">\(\mathbb{E}(\mathrm{log} (p(\tilde{y}_i | \theta))\)</span> because you do not have access to the true properties of the out-of-sample data (<span class="math inline">\(\tilde{y}\)</span>). As a result, you must settle for an estimate of <span class="math inline">\(\mathrm{elpd}\)</span>, <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span>. The simplest, and worst, way to estimate out-of-sample prediction is by simply looking at the in-sample prediction. One of the reasons that this does not really work is because making models more and more complicated will <em>always</em> improve in-sample prediction, at least a little. However, if the predictors are not related to true characteristics of the processes underlying the data, they will also tend to decrease the fit of the model to <em>new</em> data. Returning to the analogy of shirt fits discussed above. You can buy a shirt from the store and take it to a tailor to get it customized. The more the tailor changes this from the ‘standard’ shape, the better the shirt will fit you and the worse it will fit everyone else. Only alterations which conform to the ‘true’ average torso shape will increase the fit for people in general (and may in fact reduce the fit for you).</p>
<p>We can demonstrate this for some very simple models using simulations. In the code below, we generate three random variables consisting only of the values -1 and 1. We then use only the first variable (<code>x1</code>), and random normally-distributed error, to simulate two sets of random data: Our in-sample (<code>y</code>) data, and our out-of-sample data (<code>y_tilde</code>). We then fit three models to <em>only</em> the in-sample data. These models include increasingly more predictor variables (<span class="math inline">\(x_1, x_2, x_3\)</span>), however, we know that only <span class="math inline">\(x_1\)</span> is useful to understand the underlying process. Finally, we tried to predict the in-sample data <em>and</em> the out-of-sample data using the parameters estimated for each model using only the in-sample data.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">50</span>         <span class="co"># how many observations</span></span>
<span id="cb198-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-2" aria-hidden="true" tabindex="-1"></a>iter <span class="ot">=</span> <span class="dv">1000</span>    <span class="co"># how many simulations</span></span>
<span id="cb198-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-4" aria-hidden="true" tabindex="-1"></a><span class="co"># these will hold the model log likelihoods for each iteration</span></span>
<span id="cb198-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-5" aria-hidden="true" tabindex="-1"></a>lpd_hat <span class="ot">=</span> <span class="fu">matrix</span> (<span class="dv">0</span>, iter, <span class="dv">3</span>)</span>
<span id="cb198-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-6" aria-hidden="true" tabindex="-1"></a>elpd_hat <span class="ot">=</span> <span class="fu">matrix</span> (<span class="dv">0</span>, iter, <span class="dv">3</span>)</span>
<span id="cb198-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb198-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iter){</span>
<span id="cb198-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create 3 random predictors</span></span>
<span id="cb198-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-11" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">sample</span> (<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), n, <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb198-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-12" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">sample</span> (<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), n, <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb198-13"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-13" aria-hidden="true" tabindex="-1"></a>  x3 <span class="ot">=</span> <span class="fu">sample</span> (<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), n, <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb198-14"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-15"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate the observed (in sample) data with an  </span></span>
<span id="cb198-16"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># underlying process that only uses the x1 predictor</span></span>
<span id="cb198-17"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-17" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">1</span> <span class="sc">+</span> x1 <span class="sc">+</span> <span class="fu">rnorm</span> (n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb198-18"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># use the same process to simulate some &quot;out-of-sample&quot; data</span></span>
<span id="cb198-19"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-19" aria-hidden="true" tabindex="-1"></a>  y_tilde <span class="ot">=</span> <span class="dv">1</span> <span class="sc">+</span> x1 <span class="sc">+</span> <span class="fu">rnorm</span> (n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb198-20"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-20" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb198-21"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>){</span>
<span id="cb198-22"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit three models, the first using the real underlying model</span></span>
<span id="cb198-23"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (j<span class="sc">==</span><span class="dv">1</span>) mod <span class="ot">=</span> <span class="fu">lm</span> (y <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>x1)</span>
<span id="cb198-24"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the next two include random useless predictors</span></span>
<span id="cb198-25"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (j<span class="sc">==</span><span class="dv">2</span>) mod <span class="ot">=</span> <span class="fu">lm</span> (y <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>x1 <span class="sc">+</span> x2)</span>
<span id="cb198-26"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (j<span class="sc">==</span><span class="dv">3</span>) mod <span class="ot">=</span> <span class="fu">lm</span> (y <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3)</span>
<span id="cb198-27"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb198-28"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find the predicted value (mu) for each data point</span></span>
<span id="cb198-29"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-29" aria-hidden="true" tabindex="-1"></a>    mu <span class="ot">=</span> mod<span class="sc">$</span>fitted.values</span>
<span id="cb198-30"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and the estimated sigma parameter</span></span>
<span id="cb198-31"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-31" aria-hidden="true" tabindex="-1"></a>    sigma <span class="ot">=</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span>
<span id="cb198-32"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb198-33"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># equivalent to equation 6.10</span></span>
<span id="cb198-34"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-34" aria-hidden="true" tabindex="-1"></a>    lpd_hat[i,j] <span class="ot">=</span> <span class="fu">sum</span> (<span class="fu">dnorm</span> (y, mu, sigma, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb198-35"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># equivalent to equation 6.11</span></span>
<span id="cb198-36"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-36" aria-hidden="true" tabindex="-1"></a>    elpd_hat[i,j] <span class="ot">=</span> <span class="fu">sum</span> (<span class="fu">dnorm</span> (y_tilde, mu, sigma, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb198-37"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-37" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb198-38"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb198-38" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The models above are fit using a function that we have not discussed to this point. The <code>lm</code> (linear model) function uses maximum-likelihood estimation (see section <a href="probabilities-likelihood-and-inference.html#c2-inference-and-likelihood">2.8</a>) to fit regression models with relatively simple (‘unilevel’, see figure <a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fig:F4-2">4.2</a>) structures. We use it here because it estimates regression parameters very quickly and therefore is reasonable to use when we want to fit 3000 models in 5 seconds or so.</p>
<p>For each of our three candidate models, we use the predicted values (<code>mu</code>, <span class="math inline">\(\mu\)</span>) and error estimates (<code>sigma</code>, <span class="math inline">\(\sigma\)</span>) obtained using our in-sample data to estimate <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> using the in sample data:</p>
<p><span class="math display" id="eq:6-10">\[
\begin{equation}
\widehat{\mathrm{lpd}} = \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(y_{[i]} | \mu, \sigma))
\tag{6.14}
\end{equation}
\]</span>
And <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> using our out-of-sample data:</p>
<p><span class="math display" id="eq:6-11">\[
\begin{equation}
\widehat{\mathrm{elpd}} = \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(\tilde{y}_i | \mu, \sigma))
\tag{6.15}
\end{equation}
\]</span></p>
<p>In the formulas above we replace <span class="math inline">\(p(y_i | \theta)\)</span> with <span class="math inline">\(\mathrm{N}(y_{[i]} | \mu, \sigma)\)</span> because the simulations generate normally-distributed data. Our simulations result in three sets of <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> and three sets of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span>, a pair of each for each of the models above. The average of each of these six sets of values is presented in the left plot of figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a>. Since logarithmic values closer to zero are closer to one on the original scale, <em>more negative</em> log-likelihood values indicate a <em>less likely</em> outcome. We can see that the <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> improves as the model becomes more complicated, despite the fact that the <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> had absolutely no relationship to the data generating process or to the dependent variable.</p>
<div class="figure"><span style="display:block;" id="fig:F6-6"></span>
<img src="_main_files/figure-html/F6-6-1.jpeg" alt="(left) Average value of lpd and elpd estimates for each model in our simulated example. (right) The same values as on the left, however, now elpd estimates are edjusted (`elpd_adj`) based on the number of predictors in the model, as discussed in the text." width="4800" />
<p class="caption">
Figure 6.7: (left) Average value of lpd and elpd estimates for each model in our simulated example. (right) The same values as on the left, however, now elpd estimates are edjusted (<code>elpd_adj</code>) based on the number of predictors in the model, as discussed in the text.
</p>
</div>
<p>Why does this happen? One way to think about it is that the tallest person in California <em>and</em> Arizona will be <em>at least</em> as tall as the tallest person in Arizona. By increasing the number of ways it can explain your data, your more complex model will do <em>at least as well</em> as the model with fewer possible explanations. However, although the extra parameters increase the <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> estimate, they actually <em>decrease values</em> of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span>, indicating a worse out of sample fit. Why? Because since the extra predictors included in our model in no way relate to our data, the ‘answers’ they provide can only be due to <em>overfitting</em>, the learning of characteristics that are specific to the in-sample data rather than consistent properties of the out-of-sample data.</p>
<p>So, we see that adding ‘unnecessary’ predictors can decrease our out-of-sample performance. Ok, maybe we should just not ever include any ‘unnecessary’ predictors in our models? This is a bit like suggesting that since putting is hard, one should try to get a hole-in-one whenever possible. Obviously this would be ideal, however, there are some complications. In the example in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a> we knew the true underlying model and generated out-of-sample data that exactly conformed to the characteristics of our in-sample data. In real life, researchers do not usually have access to out-of-sample data, they do not know the characteristics of the ‘true’ model that underlies their data, nor can they confirm that any out-of-sample data shares the exact underlying process as their in-sample data. As a result, we can never <em>really</em> know what the difference is between in sample and out-of-sample prediction for our models.</p>
<p>Despite these complications, you may have noticed that: 1) The slopes of the lines representing <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> and <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a> seem to have predictable slopes, and 2) The lines diverge from each other at predictable rates based on the complexity of the models. Statisticians have noticed this too, and have used this to <em>adjust</em> <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> in order to estimate values of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> for out-of-sample data.</p>
</div>
<div id="c6-out-sample-adjust" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Out-of-sample prediction: Adjusting predictive accuracy<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-adjust" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The logic of adjusting <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> based on model complexity can be understood with reference to figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a>. Our goal is to select the model with the best out-of-sample prediction (i.e. the highest value on the red line), given only knowledge of the in-sample prediction (the values on the black line). In the left plot we can see that the difference between the black and red lines for each model increases by approximately one for every parameter we add. Let’s assume for the time being that this is a general property of all models. Since our models had two, three, and four parameters (including the intercept) respectively let’s subtract two, three and four from their respective <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> estimates to arrive at <em>adjusted</em> <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> estimates for each model. We can think of this as <strong>penalizing</strong> the value of <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> based on some penalty value <span class="math inline">\(\mathrm{p}\)</span> as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-12">(6.16)</a>. Different ways to estimate <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> differ in terms of how they estimate <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> and <span class="math inline">\(\mathrm{p}\)</span>.</p>
<p><span class="math display" id="eq:6-12">\[
\begin{equation}
\widehat{\mathrm{elpd}} = \widehat{\mathrm{lpd}} - \mathrm{p}
\tag{6.16}
\end{equation}
\]</span></p>
<p>You may have noticed that our penalization does not exactly recreate the line reflecting <span class="math inline">\(\mathrm{elpd}\)</span> in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a>, but rather results in a line parallel to it. We can never know the real distance between <span class="math inline">\(\mathrm{lpd}\)</span> and <span class="math inline">\(\mathrm{elpd}\)</span>, the solid red and black lines in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a>, because we can never know the ‘true’ properties of out-of-sample data. Despite this, penalization allows us to estimate a line <em>parallel</em> to <span class="math inline">\(\mathrm{elpd}\)</span> (the broken red line in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a>), and base our inference on this line. In the simple example we are discussing here, we are setting <span class="math inline">\(\mathrm{p}=k\)</span>, where <span class="math inline">\(k\)</span> is the number of parameters estimated by the model. If we carry out the operation in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-12">(6.16)</a> using the values in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-6">6.7</a>, we arrive at values of <span class="math inline">\(\mathrm{elpd}\)</span> of -71.7 (-69.7 - 2) and -72.7 (-68.7-4) for the least and most complex models. Based on these estimates of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span>, we expect that the model with the worst <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> actually has the best <span class="math inline">\(\mathrm{elpd}\)</span>.</p>
<p>In fact, we know that in this case the ‘best’ model corresponds <em>exactly</em> to the true data generating process. However, penalization will not always result in the highest values of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> for the simplest model. In figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-7">6.8</a> we simulate new fake data, except now we include <span class="math inline">\(x_2\)</span> in the ‘real’ underlying data generating process (i.e., <code>y = x1 + x2 + rnorm (n, 0, 1)</code>). As a result, for this data we actually <em>do</em> need both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> in the model. As seen below, penalization does not obscure the benefit of adding <span class="math inline">\(x_2\)</span> to our model when it is warranted. This is because the relatively small penalty associated with the increased model complexity does not overwhelm the large benefit due to the inclusion of a predictor that is actually related to our dependent variable.</p>
<div class="figure"><span style="display:block;" id="fig:F6-7"></span>
<img src="_main_files/figure-html/F6-7-1.jpeg" alt="(left) Average value of lpd and elpd for each model in our simulated example, modified to also include the second predictor (`x2`) in the data generating process. (right) The same values as on the left, however, now elpd is estimated based on adjusting the lpd using the number of model parameters. The y axis range intentionally omits the first model so that the information for the second and third models can be seen clearly." width="4800" />
<p class="caption">
Figure 6.8: (left) Average value of lpd and elpd for each model in our simulated example, modified to also include the second predictor (<code>x2</code>) in the data generating process. (right) The same values as on the left, however, now elpd is estimated based on adjusting the lpd using the number of model parameters. The y axis range intentionally omits the first model so that the information for the second and third models can be seen clearly.
</p>
</div>
<p>Two aspects involved in the traditional estimation of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> need to be updated to use these measures for multilevel Bayesian models. The first of these is the way that models are penalized based on their complexity. Historically, the <span class="math inline">\(\mathrm{p}\)</span> term is related to the number of <em>independent</em> parameters estimated by the model. Estimated parameters are those that are not specified a priori but instead depend on the data and structure of the model. Unfortunately, counting the number of independent parameters is not so straightforward in our multilevel models since parameters estimated with adaptive pooling (and shrinkage) are not fully independent. For example, a set of 10 ‘random effects’ that have all been pulled slightly towards their shared mean can hardly be considered totally independent. On the other hand they do vary from each other in unpredictable ways. As a result, it seems like a random effect with ten levels may represent somewhere between zero and ten independent parameters, based on how much they have been <em>shrunk</em> towards their mean. Our estimate of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> will need a way to estimate model complexity that takes this into account.</p>
<p>Second, <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> and <span class="math inline">\(\mathrm{p}\)</span> have traditionally been <em>point estimates</em>, single values often based on maximum-likelihood estimation. Since our models consist of posterior distributions of parameters, we instead have <em>distributions</em> of <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> and <span class="math inline">\(\mathrm{p}\)</span>. This means that we also have a distribution of values of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span>, and it would be useful to take this into account.</p>
<p>In this section we’re only going to discuss the <strong>widely available information criterion</strong> (<span class="math inline">\(\mathrm{WAIC}\)</span>) as calculated using the <code>brms</code> package. For a more complete treatment of other methods (and historical approaches), please see Gelman et al. (2014). To calculate <span class="math inline">\(\mathrm{WAIC}\)</span> we first find the average density of each data point (<span class="math inline">\(i\)</span>) given our model parameters (<span class="math inline">\(p(y|\theta^s)\)</span>) across all posterior samples (<span class="math inline">\(S\)</span>). This is presented for data point 1 in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-13">(6.17)</a>. For example, imagine a case where we have 5000 posterior draws so that <span class="math inline">\(S=5000\)</span>. To calculate the value below for our first data point (<span class="math inline">\(y_{[1]}\)</span>) we would find the density for every one of the 5000 posterior draws based on the changing values of <span class="math inline">\(\theta^s\)</span> across our draws. The average of these values is represented in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-13">(6.17)</a>.</p>
<p><span class="math display" id="eq:6-13">\[
\begin{equation}
\frac{1}{S} \sum_{s=1}^{S} p(y_{[1]} | \theta^s)
\tag{6.17}
\end{equation}
\]</span></p>
<p>To estimate <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> for a set of <span class="math inline">\(n\)</span> data points we take the logarithm of the value in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-13">(6.17)</a> and add this up across all of our observations. This is presented in equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-14">(6.18)</a>.</p>
<p><span class="math display" id="eq:6-14">\[
\begin{equation}
\widehat{\mathrm{lpd}} = \sum_{i=1}^{n} \mathrm{log} (\frac{1}{S} \sum_{s=1}^{S} p(y_{[i]} | \theta^s))
\tag{6.18}
\end{equation}
\]</span></p>
<p>Rather than count the number of parameters in our model, <span class="math inline">\(\mathrm{WAIC}\)</span> estimates the penalty term based on the characteristics of the predictive density. First, you find the log density for each individual data point across all posterior draws, and then find the variance of this. Equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-15">(6.19)</a> shows an example of this for the first data point. The <span class="math inline">\(Var_{s=1}^{\,S}\)</span> term indicates that we are finding the variance of <span class="math inline">\(\mathrm{log} (p(y_{[1]} | \theta^s))\)</span> across values of <span class="math inline">\(S\)</span>.</p>
<p><span class="math display" id="eq:6-15">\[
\begin{equation}
\mathrm{Var}_{s=1}^{\,S}(\mathrm{log} (p(y_{[1]} | \theta^s)))
\tag{6.19}
\end{equation}
\]</span></p>
<p>To estimate the penalty terms for WAIC, <span class="math inline">\(\mathrm{p_{WAIC}}\)</span>, the value in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-15">(6.19)</a> is added up for all <span class="math inline">\(n\)</span> data points as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-16">(6.20)</a>.</p>
<p><span class="math display" id="eq:6-16">\[
\begin{equation}
\mathrm{p_{\mathrm{WAIC}}} = \sum_{i=1}^{n} \mathrm{Var}_{s=1}^{\,S}(\mathrm{log} (p(y_{[i]} | \theta^s)))
\tag{6.20}
\end{equation}
\]</span></p>
<p>Why does this work? This is one of those things that you may need to <em>get used to</em> rather than <em>understand</em>, although more information can be found in Vehtari et al. (2017). The short, conceptual explanation is that more complex models exhibit more variation in their posterior probabilities. As a result, the variance of the log density across the posterior samples is a general way to estimate model complexity that avoids issues related to how many truly independent parameters a model estimates. Given our estimate of <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> and <span class="math inline">\(\mathrm{p_{WAIC}}\)</span>, we can now estimate <span class="math inline">\(\widehat{\mathrm{elpd}}_{WAIC}\)</span> as in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-17">(6.21)</a>.</p>
<p><span class="math display" id="eq:6-17">\[
\begin{equation}
\widehat{\mathrm{elpd}}_{WAIC} = \widehat{\mathrm{lpd}} - \mathrm{p_{WAIC}}
\tag{6.21}
\end{equation}
\]</span></p>
<p>Below we load the sum-coded models we fit in the last chapter:</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb199-1" aria-hidden="true" tabindex="-1"></a>model_sum_coding <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;5_model_sum_coding.RDS&#39;</span>)</span>
<span id="cb199-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb199-2" aria-hidden="true" tabindex="-1"></a>model_sum_coding_t <span class="ot">=</span> bmmb<span class="sc">::</span><span class="fu">get_model</span> (<span class="st">&#39;5_model_sum_coding_t.RDS&#39;</span>)</span></code></pre></div>
<p>And set our options to sum coding to match the coding we used when we fit the models:</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&#39;contr.sum&#39;</span>,<span class="st">&#39;contr.sum&#39;</span>))</span></code></pre></div>
<p>We can use the <code>add_criterion</code> function in <code>brms</code>, and specify <code>criterion="waic"</code> to add the <code>waic</code> criterion to our model object. We do this for both of our models.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb201-1" aria-hidden="true" tabindex="-1"></a>model_sum_coding <span class="ot">=</span> </span>
<span id="cb201-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb201-2" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">add_criterion</span> (model_sum_coding, <span class="at">criterion=</span><span class="st">&quot;waic&quot;</span>)</span>
<span id="cb201-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb201-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb201-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb201-4" aria-hidden="true" tabindex="-1"></a>model_sum_coding_t <span class="ot">=</span> </span>
<span id="cb201-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb201-5" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">add_criterion</span> (model_sum_coding_t, <span class="at">criterion=</span><span class="st">&quot;waic&quot;</span>)</span></code></pre></div>
<p>Adding the <code>waic</code> criterion to our model with Gaussian errors (<code>model_sum_coding</code>) returns an error message. To understand why we get these errors we can investigate the model <code>waic</code> information, which we can see below:</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-1" aria-hidden="true" tabindex="-1"></a>model_sum_coding<span class="sc">$</span>criteria<span class="sc">$</span>waic</span>
<span id="cb202-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb202-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Computed from 5000 by 1401 log-likelihood matrix</span></span>
<span id="cb202-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb202-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-5" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate   SE</span></span>
<span id="cb202-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-6" aria-hidden="true" tabindex="-1"></a><span class="do">## elpd_waic  -5042.3 32.5</span></span>
<span id="cb202-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-7" aria-hidden="true" tabindex="-1"></a><span class="do">## p_waic        81.8  3.7</span></span>
<span id="cb202-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-8" aria-hidden="true" tabindex="-1"></a><span class="do">## waic       10084.6 65.0</span></span>
<span id="cb202-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb202-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb202-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 27 (1.9%) p_waic estimates greater than 0.4. We recommend trying loo instead.</span></span></code></pre></div>
<p>The first two rows represent the expected log pointwise predictive density (<span class="math inline">\(\widehat{\mathrm{elpd}}_{WAIC}\)</span>, <code>elpd_waic</code>), and the penalty related to the flexibility of the model (<span class="math inline">\(\mathrm{p_{WAIC}}\)</span>, <code>p_waic</code>). The third column is the information criterion <span class="math inline">\(\mathrm{WAIC}\)</span> which is just -2 times the first row. The <span class="math inline">\(\mathrm{p_{WAIC}}\)</span> penalty term is sometimes referred to as the <strong>effective number of parameters</strong>, but Vehtari et al. (2017) caution that this usage is somewhat figurative and should not be ‘over-interpreted’. The effective number of parameters relates to the complexity and flexibility of the model and its ability to adapt to new data. Our sum coding model has 114 estimated parameters: 15 listener means, 1 listener standard deviation, 94 speaker means, 1 speaker standard deviation, 2 fixed effects terms, and one error standard deviation. Despite this, we see that our effective number of parameters is just 81.8.</p>
<p>The statistics provided above are calculated individually for each data point. We can get the pointwise information as seen below:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb203-1" aria-hidden="true" tabindex="-1"></a>model_waic_info <span class="ot">=</span> model_sum_coding<span class="sc">$</span>criteria<span class="sc">$</span>waic<span class="sc">$</span>pointwise</span>
<span id="cb203-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb203-2" aria-hidden="true" tabindex="-1"></a>model_t_waic_info <span class="ot">=</span> model_sum_coding_t<span class="sc">$</span>criteria<span class="sc">$</span>waic<span class="sc">$</span>pointwise</span></code></pre></div>
<p>There will be one row for each data point used to fit the model. Below we can have a look at the first six values:</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first six data points</span></span>
<span id="cb204-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span> (model_waic_info)</span>
<span id="cb204-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      elpd_waic   p_waic   waic</span></span>
<span id="cb204-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]    -5.108 0.234948 10.216</span></span>
<span id="cb204-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]    -3.116 0.004170  6.232</span></span>
<span id="cb204-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]    -4.431 0.153398  8.862</span></span>
<span id="cb204-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]    -3.109 0.003014  6.217</span></span>
<span id="cb204-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,]    -4.139 0.116198  8.279</span></span>
<span id="cb204-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb204-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]    -3.104 0.002666  6.208</span></span></code></pre></div>
<p>The estimates above simply correspond to the sum of each column. The standard error (SE) corresponds to the variance of the column times the square root of the number of rows (since the statistics are sums and not means). Based on this, we can recreate the summary values seen above:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the sum of each column</span></span>
<span id="cb205-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colSums</span> (model_waic_info)</span>
<span id="cb205-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-3" aria-hidden="true" tabindex="-1"></a><span class="do">## elpd_waic    p_waic      waic </span></span>
<span id="cb205-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   -5042.3      81.8   10084.6</span></span>
<span id="cb205-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the standard deviation of each column</span></span>
<span id="cb205-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-7" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span> (model_waic_info,<span class="dv">2</span>,sd) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1401</span>)</span>
<span id="cb205-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-8" aria-hidden="true" tabindex="-1"></a><span class="do">## elpd_waic    p_waic      waic </span></span>
<span id="cb205-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb205-9" aria-hidden="true" tabindex="-1"></a><span class="do">##    32.511     3.742    65.021</span></span></code></pre></div>
<p>Now we can return to the issue of the error message, which said that “<code>27 (1.9%) p_waic estimates greater than 0.4</code>”. Large values of <code>p_waic</code> suggest a poor fit between a data point and the model. To investigate this we can get the posterior mean residuals from each of the models we are considering:</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb206-1" aria-hidden="true" tabindex="-1"></a>resids <span class="ot">=</span> <span class="fu">scale</span>(<span class="fu">residuals</span>(model_sum_coding)[,<span class="dv">1</span>])</span>
<span id="cb206-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb206-2" aria-hidden="true" tabindex="-1"></a>resids_t <span class="ot">=</span> <span class="fu">scale</span>(<span class="fu">residuals</span>(model_sum_coding_t)[,<span class="dv">1</span>])</span></code></pre></div>
<p>We scale our residuals so that they are represented in terms of standard deviations from the mean, and plot these against the <code>p_waic</code> values calculated for each data point by each model. Clearly, large residuals relate to large <code>p_waic</code> values. In other words, large <code>p_waic</code> values are related to data points that are a very poor fit for our model. In our model with Gaussian errors this relationship is very predictable and quickly leads to large values in <code>p_waic</code>. However, for our model with t distributed errors (<code>model_sum_coding_t</code>), residuals beyond two standard deviations do not strongly affect <code>p_waic</code>. These differences are due to the different behaviors of the normal and t distributions in their tails, as discussed in section <a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-robustness">5.9</a>.</p>
<div class="figure"><span style="display:block;" id="fig:F6-8"></span>
<img src="_main_files/figure-html/F6-8-1.jpeg" alt="(left) Standardized model residuals plotted against values of the WAIC penalty term (p_waic) for each data point for the model with Gaussian errors, `model_sum_coding`. (right) The same as on the left but for the model with t distributed errors, `model_sum_coding_t`." width="4800" />
<p class="caption">
Figure 6.9: (left) Standardized model residuals plotted against values of the WAIC penalty term (p_waic) for each data point for the model with Gaussian errors, <code>model_sum_coding</code>. (right) The same as on the left but for the model with t distributed errors, <code>model_sum_coding_t</code>.
</p>
</div>
<p>The warning message suggest that we use <code>loo</code> (discussed in the next section) rather than <code>waic</code> because some <code>p_waic</code> values are too large. Why is this bad? Because the theory underlying the use of <span class="math inline">\(\widehat{\mathrm{elpd}}_{WAIC}\)</span> basically assumes that there will <em>not</em> be large values of <code>p_waic</code>. So, if there <em>are</em> large values of <code>p_waic</code> that suggests that something is wrong and <span class="math inline">\(\widehat{\mathrm{elpd}}_{WAIC}\)</span> may no longer be reliable. When this occurs, as the error message suggests, you shouldn’t use <span class="math inline">\(\widehat{\mathrm{elpd}}_{WAIC}\)</span> even if it seems reliable and looks ‘fine’. Think of it this way, if a bridge says it has a weight limit of three tonnes and you’re driving a truck that weighs four tonnes, should you drive across the bridge? Maybe you get across safely and save some time. Maybe you crash off the bridge into the river. When you use something like this despite being warned not to, you run the same risk: Maybe what you report is a true and reliable analysis, and maybe it’s not and you are reporting nonsense (the academic equivalent of crashing into the river).</p>
</div>
<div id="c6-out-sample-crossval" class="section level3 hasAnchor" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Out-of-sample prediction: Cross validation<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-out-sample-crossval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The last way to evaluate models that we’ll be discussing is called <strong>cross validation</strong>. Cross validation consists of dividing your data into two groups. You use one group to fit a model (i.e. your in-sample, or <strong>training data</strong>), and then use this model to predict the other group of data (i.e. the out-of-sample, or <strong>testing data</strong>). In this way, cross validation is a way to simulate out-of sample prediction using only the data you actually have.</p>
<p>There are many ways to carry out cross validation, and these mostly differ in the way that data is divided into training and testing groups. <strong>Holdout</strong> cross validation makes one single partition and uses one group to test and the other to train. Obviously this is not ideal because the results will be highly dependent on the specific groups you made. In <strong>k-fold</strong> cross validation you split your data into <span class="math inline">\(k\)</span> equal parts. You then train on parts <span class="math inline">\(k_{-i}\)</span> for iteration <span class="math inline">\(i\)</span>, and predict the <span class="math inline">\(i^{th}\)</span> group using that model. This approach can minimize the problems of holdout cross validation since it uses k, rather than one, estimates of the out-of-sample performance.</p>
<p>The logical endpoint of k-fold cross-validation is <strong>leave-one-out</strong> cross validation, sometimes referred to as <span class="math inline">\(\mathrm{LOO}\)</span>. In leave-one-out cross validation you leave out a single observation at a time. So, if you have <span class="math inline">\(n\)</span> observations you train the model on all observations but <span class="math inline">\(i\)</span> (i.e. <span class="math inline">\(y_{[-i]}\)</span>), and then predict the lone held out point (<span class="math inline">\(y_[i]\)</span>). Although <span class="math inline">\(\mathrm{LOO}\)</span> provides excellent estimates of out-of-sample prediction, this approach can be computationally intensive since it involves fitting <span class="math inline">\(n\)</span> models for <span class="math inline">\(n\)</span> data points. For example, our relatively small data set (<code>notmen</code>) has 1401 observations, potentially requiring 1401 models.</p>
<p>Luckily, recent advances have resulted in fast ways to approximate <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span>, i.e. estimates of <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> based on <span class="math inline">\(\mathrm{LOO}\)</span>, without having to refit models repeatedly. The <code>loo</code> package (Vehtari et al. 2022) in R and the associated functions implemented in <code>brms</code> make it easy to estimate <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> in a very fast an efficient manner. In addition, <span class="math inline">\(\mathrm{LOO}\)</span> works in many cases where <span class="math inline">\(\mathrm{WAIC}\)</span> does not, hence the suggestion in the error message above to use <span class="math inline">\(\mathrm{LOO}\)</span> for model comparison.</p>
<p>The calculation of <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> is more complicated than what we can explain here, however, we can provide an approximate definition. In <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-18">(6.22)</a> we use the <span class="math inline">\(\approx\)</span> symbol because we are not providing an exact definition of how <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> is calculated in practice, but only in principle. Equation <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-18">(6.22)</a> states that the estimate of <span class="math inline">\(\mathrm{elpd}\)</span> provided by LOO cross validation is approximately equal to the sum of the log density of each data point (<span class="math inline">\(y_{[i]}\)</span>) based on parameters estimated using all the data <em>except</em> for that point (<span class="math inline">\(\theta_{y_{[-i]}}\)</span>).</p>
<p><span class="math display" id="eq:6-18">\[
\begin{equation}
\widehat{\mathrm{elpd}}_{LOO} \approx \sum_{i=1}^{n} \mathrm{log} (p(y_{[i]} | \theta_{y_{[-i]}}))
\tag{6.22}
\end{equation}
\]</span></p>
<p>We define the relationship between <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> and <span class="math inline">\(\widehat{\mathrm{lpd}}\)</span> using the same general relationship outlined above, updated for <span class="math inline">\(\mathrm{LOO}\)</span> in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-19">(6.23)</a>.</p>
<p><span class="math display" id="eq:6-19">\[
\begin{equation}
\widehat{\mathrm{elpd}}_{LOO} = \widehat{\mathrm{lpd}} - \mathrm{p_{\mathrm{LOO}}}
\tag{6.23}
\end{equation}
\]</span></p>
<p>Based on this, we can estimate <span class="math inline">\(\mathrm{p_{\mathrm{LOO}}}\)</span> by rearranging the terms of the equation as seen in <a href="variation-in-parameters-random-effects-and-model-comparison.html#eq:6-20">(6.24)</a>. Just as with <span class="math inline">\(\mathrm{p_{\mathrm{WAIC}}}\)</span>, <span class="math inline">\(\mathrm{p_{\mathrm{LOO}}}\)</span> reflects the effective number of parameters, an estimate of model complexity.</p>
<p><span class="math display" id="eq:6-20">\[
\begin{equation}
\mathrm{p_{\mathrm{LOO}}} = \widehat{\mathrm{lpd}} - \widehat{\mathrm{elpd}}_{LOO}
\tag{6.24}
\end{equation}
\]</span></p>
<p>Estimates of <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> and associated statistics can be added to our models using the code below. We will now include our model with random effects for apparent age in our comparison.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-1" aria-hidden="true" tabindex="-1"></a>model_sum_coding <span class="ot">=</span> </span>
<span id="cb207-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-2" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">add_criterion</span> (model_sum_coding, <span class="at">criterion=</span><span class="st">&quot;loo&quot;</span>)</span>
<span id="cb207-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-4" aria-hidden="true" tabindex="-1"></a>model_sum_coding_t <span class="ot">=</span> </span>
<span id="cb207-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-5" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">add_criterion</span> (model_sum_coding_t, <span class="at">criterion=</span><span class="st">&quot;loo&quot;</span>)</span>
<span id="cb207-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-7" aria-hidden="true" tabindex="-1"></a>model_re_t <span class="ot">=</span> </span>
<span id="cb207-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb207-8" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">add_criterion</span> (model_re_t, <span class="at">criterion=</span><span class="st">&quot;loo&quot;</span>)</span></code></pre></div>
<p>We can see our new criteria using the code below. Notice that we no longer see the error message we got with <span class="math inline">\(\mathrm{WAIC}\)</span>.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-1" aria-hidden="true" tabindex="-1"></a>model_sum_coding<span class="sc">$</span>criteria<span class="sc">$</span>loo</span>
<span id="cb208-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb208-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Computed from 5000 by 1401 log-likelihood matrix</span></span>
<span id="cb208-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb208-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-5" aria-hidden="true" tabindex="-1"></a><span class="do">##          Estimate   SE</span></span>
<span id="cb208-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-6" aria-hidden="true" tabindex="-1"></a><span class="do">## elpd_loo  -5042.7 32.5</span></span>
<span id="cb208-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-7" aria-hidden="true" tabindex="-1"></a><span class="do">## p_loo        82.2  3.8</span></span>
<span id="cb208-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-8" aria-hidden="true" tabindex="-1"></a><span class="do">## looic     10085.4 65.1</span></span>
<span id="cb208-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-9" aria-hidden="true" tabindex="-1"></a><span class="do">## ------</span></span>
<span id="cb208-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Monte Carlo SE of elpd_loo is 0.2.</span></span>
<span id="cb208-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb208-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-12" aria-hidden="true" tabindex="-1"></a><span class="do">## All Pareto k estimates are good (k &lt; 0.5).</span></span>
<span id="cb208-13"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb208-13" aria-hidden="true" tabindex="-1"></a><span class="do">## See help(&#39;pareto-k-diagnostic&#39;) for details.</span></span></code></pre></div>
<p>Since we no longer have warnings, we can compare our models as seen below. This comparison contrasts the values of <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> for each data point across all our models. It presents the sum of the difference, and the standard error of that sum, relative to the best performing model. Below, <code>model_re_t</code> has an <code>elpd_diff</code> of 0, indicating that this is the best model according to this measure.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">loo_compare</span> (model_sum_coding, </span>
<span id="cb209-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-2" aria-hidden="true" tabindex="-1"></a>                   model_sum_coding_t, </span>
<span id="cb209-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-3" aria-hidden="true" tabindex="-1"></a>                   model_re_t, <span class="at">criterion =</span> <span class="st">&quot;loo&quot;</span>)</span>
<span id="cb209-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-4" aria-hidden="true" tabindex="-1"></a><span class="do">##                    elpd_diff se_diff</span></span>
<span id="cb209-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-5" aria-hidden="true" tabindex="-1"></a><span class="do">## model_re_t            0.0       0.0 </span></span>
<span id="cb209-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-6" aria-hidden="true" tabindex="-1"></a><span class="do">## model_sum_coding_t -204.4      19.2 </span></span>
<span id="cb209-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb209-7" aria-hidden="true" tabindex="-1"></a><span class="do">## model_sum_coding   -224.1      19.6</span></span></code></pre></div>
<p>What does the value of zero mean? It only means that this is our ‘reference point’. The value of <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> for our best model is:</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb210-1" aria-hidden="true" tabindex="-1"></a>model_re_t<span class="sc">$</span>criteria<span class="sc">$</span>loo<span class="sc">$</span>estimates[<span class="dv">1</span>,]</span>
<span id="cb210-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb210-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Estimate       SE </span></span>
<span id="cb210-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="do">## -4818.63    37.15</span></span></code></pre></div>
<p>So, if we set this value to zero then we can express the corresponding estimates for the other models relative to this value. Values of <code>elpd_diff</code> for <code>model_sum_coding_t</code> and <code>model_sum_coding</code> are -204 and -222 respectively, indicating substantial differences in <span class="math inline">\(\widehat{\mathrm{elpd}}_{LOO}\)</span> between our models. Specifically, they are around 200 less than our best model (so around -5000, as seen above). We can explore where these values come from by extracting the pointwise information from our models just as we did for <span class="math inline">\(\mathrm{WAIC}\)</span>.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb211-1" aria-hidden="true" tabindex="-1"></a>model_loo_info <span class="ot">=</span> model_sum_coding<span class="sc">$</span>criteria<span class="sc">$</span>loo<span class="sc">$</span>pointwise</span>
<span id="cb211-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb211-2" aria-hidden="true" tabindex="-1"></a>model_t_loo_info <span class="ot">=</span> model_sum_coding_t<span class="sc">$</span>criteria<span class="sc">$</span>loo<span class="sc">$</span>pointwise</span>
<span id="cb211-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb211-3" aria-hidden="true" tabindex="-1"></a>model_re_t_lo_info <span class="ot">=</span> model_re_t<span class="sc">$</span>criteria<span class="sc">$</span>loo<span class="sc">$</span>pointwise</span></code></pre></div>
<p>We can see that the <code>elpd_diff</code> is just the sum of the difference in <code>elpd</code> estimates across the two models:</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(model_t_loo_info[,<span class="dv">1</span>]<span class="sc">-</span>model_re_t_lo_info[,<span class="dv">1</span>])</span>
<span id="cb212-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb212-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -204.4</span></span></code></pre></div>
<p>The standard error of the difference is simply the standard deviation of the difference times the square root of the number of observations.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(model_t_loo_info[,<span class="dv">1</span>]<span class="sc">-</span>model_re_t_lo_info[,<span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1401</span>)</span>
<span id="cb213-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 19.21</span></span></code></pre></div>
<p>We can use this knowledge to calculate the difference between our two worst performing models since these are not directly compared above:</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(model_loo_info[,<span class="dv">1</span>]<span class="sc">-</span>model_t_loo_info[,<span class="dv">1</span>])</span>
<span id="cb214-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb214-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -19.64</span></span>
<span id="cb214-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb214-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(model_loo_info[,<span class="dv">1</span>]<span class="sc">-</span>model_t_loo_info[,<span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1401</span>)</span>
<span id="cb214-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb214-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 7.578</span></span></code></pre></div>
<p>And see that these values correspond to those obtained using the ‘official’ comparison function as seen below:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb215-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">loo_compare</span> (model_sum_coding, model_sum_coding_t, <span class="at">criterion =</span> <span class="st">&quot;loo&quot;</span>)</span>
<span id="cb215-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb215-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                    elpd_diff se_diff</span></span>
<span id="cb215-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb215-3" aria-hidden="true" tabindex="-1"></a><span class="do">## model_sum_coding_t   0.0       0.0  </span></span>
<span id="cb215-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb215-4" aria-hidden="true" tabindex="-1"></a><span class="do">## model_sum_coding   -19.6       7.6</span></span></code></pre></div>
<p>Finally, we can check out the estimated number of parameters by summing the third column of our pointwise information, which contains information about <span class="math inline">\(\mathrm{p_{\mathrm{LOO}}}\)</span> for each model. We can compare this to the actual number of parameters estimated by our models. An easy way to find the number of estimated parameters is to get the posterior draws for a model using the <code>get_samples</code> function from <code>bmmb</code>. This will return a data frame that contains one column for each estimated parameter, plus two columns for the <code>lp</code> and <code>lprior</code> (discussed in section <a href="fitting-bayesian-regression-models-with-brms.html#c3-log-posterior">3.8</a>). So, if we find the number of columns minus two, we can quickly count the number of estimated parameters for most kinds of models. However, keep in mind that this works as of the writing of this sentence (October 14th, 2022), and the output of software can change. It’s worth checking your expectations before taking shortcuts like this, for example by confirming the number of columns and seeing the information contained in each column before using this method to report the number of parameters.</p>
<p>Below, we compare the actual number of parameters for each model with our effective number of parameters. For example, we can see that going from Gaussian to t distributed errors results in the estimation of one additional ‘real’ parameter (<span class="math inline">\(\nu\)</span>), but the number of ‘effective’ parameters goes up by about 3. Adding random effects for apparent age to our model required the addition of 17 parameters: 15 listener-specific age effects, a standard deviation for these effects, and the correlation between the listener specific age and intercept effects. However, we can see that this has led to an increase of about 24 effective parameters. We can potentially explain this by thinking about how the inclusion of random effects for apparent age may have affected the estimation of the other random effects already included in the model. However, we are not going to worry too much and ‘over-interpret’ the effective number of parameters.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual and effective number of parameters for simplest model</span></span>
<span id="cb216-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ncol</span>(bmmb<span class="sc">::</span><span class="fu">get_samples</span>(model_sum_coding))<span class="sc">-</span><span class="dv">2</span></span>
<span id="cb216-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 114</span></span>
<span id="cb216-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(model_loo_info[,<span class="st">&#39;p_loo&#39;</span>])</span>
<span id="cb216-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 82.17</span></span>
<span id="cb216-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual and effective number of parameters for t model</span></span>
<span id="cb216-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ncol</span>(bmmb<span class="sc">::</span><span class="fu">get_samples</span>(model_sum_coding_t))<span class="sc">-</span><span class="dv">2</span></span>
<span id="cb216-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 115</span></span>
<span id="cb216-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-10" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(model_t_loo_info[,<span class="st">&#39;p_loo&#39;</span>])</span>
<span id="cb216-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 85.23</span></span>
<span id="cb216-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-13"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual and effective number of parameters for &#39;random effects&#39; model</span></span>
<span id="cb216-14"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ncol</span>(bmmb<span class="sc">::</span><span class="fu">get_samples</span>(model_re_t))<span class="sc">-</span><span class="dv">2</span></span>
<span id="cb216-15"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 132</span></span>
<span id="cb216-16"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-16" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(model_re_t_lo_info[,<span class="st">&#39;p_loo&#39;</span>])</span>
<span id="cb216-17"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb216-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 109.1</span></span></code></pre></div>
</div>
<div id="selecting-a-model" class="section level3 hasAnchor" number="6.4.4">
<h3><span class="header-section-number">6.4.4</span> Selecting a model<a href="variation-in-parameters-random-effects-and-model-comparison.html#selecting-a-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Which model should we use? Based on the comparison above, we see that <code>model_re_t</code> has an <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> that is 204 greater than the next best model. How large does an <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> difference need to be in order to be meaningful? There are two things that need to be taken into account: The magnitude of a difference and the uncertainty in the difference. A general rule of thumb seems to be that a difference of around 4 in <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> suggests a meaningful difference between the models (Vehtari et al. 2017). However, this is simply a general guideline and doesn’t mean that a difference of 3.99 does not matter and a difference of 4.01 does. On the other hand, even large differences between models may not be reliable in the presence of large standard errors (i.e. large amounts of variation in the difference). For example, below we see the difference between our two smaller models:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb217-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">loo_compare</span> (model_sum_coding, model_sum_coding_t, <span class="at">criterion =</span> <span class="st">&quot;loo&quot;</span>)</span>
<span id="cb217-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                    elpd_diff se_diff</span></span>
<span id="cb217-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb217-3" aria-hidden="true" tabindex="-1"></a><span class="do">## model_sum_coding_t   0.0       0.0  </span></span>
<span id="cb217-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb217-4" aria-hidden="true" tabindex="-1"></a><span class="do">## model_sum_coding   -19.6       7.6</span></span></code></pre></div>
<p>The difference is 19.6, which is obviously large enough to be considered a meaningful improvement in the model. However, the standard error is 7.6. This means that the difference is only about 2.6 standard errors from zero. How many standard errors away from zero does a difference need to be in order to be ‘real’? This is impossible to say, and in fact the ‘reality’ of any given model can not be definitively established by any statistical test. Ok, so how many standard errors from zero should it be before we make a fuss about it? The answer to this seems to be somewhere in the neighborhood of 2-4 at least. If we think of improvements to our model as continuous, we can think of differences that are two standard errors away as potentially ‘less reliable’ and differences that are &gt;5 standard errors away as ‘more reliable’.</p>
<p>However, <span class="math inline">\(\mathrm{elpd}\)</span> is just a tool and is not really meant to be used to select the ‘best’ or ‘real’ model from a set of alternatives. The decision about which model to use is up to the researcher and can’t be handed off to the models themselves. In addition to considering differences in <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> in isolation, the researcher should also consider how theoretically or practically warranted differences in their models are. For example, we know that our data contains way too many outliers to have proper normally distributed errors. Based on previous experience, we knew a priori that outliers are typical for listener judgements of things like apparent height. As a result, although the improvement provided by using t distributed errors is not large, we may choose to keep using t distributed errors in our model anyways. On the other hand, given the small difference between the estimates provided by both models and the not-huge difference in <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> between the models, a researcher may be warranted in simply using normally distributed errors in their model.</p>
<p>In contrast to these small differences, we see a difference of 204 in <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span> due to the addition of random effects for apparent age, 10.7 standard errors away from zero. Keep in mind this does not mean that a researcher <em>must</em> include this predictor nor that this structure will necessarily be an aspect of the <em>real</em> model. What it <em>does</em> mean is that: 1) The addition of random effects for apparent age greatly improves the fit of our model to our data, 2) This increase in fit is likely to extend to the data we did <em>not</em> observe, and 3) We have pretty good evidence that the difference between the models is reliable. We could add to this: 4) It makes <em>sense</em> that there would be listener-dependent variation in the effect for apparent age, for many reasons. As a result, in this situation it seems that random age effects should be included in our model.</p>
<p>Before finishing our discussion of model comparison, we want to show an example of a situation where adding a predictor does <em>not</em> help. To do this we add a useless predictor to our data frame, as below. This predictor is a random sample of -1 and 1 with no relationship to our height judgments. We can see below that the average reported height is basically the same for both values of our useless predictor.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb218-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb218-2" aria-hidden="true" tabindex="-1"></a>notmen<span class="sc">$</span>useless <span class="ot">=</span> <span class="fu">sample</span> (<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">nrow</span>(notmen), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb218-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb218-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span> (notmen<span class="sc">$</span>height, notmen<span class="sc">$</span>useless, mean)</span>
<span id="cb218-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb218-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  -1   1 </span></span>
<span id="cb218-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb218-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 153 154</span></span></code></pre></div>
<p>Below we fit our t distributed random effects model again, however, this time we include the useless predictor in the model formula, and even include a random effect for it:</p>
<p><code>height ~ A + useless + (A + useless|L) + (1|S)</code></p>
<p>We haven’t discussed including multiple predictors in our formulas yet, and in fact we will do so in the following chapter. For now, we are only interested in considering how this useless predictor affects our model comparisons. We fit this model below:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself</span></span>
<span id="cb219-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-2" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&#39;contr.sum&#39;</span>,<span class="st">&#39;contr.sum&#39;</span>))</span>
<span id="cb219-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-3" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">=</span> <span class="fu">c</span>(brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,156, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb219-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-4" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,0, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb219-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-5" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,0, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb219-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-6" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>), </span>
<span id="cb219-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-7" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;gamma(2, 0.1)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;nu&quot;</span>),</span>
<span id="cb219-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-8" aria-hidden="true" tabindex="-1"></a>           brms<span class="sc">::</span><span class="fu">set_prior</span>(<span class="st">&quot;student_t(3,0, 12)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sigma&quot;</span>))</span>
<span id="cb219-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-10" aria-hidden="true" tabindex="-1"></a>model_re_t_tooBig <span class="ot">=</span>  </span>
<span id="cb219-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-11" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">brm</span> (height <span class="sc">~</span> A <span class="sc">+</span> useless <span class="sc">+</span> (A <span class="sc">+</span> useless<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data =</span> notmen, </span>
<span id="cb219-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">4</span>, </span>
<span id="cb219-13"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb219-13" aria-hidden="true" tabindex="-1"></a>             <span class="at">prior =</span> priors, <span class="at">family =</span> <span class="st">&quot;student&quot;</span>)</span></code></pre></div>
<p>And add the <code>loo</code> criterion to it.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb220-1" aria-hidden="true" tabindex="-1"></a>model_re_t_tooBig <span class="ot">=</span> brms<span class="sc">::</span><span class="fu">add_criterion</span> (model_re_t_tooBig, <span class="at">criterion=</span><span class="st">&quot;loo&quot;</span>)</span></code></pre></div>
<p>Finally, we compare this to our equivalent model, minus the useless predictor.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb221-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">loo_compare</span> (model_re_t, model_re_t_tooBig)</span>
<span id="cb221-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                   elpd_diff se_diff</span></span>
<span id="cb221-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb221-3" aria-hidden="true" tabindex="-1"></a><span class="do">## model_re_t_tooBig  0.0       0.0   </span></span>
<span id="cb221-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb221-4" aria-hidden="true" tabindex="-1"></a><span class="do">## model_re_t        -0.7       2.8</span></span></code></pre></div>
<p>The more-complex model has a slightly higher <span class="math inline">\(\widehat{\mathrm{elpd}}\)</span>, but this difference is only 1/4 the magnitude of the standard error. In addition, we can look at a summary of the fixed effects and see that the value of the useless predictor is near zero (0.3), and the 95% credible interval is wide relative to this value and includes zero and very small values ([-.2, .8]).</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb222-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">fixef</span>(model_re_t_tooBig)</span>
<span id="cb222-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb222-2" aria-hidden="true" tabindex="-1"></a><span class="do">##           Estimate Est.Error     Q2.5    Q97.5</span></span>
<span id="cb222-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb222-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept 155.4454    1.1583 153.1882 157.7513</span></span>
<span id="cb222-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb222-4" aria-hidden="true" tabindex="-1"></a><span class="do">## A1          8.5267    1.1226   6.2757  10.7094</span></span>
<span id="cb222-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb222-5" aria-hidden="true" tabindex="-1"></a><span class="do">## useless     0.3362    0.2522  -0.1688   0.8357</span></span></code></pre></div>
<p>So, does this mean that this predictor should be excluded from the model? We would be justified in doing so, but may not necessarily want to. For example, if one of our important research questions centered on the value of the useless predictor, the fact that it is around zero and not some other value (e.g. -100) may be useful information. For example, we might want to keep the predictor in the model so we can say something like “So &amp; So (2007) reported a value of -100 for the useless predictor. However, our results suggest that the effect of this predictor is likely to be near zero [mean = 0.34, sd = 0.25, 95% CI = [-0.17, 0.84])”.</p>
</div>
</div>
<div id="c6-answering" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Answering our research questions<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-answering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ve selected the model with random effects as the ‘best’ model and are ready to revisit our research questions from last chapter again:</p>
<p>(Q1) How tall do speakers perceived as adult females sound?</p>
<p>(Q2) How tall do speakers perceived as children sound?</p>
<p>(Q3) What is the difference in apparent height associated with the perception of adultness?</p>
<p>Our answers to the main research questions are largely unchanged from the previous chapter (section <a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-answering-qs">5.12</a>), however, there have been some changes that highlight interesting aspects of our data. We’re going to focus on what our model tells us about the listener dependent intercepts and effects for apparent age.</p>
<p>We saw that there was a strong negative correlation between the listener means and age effects. The reason for this is clearly evident in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-1">6.1</a>. Listeners generally provided quite stable estimates for adult speakers. However, they exhibited substantial variation in their average judged height for child speakers. This makes sense. Most people probably have a good sense of approximately how tall adult females are on average. However, how tall are children 10-12? This is harder to answer for the average person. Not only is there more variability in height for children in these age ranges, but in addition most people are not around large numbers of children 10-12 and so may not have a good sense of how tall they are on average. Since reported adult heights are more or less fixed across listeners, listeners that indicated shorter children necessarily indicated shorter speakers overall. In addition, listeners that indicated shorter children also would tend to exhibit larger age effects. In this way, the listener means and age effect become negatively correlated.</p>
<p>Although we said we would stick to sum coding for this book, it is worth noting that this is actually a situation where it might make sense to use treatment coding, using adult as the reference level (see section <a href="comparing-two-groups-of-observations-factors-and-contrasts.html#c5-treatment-coding">5.6.1</a>). This would not affect our answers to the questions above but it would make the structure of the model more closely reflect the structure of the data, and it would decrease the correlation between the listener-dependent random effects. Why? Because when the listener intercepts correspond to their adult means (and not the grand mean), the intercept no longer helps you predict their age effect. As we can see in figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-1">6.1</a>, all listeners rate adult speakers about the same height regardless of how tall/short they think the children are.</p>
</div>
<div id="c6-frequentist" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> ‘Traditionalists’ corner<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-frequentist" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In traditionalists corner, we’re going to compare the output of <code>brms</code> to some more ‘traditional’ approaches. We’re not going to talk about the traditional models in any detail, the focus of this section is simply to highlight the similarities between different approaches, and to point out where to find equivalent information in the different models. If you are already familiar with these approaches, these sections may be helpful. If not, some of the information provided here may not make much sense, although it may still be helpful. If you want to know more about the statistical methods being discussed here, please see the preface for a list of suggested background reading in statistics.</p>
<div id="c6-vs-lmer" class="section level3 hasAnchor" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> Bayesian multilevel models vs. lmer<a href="variation-in-parameters-random-effects-and-model-comparison.html#c6-vs-lmer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we compare the output of <code>brms</code> to the output of the <code>lmer</code> (linear mixed-effects regression) function, a very popular function for fitting multilevel models in the <code>lme4</code> package in R. Below we fit a model that is analogous to our <code>model_re_t</code> model, save for the use of Gaussian rather than t-distributed errors. Since we set contrasts to sum coding using the options above, this will still be in effect for this model. If you have not done so, run the line:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb223-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&quot;contr.sum&quot;</span>,<span class="st">&quot;contr.sum&quot;</span>))</span></code></pre></div>
<p>Before fitting the model below so that its output looks as expected. We can fit a model with random age effects using <code>lmer</code> with the code below, and check out the model print statement.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get data</span></span>
<span id="cb224-2"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-2" aria-hidden="true" tabindex="-1"></a>notmen <span class="ot">=</span> bmmb<span class="sc">::</span>exp_data[exp_data<span class="sc">$</span>C_v<span class="sc">!=</span><span class="st">&#39;m&#39;</span> <span class="sc">&amp;</span> exp_data<span class="sc">$</span>C<span class="sc">!=</span><span class="st">&#39;m&#39;</span>,]</span>
<span id="cb224-3"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-4"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-4" aria-hidden="true" tabindex="-1"></a>lmer_model <span class="ot">=</span> lme4<span class="sc">::</span><span class="fu">lmer</span> (height <span class="sc">~</span> A <span class="sc">+</span>  (A<span class="sc">|</span>L) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>S), <span class="at">data =</span> notmen)</span>
<span id="cb224-5"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-6"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-6" aria-hidden="true" tabindex="-1"></a>lmer_model</span>
<span id="cb224-7"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Linear mixed model fit by REML [&#39;lmerMod&#39;]</span></span>
<span id="cb224-8"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula: height ~ A + (A | L) + (1 | S)</span></span>
<span id="cb224-9"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-9" aria-hidden="true" tabindex="-1"></a><span class="do">##    Data: notmen</span></span>
<span id="cb224-10"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-10" aria-hidden="true" tabindex="-1"></a><span class="do">## REML criterion at convergence: 9890</span></span>
<span id="cb224-11"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Random effects:</span></span>
<span id="cb224-12"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  Groups   Name        Std.Dev. Corr </span></span>
<span id="cb224-13"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  S        (Intercept) 3.53          </span></span>
<span id="cb224-14"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  L        (Intercept) 4.12          </span></span>
<span id="cb224-15"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-15" aria-hidden="true" tabindex="-1"></a><span class="do">##           A1          3.98     -0.92</span></span>
<span id="cb224-16"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-16" aria-hidden="true" tabindex="-1"></a><span class="do">##  Residual             7.67          </span></span>
<span id="cb224-17"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of obs: 1401, groups:  S, 94; L, 15</span></span>
<span id="cb224-18"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Fixed Effects:</span></span>
<span id="cb224-19"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-19" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)           A1  </span></span>
<span id="cb224-20"><a href="variation-in-parameters-random-effects-and-model-comparison.html#cb224-20" aria-hidden="true" tabindex="-1"></a><span class="do">##      155.05         8.64</span></span></code></pre></div>
<p>The model now includes estimates of the correlation between the listener random effects, and of the standard deviation of the listener-dependent age effects. Figure <a href="variation-in-parameters-random-effects-and-model-comparison.html#fig:F6-9">6.10</a> is a comparison of the listener means and age effects fit by both approaches. Clearly the results are very similar save for the fact that just as with our correlations and standard deviations, <code>brms</code> gives us intervals while <code>lmer</code> only gives us point estimates.</p>
<div class="figure"><span style="display:block;" id="fig:F6-9"></span>
<img src="_main_files/figure-html/F6-9-1.jpeg" alt="(left) Listener-dependent intercept effects and 95% credible intervals estimated using brms models. Crosses indicate random effects estimated by lmer. (right) Same as the left plot but showing the listener-dependent age effects." width="4800" />
<p class="caption">
Figure 6.10: (left) Listener-dependent intercept effects and 95% credible intervals estimated using brms models. Crosses indicate random effects estimated by lmer. (right) Same as the left plot but showing the listener-dependent age effects.
</p>
</div>
</div>
</div>
<div id="exercises-5" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Exercises<a href="variation-in-parameters-random-effects-and-model-comparison.html#exercises-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The analyses in the main body of the text all involve only the unmodified ‘actual’ resonance level (in <code>exp_data</code>). Responses for the stimuli with the simulate ‘big’ resonance are reserved for exercises throughout. You can get the ‘big’ resonance in the <code>exp_ex</code> data frame, or all data in the <code>exp_data_all</code> data frame.</p>
<p>Fit and interpret one or more of the suggested models:</p>
<ol style="list-style-type: decimal">
<li><p>Easy: Analyze the (pre-fit) model that’s exactly like <code>model_re_t</code>, except using the data in <code>exp_ex</code> (<code>bmmb::get_model("6_model_re_t_ex.RDS")</code>).</p></li>
<li><p>Medium: Fit a model like <code>model_re_t</code>, but comparing any two groups across resonance levels.</p></li>
<li><p>Hard: Fit two models like <code>model_re_t</code>, but comparing any two groups across resonance levels. Compare results across models to think about group differences.</p></li>
</ol>
<p>In any case, describe the model, present and explain the results, and include some figures. This time, also talk about the relationships between the random effects and any possible correlation between them.</p>
</div>
<div id="references-3" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> References<a href="variation-in-parameters-random-effects-and-model-comparison.html#references-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gelman, A., Hwang, J., &amp; Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and computing, 24(6), 997-1016.</p>
<p>McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.</p>
<p>Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and computing, 27(5), 1413-1432.</p>
<p>Vehtari A, Gabry J, Magnusson M, Yao Y, Bürkner P, Paananen T, Gelman A (2022). “loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models.” R package version 2.5.1, <a href="https://mc-stan.org/loo/" class="uri">https://mc-stan.org/loo/</a>.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="comparing-two-groups-of-observations-factors-and-contrasts.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="comparing-many-groups-interactions-and-posterior-predictive-checks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
